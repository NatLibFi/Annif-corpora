FAKULTETSOMRÃ…DET FÃ–R
NATURVETENSKAPER OCH TEKNIK

pro gradu-avhandling
Matematik

Optimal stokastisk reglering och
estimering med Kalmanltret
Skribent:

Handledare:

Tuomas Virtanen

Mikael Kurula

2020

Tuomas Virtanen

FÃ¶rord
Arbetet som presenteras i denna avhandling, som handlar om Kalmanltret och
optimal reglering av linjÃ¤ra stokastiska system, har gjorts vid Fakulteten fÃ¶r naturvetenskaper och teknik vid Ã…bo Akademi. StÃ¶rsta delen av arbetet gjordes
under sommaren och hÃ¶sten 2019. Stort tack till min handledare Mikael Kurula
med all hjÃ¤lp och tid under arbetets gÃ¥ng. Jag vill Ã¤ven tacka professor Paavo
Salminen fÃ¶r hans kommentarer. Jag vill ocksÃ¥ tacka min familj och mina vÃ¤nner
fÃ¶r all stÃ¶d under denna tid.

Ã…bo, Juli 2020

Tuomas Virtanen

ii

Tuomas Virtanen

Abstrakt
LinjÃ¤ra system anvÃ¤nds fÃ¶r matematiska modeller inom mÃ¥nga olika omrÃ¥den, bland annat reglerteknik. Avhandlingen behandlar teori fÃ¶r MIMO-system
(eng. Multiple Input Multiple Output), det vill sÃ¤ga modeller som bestÃ¥r av
tillstÃ¥ndsvariabler, en styrsignal bestÃ¥ende av era inputvariabler och en mÃ¤tsignal som ger information om era av systemets tillstÃ¥ndsvariabler. DÃ¥ systemet
dessutom pÃ¥verkas av stÃ¶rningar, mÃ¤tsignalen pÃ¥verkas av mÃ¤tfel och endast nÃ¥gra av tillstÃ¥ndsvariablerna kan mÃ¤tas Ã¤r systemet stokastiskt. Huvudsyftet med
avhandlingen Ã¤r att se pÃ¥ hur man kan reglera ett stokastiskt MIMO-system
optimalt med avseende pÃ¥ att minimera en kvadratisk kostnadsfunktional. Detta Ã¤r det sÃ¥ kallade LQG-problemet (eng. Linear Quadratic Gaussian) som Ã¤r
ett tvÃ¥delat problem. Eftersom systemet Ã¤r stokastiskt mÃ¥ste systemets tillstÃ¥nd
estimeras och sedan mÃ¥ste en styrsignal bestÃ¤mmas sÃ¥ att kostnaden minimeras. Metoden fÃ¶r estimeringen av tillstÃ¥ndet Ã¤r ett Kalmanlter och i denna avhandling delas ltret upp i tvÃ¥ delar, Kalmanprediktorn och Kalmankorrektorn.
Minimeringen av kostnaden kan gÃ¶ras separat och sedan kan skattningen given
av prediktorn eller korrektorn anvÃ¤ndas fÃ¶r att berÃ¤kna den optimala reglersignalen fÃ¶r systemet. Avhandlingen belyser med ett exempel pÃ¥ reglering av tvÃ¥
kopplade elnÃ¤tverk att reglering med anvÃ¤ndning av skattningen som ges av prediktorn fungerar och bÃ¶r anvÃ¤ndas Ã¤ven dÃ¥ mÃ¤tsignalen saknas en lÃ¤ngre tid. FÃ¶r
exemplet har ett simuleringsprogram skrivits i MatLab.

iii

INNEHÃ…LL

Tuomas Virtanen

InnehÃ¥ll
1 Inledning

1

2 LinjÃ¤r systemteori

4

2.1

2.2

2.3

2.4

Introduktion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

2.1.1

Sampling av tidskontinuerliga linjÃ¤ra system . . . . . . . .

6

Egenskaper hos tidsdiskreta LTI-system . . . . . . . . . . . . . . .

8

2.2.1

Styrbarhet . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

2.2.2

Observerbarhet

. . . . . . . . . . . . . . . . . . . . . . . .

10

2.2.3

Stabiliserbarhet . . . . . . . . . . . . . . . . . . . . . . . .

12

LuenbergerobservatÃ¶r . . . . . . . . . . . . . . . . . . . . . . . . .

14

2.3.1

. . . . . . . . . . . . . . . . . . . . .

15

Optimal reglering . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

2.4.1

21

Separationsprincipen

OÃ¤ndlig tidshorisont

. . . . . . . . . . . . . . . . . . . . .

3 Optimal stokastisk estimering

23

3.1

Denitioner och antaganden fÃ¶r diskreta stokastiska system . . . .

23

3.2

Estimering med Kalmanltret

. . . . . . . . . . . . . . . . . . . .

26

3.3

StationÃ¤rt Kalmanlter . . . . . . . . . . . . . . . . . . . . . . . .

39

3.4

Sampling av tidskontinuerliga stokastiska system . . . . . . . . . .

41

4 Optimal stokastisk reglering

47

4.1

Reglerproblemet . . . . . . . . . . . . . . . . . . . . . . . . . . . .

47

4.2

Minimering av kostnaden genom tillstÃ¥ndsÃ¥terkoppling

51

4.3

Minimering av kostnaden genom Ã¥terkoppling av det skattade till-

4.4

. . . . . .

stÃ¥ndet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

54

Optimal reglering av tvÃ¥ kopplade elnÃ¤tverk

58

iv

. . . . . . . . . . . .

INNEHÃ…LL

Tuomas Virtanen

5 Sammanfattande diskussion

65

A Matristeori

67

A.1

Matrisalgebra

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

A.2

Matrisexponentialfunktionen . . . . . . . . . . . . . . . . . . . . .

70

B Sannolikhetsteori
B.1

B.2

72

GrundlÃ¤ggande sannolikhetslÃ¤ra . . . . . . . . . . . . . . . . . . .

72

B.1.1

VÃ¤ntevÃ¤rde och varians . . . . . . . . . . . . . . . . . . . .

73

B.1.2

FÃ¶rdelningsfunktion . . . . . . . . . . . . . . . . . . . . . .

74

B.1.3

Oberoende stokastiska variabler

. . . . . . . . . . . . . . .

75

B.1.4

Betingat vÃ¤ntevÃ¤rde

. . . . . . . . . . . . . . . . . . . . .

76

Flerdimensionella stokastiska variabler
B.2.1

p-dimensionell

. . . . . . . . . . . . . . .

77

normalfÃ¶rdelning . . . . . . . . . . . . . . .

80

C Programkod

81

C.1

Exempel 3.24

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

81

C.2

Kopplade elnÃ¤tverk . . . . . . . . . . . . . . . . . . . . . . . . . .

85

Litteratur

90

v

KAPITEL 1.

INLEDNING

Tuomas Virtanen

Kapitel 1
Inledning
Ett system av linjÃ¤ra dierentialekvationer anvÃ¤nds fÃ¶r att bland annat beskriva
biologiska, fysikaliska eller nansiella modeller. Inom reglerteori studeras system
som har styrbara variabler, exempelvis acceleration, samt en mÃ¤tsignal som ger
nÃ¥gon information om systemets olika tillstÃ¥nd, exempelvis hastighet och position. DÃ¥ systemets tillstÃ¥nd dessutom pÃ¥verkas av stÃ¶rningar och mÃ¤tsignalen
innehÃ¥ller mÃ¤tfel, kan systemet modelleras som ett stokastiskt system. Ett exempel pÃ¥ ett sÃ¥dant system Ã¤r satellitnavigering, dÃ¤r GPS-mottagaren berÃ¤knar
sin position utgÃ¥ende frÃ¥n mÃ¤tsignaler frÃ¥n era satelliter. Ã…terkoppling Ã¤r ett
centralt begrepp inom reglerteori och med detta avses att mÃ¤tsignalen anvÃ¤nds
fÃ¶r att reglera systemet.
Det linjÃ¤rkvadratiska gaussiska reglerproblemet (LQG-problemet) Ã¤r ett
vÃ¤lstuderat och fundamentalt stokastiskt reglerproblem dÃ¤r modellen Ã¤r linjÃ¤r,
kostnadsfunktionalen Ã¤r kvadratisk, stÃ¶rningarna samt mÃ¤tfelen Ã¤r additivt
gaussiskt vitt brus och problemet Ã¤r att hitta en reglersignal som minimerar
kostnadsfunktionalen.

Detta

problem

Ã¤r

tvÃ¥delat,

Ã¥

ena

sidan

har

vi

det

linjÃ¤rkvadratiska estimeringsproblemet (LQE) att skatta systemets tillstÃ¥nd, Ã¥
andra sidan har vi det linjÃ¤rkvadratiska reglerproblemet (LQR) att konstruera
sjÃ¤lva regulatorn som reglerar systemet optimalt. Det visar sig att dessa tvÃ¥
problem kan lÃ¶sas oberoende av varandra och lÃ¶sningen pÃ¥ estimeringsproblemet
i diskret tid ges av det sÃ¥ kallade Kalmanltret, utvecklat huvudsakligen av den
ungerskfÃ¶dda amerikanska matematikern Rudolf Kalman pÃ¥ 1960-talet.
Avhandlingens upplÃ¤gg Ã¤r att i kapitel 2 ge en kort introduktion till linjÃ¤r systemteori av deterministiska system. I denna avhandling anvÃ¤nds en sÃ¥

1

KAPITEL 1.

INLEDNING

Tuomas Virtanen

kallad tillstÃ¥ndsmodell fÃ¶r linjÃ¤ra system som introduceras i introduktionskapitlet. TillstÃ¥ndsmodellen bygger pÃ¥ dierentialekvationer som beskriver systemet.
Ddierentialekvationen fÃ¶r tillstÃ¥ndsvektorn lÃ¶ses i introduktionskapitlet. Denna
lÃ¶sning samplas, vilket betyder att systemet i kontinuerlig tid Ã¶verfÃ¶rs till ett
diskretiserat system. Styrbarhet, observerbarhet och stabiliserbarhet Ã¤r nÃ¥gra
egenskaper som gÃ¥s igenom i korthet fÃ¶r system i diskret tid. Dessa egenskaper Ã¤r viktiga fÃ¶r att regleringen av systemet ska vara anvÃ¤ndbar och speciellt
viktiga fÃ¶r att kunna minimera kostnaden fÃ¶r regleringen. Ã…terkoppling och optimal reglering av deterministiska system samt konstruktionen av en sÃ¥ kallad
LuenbergerobservatÃ¶r introduceras ocksÃ¥ i introduktionskapitlet. FÃ¶r detta introduktionskapitel har i huvudsak [HRS07], [Lue79], [LXP08] och [TSH01] anvÃ¤nts.
Kapitel 3 behandlar teorin fÃ¶r optimal stokastisk estimering. I detta kapitel
denieras stokastiska system och vilka antaganden som sedan anvÃ¤nds fÃ¶r att
bygga upp teorin fÃ¶r att kunna estimera tillstÃ¥ndsvektorn. Detta gÃ¶rs vanligtvis
med ett sÃ¥ kallat Kalmanlter, men i denna avhandling ges en tvÃ¥delad metod fÃ¶r
skattningen genom Kalmanprediktorn och Kalmankorrektorn. IdÃ©n fÃ¶r att dela
upp Kalmanltret har tagits frÃ¥n [LXP08] och i denna avhandling gÃ¶rs ett fÃ¶rsÃ¶k att rigorÃ¶st deniera och hÃ¤rleda formler fÃ¶r att berÃ¤kna dessa skattningar,
dÃ¤r ocksÃ¥ [LR95] och [Ã…st70] har varit till stor hjÃ¤lp. FÃ¶r den stokastiska teorin
har [Gir03], [Kay93], [JP03] och [Ros10] anvÃ¤nts som kÃ¤llor. Meningen med det
uppdelade Kalmanltret Ã¤r att ha en optimal skattning av tillstÃ¥ndet bÃ¥de dÃ¥ ny
information om systemet Ã¤r tillgÃ¤ngligt och dÃ¥ ingen ny information Ã¤r tillgÃ¤nglig men en optimal skattning behÃ¶vs fÃ¶r att optimalt reglera systemet. I detta
kapitel introduceras ocksÃ¥ kort det stationÃ¤ra Kalmanltret och samplingen av
stokastiska system i kontinuerlig tid, fÃ¶r dessa har utÃ¶ver [Ã…st70] ocksÃ¥ [HRS07]
anvÃ¤nts som kÃ¤lla.
Kapitel 4 behandlar teorin fÃ¶r optimal stokastisk reglering. I detta kapitel presenteras LQG-problemet och verktygen fÃ¶r att lÃ¶sa minimeringsproblemet av den
kvadratiska kostnadsfunktionalen hÃ¤rleds. Konstruktionen av den optimala regulatorn fÃ¶r systemet bestÃ¤ms bÃ¥de dÃ¥ mÃ¤tsignalen ger exakt systemets tillstÃ¥nd
och dÃ¥ mÃ¤tsignalen pÃ¥verkas av brus. Det andra fallet Ã¤r huvudresultatet och ger
alltsÃ¥ den optimala reglersignalen fÃ¶r det stokastiska systemet dÃ¤r tillstÃ¥ndsvektorn estimeras med Kalmanprediktorn eller Kalmankorrektorn introducerade i
kapitel 3.

2

KAPITEL 1.

INLEDNING

Tuomas Virtanen

FÃ¶r att tillÃ¤mpa resultatet nns ett exempel pÃ¥ reglering av kopplade elnÃ¤tverk dÃ¤r artiklarna [AA11], [PMH13], [Ros+13] och [Sha+16] har studerats fÃ¶r
Ã¤ndamÃ¥let. Dierentialekvationerna samt olika parametervÃ¤rden Ã¤r tagna ur artiklarna. FÃ¶r tillÃ¤mpningen har ett simuleringsprogram skrivits i MatLab och
programkoden fÃ¶r detta hittas i bilagan.
I bilagan nns ocksÃ¥ grundlÃ¤ggande teori fÃ¶r matrisalgebra, matrisexponentialfunktionen och sannolikhetsteori. Teorin fÃ¶r betingade vÃ¤ntevÃ¤rdet och erdimensionella normalfÃ¶rdelningen Ã¤r synnerligen viktiga och lÃ¤saren ombeds bekanta sig med dessa fÃ¶re kapitel 3.

3

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

Kapitel 2
LinjÃ¤r systemteori
2.1 Introduktion
Matematiska modeller vars beteende kan beskrivas med hjÃ¤lp av era linjÃ¤ra
dierentialekvationer ger upphov till teorin om linjÃ¤ra system. Detta kapitel introducerar deterministiska tidsinvarianta linjÃ¤ra system och deras egenskaper.
Deterministisk system- och reglerteori fungerar som en introduktion till den mera krÃ¤vande stokastiska reglerteorin som Ã¤r den huvudsakliga teorin i denna avhandling. Som grundlÃ¤ggande teori fÃ¶r linjÃ¤r systemteori anvÃ¤nds matristeori
samt linjÃ¤r algebra och en kort sammanfattning av det som anvÃ¤nts hÃ¤r nns i
bilaga A.
Exemplet nedan anvÃ¤nds som ett genomgÃ¥ende exempel i avhandlingen fÃ¶r
att belysa de olika teoridelarna. Exemplet Ã¤r ett utvidgat exempel av [SÃ¤r13,
Exempel 3.6, s. 4345].

Exempel 2.1.
i planet. SÃ¤tt
respektive
vill sÃ¤ga

LÃ¥t

x3 =

x1

=0

och

x2

dx1
och
dt

y -riktning.

dx3
dt

och

vara

x4 =

x-

dx2
. DÃ¥ Ã¤r
dt

dx4
dt

u = (u1 , u2 )T . DÃ¥
â› â
dx1
dt
âœ dx
âŸ
âœ dt2 âŸ
âœ âŸ
âœ dx3 âŸ
â dt â 
dx4
dt

x3

y -koordinaterna

och

x4

fÃ¶r en punkt

punktens hastighet i

x-

Anta att accelerationen Ã¤r noll i bÃ¥da riktningarna, det

= 0.

Vidare anta att vi kan styra punktens acceleration

och beteckna dessa inputvariabler med
och

respektive

u1

och

u2 .

LÃ¥t nu

x = (x1 , x2 , x3 , x4 )T

kan vi beskriva systemets beteende pÃ¥ vektorform enligt

â›
0
âœ
âœ0
=âœ
âœ0
â
0

ââ› â â›
0 1 0
x1
0
âŸâœ âŸ âœ
âœ âŸ âœ
0 0 1âŸ
âŸ âœx2 âŸ + âœ0
âœ âŸ âœ
0 0 0âŸ
â  âx3 â  â1
0 0 0
x4
0
4

0

â

âŸ (ï¸„ )ï¸„
0âŸ
âŸ u1 ,
0âŸ
â  u2
1

KAPITEL 2.

eller kort

LINJÃ„R SYSTEMTEORI

xÌ‡ = Ax + Bu.

Tuomas Virtanen

Om vi har tillgÃ¥ng till den exakta positionen av

punkten men inte dess hastighet eller acceleration sÃ¥ kan vi beskriva en utsignal
fÃ¶r systemet enligt

â› â
x
(ï¸„ )ï¸„ (ï¸„
)ï¸„ âœ 1 âŸ
âŸ
y1
1 0 0 0 âœ
âœ x2 âŸ ,
=
âŸ
y2
0 1 0 0 âœ
â x3 â 
x4
y = Cx.

eller kort

Modellen enligt vilken systemet i exemplet beskrivs kallas

tillstÃ¥ndsmodellen

och Ã¤r speciellt anvÃ¤ndbar fÃ¶r linjÃ¤ra system med era input- och outputvariabler.

MIMO-system

SÃ¥dana system kallas ocksÃ¥

put). Den formella denitionen fÃ¶r ett

(eng. Multiple Input Multiple Out-

linjÃ¤rt tidsinvariant system

(LTI-system)

i kontinuerlig tid Ã¤r fÃ¶ljande:

Denition 2.2.
t

Vektorn

x(t) âˆˆ Rn

beskriver systemets tillstÃ¥nd vid tidpunkt

tillstÃ¥ndsvektorn, u(t) âˆˆ Rm

och kallas

systemet och

p

y(t) âˆˆ R

Ã¤r en

utsignal

Ã¤r en deterministisk

mÃ¤tning

som ger en

insignal

som styr

av systemets tillstÃ¥nd.

DÃ¥ kan systemet skrivas pÃ¥ tillstÃ¥ndsmodellen

TillstÃ¥ndet

x(0) = x0

Ì‡
x(t)
= Ax(t) + Bu(t),

(2.1)

y(t) = Cx(t) + Du(t).

(2.2)

Ã¤r systemets

starttillstÃ¥nd.

matris, B insignalmatris, C utsignalmatris
Matriserna

A, B , C

och

D

Avbildningen

A

kallas

system-

D direktmatris.

och

antas hÃ¤r vara konstanta, dÃ¤rfÃ¶r kallas systemet

tidsinvariant. Oftast kan direktmatrisen

D

utelÃ¤mnas och dÃ¥ kallas systemet

strikt propert. Systemet i exempel 2.1 Ã¤r ett exempel pÃ¥ ett strikt propert system.
Systemets tillstÃ¥nd vid tidpunkt
tillstÃ¥nd

x0

och en given styrsignal

t kan berÃ¤knas utgÃ¥ende frÃ¥n ett kÃ¤nt startu(t)

genom att lÃ¶sa fÃ¶ljande begynnelsevÃ¤r-

desproblem:

Ì‡
x(t)
= Ax(t) + Bu(t),

LÃ¶sning:

x(0) = x0 .

Skriv om dierentialekvationen som

Ì‡ âˆ’ Ax(t) = Bu(t)
x(t)
och multiplicera med den integrerande faktorn
matrisexponentialfunktionen (se A.2), fÃ¶r att fÃ¥

5

âˆ«ï¸

e

(âˆ’A)dt

= eâˆ’At ,

dÃ¤r

eâˆ’At

Ã¤r

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

)ï¸
eâˆ’At x(t) = eâˆ’At Bu(t)
âˆ«ï¸‚ t
âˆ’At
eâˆ’As Bu(s)ds + x(0)
â‡â‡’ e x(t) =
0
âˆ«ï¸‚ t
At
â‡â‡’ x(t) = e x(0) +
eA(tâˆ’s) Bu(s)ds,

Ì‡ âˆ’ Ax(t)) = eâˆ’At Bu(t) â‡â‡’
eâˆ’At (x(t)

d
dt

(ï¸

0
dÃ¤r sista ekvivalensen gÃ¤ller enligt sats A.6. Den entydiga lÃ¶sningen till (2.1) blir

t

âˆ«ï¸‚

At

eA(tâˆ’s) Bu(s)ds.

x(t) = e x0 +

(2.3)

0

2.1.1

Sampling av tidskontinuerliga linjÃ¤ra system

I praktiken sker styrning eller reglering av system med datorer som opererar i
diskret tid men modellerna av system beskrivs oftast av dierentialekvationer enligt fysikaliska modeller och Ã¤r pÃ¥ formen (2.1)(2.2). FÃ¶r regleringen behÃ¶ver vi
beskriva systemet i diskret tid och detta utfÃ¶rs med
pÃ¥ det sÃ¤ttet att systemets tillstÃ¥nd

tk := kT ,

dÃ¤r

T >0

evalueras vid de diskreta tidpunkterna

Ã¤r lÃ¤ngden av samplingsperioden. MÃ¤tningarna av utsigna-

len gÃ¶rs vid tidpunkterna

tk

u(t) hÃ¥lls konstant pÃ¥ varje samp[ï¸
)ï¸
t âˆˆ kT, (k + 1)T . DÃ¥ styrsignalen skickas

och styrsignalen

uk := u(tk )

lingsintervall sÃ¥ att

x(t)

sampling. Samplingen utfÃ¶rs

dÃ¥

frÃ¥n datorn till systemet gÃ¶rs detta oftast med

digital-till-analog-omvandlare

och

dÃ¥ styrsignalen Ã¤r konstant pÃ¥ varje samplingsintervall har D\A-omvandlaren en

nollte ordningens hÃ¥llkrets

(eng. zero-order hold; ZOH). Det samplade systemet

Ã¶nskas beskrivas av dierensekvationen
och dÃ¥ behÃ¶ver matriserna

As

och

Bs

xk+1 = As xk + Bs uk ,

dÃ¤r

xk := x(tk )

bestÃ¤mmas.

Enligt (2.3) fÃ¥s att

Atk+1

xk+1 = x(tk+1 ) = e

âˆ«ï¸‚
x0 +

tk+1

eA(tk+1 âˆ’s) Bu(s)ds

0

= eA(k+1)T x0 +

âˆ«ï¸‚

(k+1)T

eA((k+1)T âˆ’s) Bu(s)ds

0

âˆ«ï¸‚

kT

âˆ«ï¸‚

AT A(kT âˆ’s)

(k+1)T

e
x0 +
e e
Bu(s)ds +
eA((k+1)T âˆ’s) Bu(s)ds
0
(ï¸ƒ
)ï¸ƒ âˆ«ï¸‚ TkT
âˆ«ï¸‚ tk
= eAT eAtk x0 +
eA(tk âˆ’s) Bu(s)ds +
eAÏ„ Bu((k + 1)T âˆ’ Ï„ )dÏ„
0
0
(ï¸ƒâˆ«ï¸‚ T
)ï¸ƒ
âˆ«ï¸‚ T
AT
AÏ„
AT
AÏ„
= e x(tk ) +
e Bu(tk )dÏ„ = e xk +
e dÏ„ Buk .

=e

AT AkT

0

0
(2.4)
6

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

FÃ¶r utsignalen fÃ¥s att

yk : = y(tk ) = Cx(tk ) + Du(tk )
= Cxk + Duk .
Det samplade systemet kan nu skrivas som

xk+1 = As xk + Bs uk ,
yk = Cs xk + Ds uk ,
fÃ¶r

k = 0, 1, 2, . . .
As := e

AT

och matriserna ges av

T

(ï¸ƒâˆ«ï¸‚
,

Bs :=

e

AÏ„

)ï¸ƒ
dÏ„

B,

Cs := C

Ds := D.

och

0

Exempel 2.3.

Vi utfÃ¶r sampling med ZOH-metoden av systemet som introdu-

cerades i exempel 2.1. Beteckna samplingsperioden med
och sÃ¥ledes Ã¤r

Ak = 0

fÃ¶r varje

kâ‰¥2

âˆ†t.

Eftersom

A2 = 0

fÃ¥s enligt denitionen (A.2) fÃ¶r matrisex-

ponentialfunktionen att

â›
As = e

Aâˆ†t

1 0 âˆ†t

âœ
âœ0 1
= I + Aâˆ†t = âœ
âœ0 0
â

0

0 0

0

1

0

â

âŸ
âˆ†tâŸ
âŸ.
0âŸ
â 
1

(2.5)

Vidare har vi att

â›

1 0 Ï„ 0

â

â›

0 0

â

âˆ†t

âŸ
âŸ âœ
âœ
âœ0 1 0 Ï„ âŸ âœ0 0 âŸ
âŸ
âŸ âœ
âœ
Bs =
âœ0 0 1 0âŸ dÏ„ âœ1 0âŸ
0
â  â
â 
â
0 1
0 0 0 1
â›
â
â›
â â› (âˆ†t)2
2
âˆ†t 0 (âˆ†t)
0
0 0
2
âœ
âŸ âœ 2
(âˆ†t)2 âŸ âœ
âœ 0 âˆ†t
âŸ âœ0 0 âŸ âœ 0
0
2 âŸâœ
âŸ=âœ
=âœ
âœ0
âŸ
âœ
âœ
0
âˆ†t
0 â  â1 0 âŸ
â
â  â âˆ†t
0
0
0
âˆ†t
0 1
0
âˆ«ï¸‚

Nu fÃ¥s det diskretiserade systemet

xk+1 = As xk + Bs uk ,
yk = Cs xk ,
dÃ¤r

As

och

Bs

ges av (2.5)(2.6) och

Cs =
7

(ï¸„
)ï¸„
1 0 0 0
0 1 0 0

.

0

â

(âˆ†t)2 âŸ
âŸ
2 âŸ

.
0 âŸ
â 
âˆ†t

(2.6)

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

2.2 Egenskaper hos tidsdiskreta LTI-system
I fortsÃ¤ttningen behandlar vi endast strikt propra system i diskret tid och vÃ¤ljer
dÃ¤rfÃ¶r direktmatrisen
Det strikt propra

D = 0 och lÃ¤mnar bort det nedre indexet s pÃ¥ matriserna.

tidsdiskreta LTI-systemet

enligt tillstÃ¥ndsmodellen ges dÃ¥ av:

xk+1 = Axk + Buk ,

(2.7)

yk = Cxk .
x0

FÃ¶r givet starttillstÃ¥nd

och en given styrsignal

âˆ’1
u := {uk }N
k=0

kan syste-

mets (2.7) tillstÃ¥ndsekvation itereras fÃ¶r att fÃ¥

x1 = Ax0 + Bu0 ,
x2 = Ax1 + Bu1 = A(Ax0 + Bu0 ) + Bu1 ,
= A2 x0 + A1 Bu0 + A0 Bu1 ,
.
.
.

xN = AN x0 + AN âˆ’1 Bu0 + . . . + A0 BuN âˆ’1
och lÃ¶sningen till systemets (2.7) tillstÃ¥ndsekvation Ã¤r dÃ¥

N

xN = A x0 +

N
âˆ’1
âˆ‘ï¸‚

AN âˆ’1âˆ’k Buk .

(2.8)

k=0
Egenskaper som Ã¤r viktiga att analysera Ã¤r systemets styrbarhet, observerbarhet
och stabiliserbarhet. I detta avsnitt ges kortfattat vad som menas med dessa egenskaper och hur systemmatriserna i (2.7) anvÃ¤nds fÃ¶r att kontrollera egenskaperna.
Denitioner och satser fÃ¶r styrbarhet och observerbarhet fÃ¶r bÃ¥de tidsdiskreta
och tidskontinuerliga system hittas exempelvis i [Lue79, s. 276289].

2.2.1

Styrbarhet

Denition 2.4 (Styrbarhet). Ett tidsdiskret system (2.7) Ã¤r styrbart om fÃ¶r varje
tillstÃ¥nd

xâˆ— âˆˆ Rn

en styrsignal

och starttillstÃ¥ndet

N âˆ’1
u := {uk }k=0

x0 = 0 nns ett Ã¤ndligt index Nxâˆ— > 0 och

som tillÃ¤mpat pÃ¥ (2.7) ger att

FrÃ¥gan Ã¤r om det nns ett Ã¤ndligt index
varje tillstÃ¥nd

xâˆ— âˆˆ Rn

kan nÃ¥s frÃ¥n origo pÃ¥

8

N
N

xNxâˆ— = xâˆ— .

som inte beror av
steg?

xâˆ— ,

men att

KAPITEL 2.

DÃ¥

x0 = 0

LINJÃ„R SYSTEMTEORI

kan lÃ¶sningen (2.8) skrivas pÃ¥ matrisform enligt

â›
â
u
)ï¸‚ âœ N âˆ’1 âŸ
.
N âˆ’1
âŸ
.
A
B âœ
â . â .
u0

(ï¸‚
xN = B AB Â· Â· Â·

Blockmatrisen framfÃ¶r styrsignalen

Sats 2.5.

Tuomas Virtanen

(ï¸
)ï¸
T T
u = uT
N âˆ’1 , . . . , u0

Det tidsdiskreta systemet

betecknas

CN (A, B).

med A âˆˆ RnÃ—n och B âˆˆ RnÃ—m Ã¤r

(2.7)

styrbart om och endast om det gÃ¤ller fÃ¶r n Ã— nm styrbarhetsmatrisen
(ï¸‚
)ï¸‚
Cn (A, B) = B AB Â· Â· Â· Anâˆ’1 B
att

rang Cn (A, B) = n.
FÃ¶r beviset av satsen behÃ¶vs fÃ¶rst ett hjÃ¤lpresultat.

Lemma 2.6.

LÃ¥t A âˆˆ RnÃ—n ha full rang. DÃ¥ gÃ¤ller att An+k Ã¤r en linjÃ¤rkombi-

nation av matriserna I , A, . . ., Anâˆ’1 fÃ¶r varje k â‰¥ 0.
Bevis.

Enligt

Cayley-Hamiltons sats

pA (A) = 0,

Ã¤r

dÃ¤r

pA (Î»)

Ã¤r det karakteris-

tiska polynomet

n
âˆ‘ï¸‚

pA (Î») = det (Î»I âˆ’ A) =

ci Î»i ,

cn = 1.

i=0
Vi fÃ¥r att

n

0 = pA (A) = A +

nâˆ’1
âˆ‘ï¸‚

ci A

i

â‡â‡’

n

A =âˆ’

nâˆ’1
âˆ‘ï¸‚

i=0
det vill sÃ¤ga

An

i=0

Ã¤r en linjÃ¤rkombination av

A

n+k

ci A i ,

=âˆ’

nâˆ’1
âˆ‘ï¸‚

I, A, . . . , Anâˆ’1 .

Vidare gÃ¤ller att

ci Ai+k ,

i=0
varfÃ¶r

An+k

en linjÃ¤rkombination av

Bevis av Sats 2.5.

Om

I, A, . . . , Anâˆ’1

rang Cn (A, B) = n

fÃ¶r varje

k â‰¥ 0.

sÃ¥ Ã¤r matrisen surjektiv, vilket impli-

cerar att systemet Ã¤r styrbart.
Om systemet Ã¤r styrbart sÃ¥ nns ett
att

CN (A, B) u = xâˆ— ,

dÃ¤r

xâˆ— âˆˆ Rn

N > 0 och en styrsignal u0 , . . . , uN âˆ’1

Ã¤r godtyckligt. Anta att

9

N â‰¥ n,

sÃ¥

dÃ¥ fÃ¥s som

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

fÃ¶ljd av lemma 2.6 att matrisen

B, AB, . . . , Anâˆ’1 B

An+k B

Tuomas Virtanen

Ã¤r en linjÃ¤rkombination av matriserna

i styrbarhetsmatrisen

Cn (A, B)

fÃ¶r varje

k â‰¥ 0.

DÃ¥ kan vi

âˆ—
âˆ—
hitta en ny styrsignal u0 , . . . , unâˆ’1 , sÃ¥ att

xâˆ— = CN (A, B)u = Cn (A, B)uâˆ— .
Detta visar att

Cn (A, B)

Ã¤r surjektiv och har sÃ¥ledes rangen

n.

Eftersom systemets styrbarhet Ã¤r oberoende av matriserna
matrisparet

(A, B)

Exempel 2.7.

C

och

D

sÃ¤gs att

Ã¤r styrbart.

Vi undersÃ¶ker styrbarhetsmatrisen fÃ¶r systemet i exempel 2.3.

Systemmatriserna ges av

â›

1 0 âˆ†t

âœ
âœ0 1
A=âœ
âœ0 0
â
0 0

â

â› (âˆ†t)2

âŸ
âˆ†tâŸ
âŸ
0âŸ
â 
1

âœ 2
âœ 0
B=âœ
âœ âˆ†t
â
0

0

0
1
0

och

0

â

(âˆ†t)2 âŸ
âŸ
2 âŸ

.
0 âŸ
â 
âˆ†t

Styrbarhetsmatrisen fÃ¶r detta system Ã¤r dÃ¥

â› (âˆ†t)2
2

âœ
âœ 0
C4 (A, B) = âœ
âœ âˆ†t
â
0

0

3(âˆ†t)2
2

0

5(âˆ†t)2
2

0

7(âˆ†t)2
2

(âˆ†t)2
2

0

3(âˆ†t)2
2

0

5(âˆ†t)2
2

0

0

âˆ†t

0

âˆ†t

0

âˆ†t

âˆ†t

0

âˆ†t

0

âˆ†t

0

4

Vi kan se att matrisen har rangen

2.2.2

â

7(âˆ†t)2 âŸ
âŸ
2 âŸ

.
0 âŸ
â 
âˆ†t

vilket implicerar att systemet Ã¤r styrbart.

Observerbarhet

Denition 2.8

.

(Observerbarhet)

om det nns ett Ã¤ndligt

observerbart

kan entydigt bestÃ¤mmas

y0 , . . . , yN âˆ’1 .

FrÃ¥gan Ã¤r om det nns ett

N

Ett tidsdiskret system (2.7) Ã¤r

N > 0 sÃ¥ att starttillstÃ¥ndet x0

utgÃ¥ende frÃ¥n mÃ¤tningarna

hjÃ¤lp av

0

stycken mÃ¤tningar

N

sÃ¥ att

x0

kan alltid bestÃ¤mmas entydigt med

y0 , . . . , yN âˆ’1 ?

stemets (2.7) utsignal vid tidpunkt

N

FrÃ¥n lÃ¶sningen (2.8) ser vi att sy-

Ã¤r

yN = CxN = CAN x0 + CAN âˆ’1 Bu0 + . . . + CA0 BuN âˆ’1 .

10

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

DÃ¥ styrsignalen Ã¤r kÃ¤nd kan den, utan att inskrÃ¤nka pÃ¥ allmÃ¤ngiltigheten, antas
vara noll och dÃ¥ fÃ¥s att

yN = CAN x0 .
Om mÃ¤tningarna

y0 , . . . , yN âˆ’1 skrivs som en pN Ã— 1 vektor
â›
â â›
â
y
C
âœ 0 âŸ âœ
âŸ
âœ y1 âŸ âœ CA âŸ
âœ
âŸ âœ
âŸ
âœ . âŸ=âœ
âŸ x0 .
.
.
âœ .. âŸ âœ
âŸ
.
â
â  â
â 
yN âˆ’1
CAN âˆ’1

Blockmatrisen framfÃ¶r starttillstÃ¥ndet

Sats 2.9.

Det tidsdiskreta systemet

x0

betecknas med

(2.7)

fÃ¥s att

(2.9)

ON (C, A).

med A âˆˆ RnÃ—n och C âˆˆ RpÃ—n Ã¤r

observerbart om och endast om det gÃ¤ller fÃ¶r pn Ã— n
â
â›
C
âŸ
âœ
âœ CA âŸ
âŸ
âœ
On (C, A) = âœ . âŸ
âœ .. âŸ
â 
â
nâˆ’1
CA

observerbarhetsmatrisen

att

rang On (C, A) = n.
Bevis.

Om

rang On (C, A) = n

sÃ¥ Ã¤r matrisen injektiv, vilket implicerar att

systemet observerbart.
Om systemet Ã¤r observerbart sÃ¥ gÃ¤ller att (2.9) har en entydig lÃ¶sning
nÃ¥got

N > 0,

ger att
varje

vilket implicerar att matrisen

CAn+k

k â‰¥ 0.

Ã¤r en linjÃ¤rkombination av matriserna

DÃ¥ fÃ¶ljer att om

sÃ¥ledes rangen

ON (C, A)

N â‰¥n

sÃ¥ mÃ¥ste

x0

fÃ¶r

Ã¤r injektiv. Lemma 2.6

C, CA, . . . , CAnâˆ’1

On (C, A)

fÃ¶r

vara injektiv och har

n.

Eftersom systemets observerbarhet Ã¤r oberoende av matriserna
sÃ¤gs ocksÃ¥ att matrisparet

(C, A)

B

och

D,

sÃ¥

Ã¤r observerbart.

(ï¸
)ï¸T
AnmÃ¤rkning 2.10. Observera att matriserna On (C, A) = Cn (AT , C T ) och
(ï¸
)ï¸T
Cn (A, B) = On (B T , AT ). Detta ger att matrisparet (A, B) Ã¤r styrbart om
och endast om matrisparet

(B T , AT ) Ã¤r observerbart och matrisparet (C, A) Ã¤r

observerbart om och endast om matrisparet
och observerbarhet Ã¤r alltsÃ¥

duala

(AT , C T )

Ã¤r styrbart. Styrbarhet

egenskaper fÃ¶r ett linjÃ¤rt system.

11

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

Notera dock att detta betyder inte att ett styrbart system Ã¤r observerbart, ef-

B medan observerbarhetsmatrisen

tersom styrbarhetsmatrisen beror av matrisen
beror av matrisen

Exempel 2.11.

C.
Vi undersÃ¶ker observerbarhetsmatrisen fÃ¶r systemet i exem-

pel 2.3. Systemmatriserna ges av

â›

1 0 âˆ†t

âœ
âœ0 1
A=âœ
âœ0 0
â

0

0 0

0

â

0

(ï¸„

âŸ
âˆ†tâŸ
âŸ
0âŸ
â 
1

1

C=

och

)ï¸„
1 0 0 0
0 1 0 0

.

Observerbarhetsmatrisen fÃ¶r detta system Ã¤r dÃ¥

â›

1
âœ
âœ0
âœ
âœ
âœ1
âœ
âœ0
âœ
O4 (C, A) = âœ
âœ1
âœ
âœ
âœ0
âœ
âœ1
â
0
Vi kan se att matrisen har rangen

2.2.3

0

0

0

â

âŸ
0 âŸ
âŸ
âŸ
0 âˆ†t
0 âŸ
âŸ
1 0
âˆ†t âŸ
âŸ
âŸ.
0 2âˆ†t 0 âŸ
âŸ
âŸ
1 0 2âˆ†tâŸ
âŸ
0 3âˆ†t 0 âŸ
â 
1 0 3âˆ†t
1

0

4 vilket implicerar att systemet Ã¤r observerbart.

Stabiliserbarhet

Ett system med fÃ¶ljande tillstÃ¥ndsekvation kallas ett

autonomt

xk+1 = Axk

system
(2.10)

och enligt (2.8) lÃ¶sningen

x k = Ak x 0 ,

Denition 2.12 (Stabilitet).
bilt

k â‰¥ 0.

Ett autonomt system (2.10) Ã¤r

(2.11)

(asymptotiskt) sta-

om dess lÃ¶sning (2.11) uppfyller

lim xk = 0,

kâ†’âˆ
fÃ¶r varje val av starttillstÃ¥nd
system (2.7) med styrsignalen

x0 âˆˆ R n .
u = 0

Om detta uppfylls fÃ¶r icke-autonoma

sÃ¥ Ã¤r ocksÃ¥ systemet (2.7) stabilt. DÃ¥

systemet Ã¤r stabilt sÃ¤gs ocksÃ¥ att matrisen

12

A

Ã¤r stabil.

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

Om systemet har endast en tillstÃ¥ndsvariabel

x

sÃ¥ gÃ¤ller att

xk = Î± k x 0
och dÃ¥ Ã¤r detta system stabilt fÃ¶r varje

x0 âˆˆ R

om och endast om

|Î±| < 1.

FÃ¶r system med era tillstÃ¥ndsvariabler fÃ¥s fÃ¶ljande resultat enligt [Lue79, s.
155156].

Sats 2.13.

Systemet

(2.7)

Ã¤r asymptotiskt stabilt om och endast om fÃ¶r varje

egenvÃ¤rde Î»i , i âˆˆ {1, . . . , n} till systemmatrisen A gÃ¤ller att |Î»i | < 1.
Bevis.

Vi visar fallet dÃ¥

A Ã¤r diagonaliserbar. DÃ¥ existerar en inverterbar matris,

sÃ¥ att

A = M Î›M âˆ’1

dÃ¤r

Î»1 , . . . , Î»n

â›
â
Î»1 0 . . . 0
âœ
âŸ
âœ 0 Î»2 . . . 0 âŸ
âœ
âŸ âˆ’1
=Mâœ.
,
.
. âŸM
..
.
.
âœ ..
.
.
. âŸ
â
â 
0 0 . . . Î»n

Ã¤r egenvÃ¤rdena till

A.

Nu fÃ¥s att

xk = Ak x0 = M Î›M âˆ’1 M Î›M âˆ’1 Â· Â· Â· M Î›M âˆ’1 x0
= M Î›k M âˆ’1 x0
â
â›
k
Î»
0 ... 0
âŸ
âœ 1
âœ 0 Î»k . . . 0 âŸ
2
âŸ âˆ’1
âœ
x0
=Mâœ .
.
. âŸM
.
.
..
. âŸ
âœ ..
.
.
â 
â
0 0 . . . Î»kn
och dÃ¥ gÃ¤ller att

lim xk = 0 â‡â‡’ |Î»i | < 1,

kâ†’âˆ

Beviset fÃ¶r fallet dÃ¥
istÃ¤llet pÃ¥
resultat.

A

fÃ¶r varje

i âˆˆ {1, . . . , n}.

inte Ã¤r diagonaliserbar baserar sig pÃ¥ att

Jordans normalform

Stabiliseringsproblemet

A

skrivs

och dÃ¥ fÃ¥s med liknande berÃ¤kningar samma
gÃ¥r ut pÃ¥ att konstruera en

regulator

som tar

systemets (2.7) utsignal som input och ger som output en styrsignal till systemet (2.7), sÃ¥dant att det ihopkopplade system Ã¤r stabilt [TSH01, s. 57]. DÃ¥
systemets tillstÃ¥nd Ã¤r tillgÃ¤ngligt, det vill sÃ¤ga matrisen
len, och regulatorn Ã¤r statisk, fÃ¥s en styrsignal av formen

C = I

fÃ¶r utsigna-

uk = âˆ’Lxk

fÃ¶r varje

k = 0, 1, . . ., dÃ¤r L Ã¤r en godtycklig m Ã— n matris. Denna form av reglering kallas

13

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

statisk tillstÃ¥ndsÃ¥terkoppling

Tuomas Virtanen

och denna styrsignal tillÃ¤mpad pÃ¥ systemet (2.7)

ger tillstÃ¥ndsekvationen

xk+1 = Axk + Buk = Axk âˆ’ BLxk = (A âˆ’ BL)xk .

(2.12)

Denna ekvation Ã¤r nu i form av ett autonomt system och kallas ocksÃ¥ ett

system

slutet

eftersom tillstÃ¥ndet Ã¥terkopplas genom styrsignalen. Stabiliseringsproble-

met i detta fall reduceras till att bestÃ¤mma matrisen

Denition 2.14 (Stabiliserbarhet).
det nns en matris

Sats 2.15.

L,

Matrisparet

sÃ¥ att matrisen

A âˆ’ BL

L

sÃ¥ att (2.12) Ã¤r stabil.

(A, B) kallas stabiliserbart

om

Ã¤r stabil.

LÃ¥t matrisparet (A, B) vara styrbart. DÃ¥ gÃ¤ller att matrisparet

(A, B) Ã¤r stabiliserbart.
Beviset utelÃ¤mnas men baserar sig pÃ¥ satsen om polplacering [Lue79, s. 299].

2.3 LuenbergerobservatÃ¶r
TillstÃ¥ndsÃ¥terkopplingen (2.12) krÃ¤ver att tillstÃ¥ndsvektorn

xk

observatÃ¶r

ta Ã¤r ofta inte fallet. HÃ¤rnÃ¤st kommer en sÃ¥ kallad

konstrueras, med avsikten att rekonstruera tillstÃ¥ndsvektorn
mÃ¤tningar

y0 , . . . , yk

och en given styrsignal

u.

Ã¤r kÃ¤nd, men det-

fÃ¶r LTI-system att

xk

utgÃ¥ende frÃ¥n

LuenbergerobservatÃ¶ren Ã¤r en

deterministisk version av Kalmanltret som konstrueras i nÃ¤sta kapitel.
Konstruktionen av LuenbergerobservatÃ¶ren gÃ¶rs enligt [Ã…st70, s. 142] pÃ¥ fÃ¶ljande sÃ¤tt. FÃ¶rst skapas ett modellsystem

Ì‚ k+1 = Ax
Ì‚ k + Buk ,
x
Ì‚k,
yÌ‚k = C x
dÃ¤r

Ì‚k
x

betecknar skattningen av tillstÃ¥ndet

som i (2.7) och styrsignalen

Ì‚ 0 = x0
x

u

xk .

Systemmatriserna Ã¤r samma

Ã¤r samma som fÃ¶r det ursprungliga systemet. Om

sÃ¥ skulle modellens lÃ¶sning sammanfalla med systemets (2.7) lÃ¶sning,

vilket skulle medfÃ¶ra att

Ì‚k
x

Ã¤r en exakt skattning av tillstÃ¥ndet fÃ¶r varje

Problemet Ã¤r att starttillstÃ¥ndet

x0

Ã¤r okÃ¤nt och endast mÃ¤tningar

tillgÃ¤ngliga som inte beaktas av den ovanstÃ¥ende modellen. LÃ¥t

k â‰¥ 0.

y0 , . . . , yk

Ã¤r

Ì‚k
ek = xk âˆ’ x

beteckna felet mellan skattningen och det verkliga tillstÃ¥ndet. DÃ¥ ger den sÃ¥

14

KAPITEL 2.

kallade

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

innovationen yÌƒk := yk âˆ’ yÌ‚k = Cek

tillstÃ¥ndet

xk ,

ett mÃ¥tt pÃ¥ hur bra

Ì‚k
x

skattar

och detta antyder att tillstÃ¥ndsekvationen har formen

Ì‚ k+1 = Ax
Ì‚ k + Buk + K yÌƒk ,
x
fÃ¶r nÃ¥gon matris
fÃ¥s

K.

(2.13)

Subtraheras tillstÃ¥ndet (2.13) frÃ¥n systemets tillstÃ¥nd (2.7)

felsystemet

Ì‚ k+1 = A(xk âˆ’ x
Ì‚ k ) âˆ’ K yÌƒk
ek+1 = xk+1 âˆ’ x

(2.14)

= (A âˆ’ KC)ek .
Om det gÃ¤ller fÃ¶r felsystemet (2.14) att

x0

och varje val av skattning

Denition 2.16

Ì‚0,
x

sÃ¥ kallas (2.13) en

.

(Detekterbarhet)

det nns en matris

K,

ek â†’ 0 dÃ¥ k â†’ âˆ, fÃ¶r varje starttillstÃ¥nd

Matrisparet

sÃ¥ att matrisen

A âˆ’ KC

tillstÃ¥ndsobservatÃ¶r.

(C, A)

kallas

detekterbart

om

Ã¤r stabil.

Det Ã¤r alltsÃ¥ mÃ¶jligt att konstruera en LuenbergerobservatÃ¶r om och endast
om matrisparet

Sats 2.17.

(C, A)

Ã¤r detekterbart.

LÃ¥t matrisparet (C, A) vara observerbart. DÃ¥ gÃ¤ller att matrisparet

(C, A) Ã¤r detekterbart.
Bevis.

Resultatet fÃ¥s genom dualitet. Enligt anmÃ¤rkning 3.4 fÃ¥s att om

(C, A)

T
T
T
T
Ã¤r observerbart sÃ¥ Ã¤r (A , C ) styrbart. Sats 2.15 ger att dÃ¥ Ã¤r (A , C ) stabiliserbart. Denitionen fÃ¶r stabiliserbarhet sÃ¤ger att det nns en matris
matrisen

AT âˆ’C T K T

enligt denitionen fÃ¶r detekterbarhet Ã¤r matrisparet

2.3.1

Aâˆ’KC

Ã¤r stabil. DÃ¥ Ã¤r ocksÃ¥ matrisen

(C, A)

K T , sÃ¥ att

stabil och sÃ¥ledes

detekterbart.

Separationsprincipen

LÃ¥t nu styrsignalen till systemet (2.7) vara

Ì‚k,
uk = âˆ’Lx

dÃ¤r

Ì‚k
x

Ã¤r skattningen

given av observatÃ¶ren (2.13) och Ã¤r kÃ¤nd eftersom observatÃ¶ren Ã¤r konstruerad
pÃ¥ det sÃ¤ttet. DÃ¥ blir tillstÃ¥ndsekvationen fÃ¶r det slutna systemet

Ì‚k
xk+1 = Axk âˆ’ BLx
= Axk âˆ’ BL(xk âˆ’ ek )
= (A âˆ’ BL)xk + BLek ,

15

(2.15)

KAPITEL 2.

dÃ¤r felet

ek

av matrisen

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

ges av (2.14). Ã… ena sidan kan matrisen

L

K

bestÃ¤mmas oberoende

sÃ¥ att skattningen given av observatÃ¶ren konvergerar mot syste-

mets egentliga tillstÃ¥nd. Ã… andra sidan kan matrisen
matrisen

K

L bestÃ¤mmas oberoende av

sÃ¥ att systemet stabiliseras. Detta kallas fÃ¶r

separationsprincipen

i

deterministisk reglerteori och gÃ¤ller fÃ¶r linjÃ¤ra system [Lue79, s. 307]. Felet (2.14)
och tillstÃ¥ndet (2.15) kan skrivas som ett autonomt system

(ï¸„

xk+1
ek+1

)ï¸„
=

(ï¸„
A âˆ’ BL

)ï¸„ (ï¸„ )ï¸„
xk

BL
A âˆ’ KC

0

ek

.

(2.16)

EgenvÃ¤rdena fÃ¶r denna matris Ã¤r precis unionen av egenvÃ¤rdena fÃ¶r matriserna

A âˆ’ BL

och

A âˆ’ KC

och detta slutna system Ã¤r stabilt dÃ¥ matrisparet

Ã¤r detekterbart, matrisparet
har valts sÃ¥ att

A âˆ’ BL

och

(A, B)

Ã¤r stabiliserbart och matriserna

A âˆ’ KC

(C, A)

K

och

L

Ã¤r stabila.

2.4 Optimal reglering
Det Ã¤r av intresse att bestÃ¤mma matriserna

K

och

L

pÃ¥ ett optimalt sÃ¤tt. I

denna avhandling Ã¤r huvudintresset stokastiska system, fÃ¶r vilka Kalmanltret
kan ses som en observatÃ¶r, varfÃ¶r en explicit konstruktionen av matrisen

K

fÃ¶r

LuenbergerobservatÃ¶ren utelÃ¤mnas hÃ¤r.
Med optimal reglering avses att konstruera en

regulator

som styr systemet till

origo sÃ¥ att en given kostnad minimeras. Om regulatorns styrsignal Ã¤r i form av
tillstÃ¥ndsÃ¥terkoppling
lingsmatrisen

uk = âˆ’Lxk , Ã¤r optimal reglering bestÃ¤mning av Ã¥terkopp-

L sÃ¥ att kostnaden minimeras. En kostnadsfunktional J(x0 , u) ger

ett skalÃ¤rt vÃ¤rde pÃ¥ kostnaden att utfÃ¶ra styrningen
tillstÃ¥ndet

âˆ’1
u = {uk }N
k=0 ,

givet start-

x0 .

HÃ¤r anvÃ¤nds fÃ¶ljande kvadratiska kostnadsfunktional

JN (x0 , u) := xT
N SN xN +

N
âˆ’1
âˆ‘ï¸‚

T
(xT
k Qxk + uk Ruk ),

(2.17)

k=0
eftersom den Ã¤r enkel att hantera matematiskt och Ã¤r tillrÃ¤ckligt exibel fÃ¶r
mÃ¥nga tillÃ¤mpningar. Denna kostnadsfunktional har en
och

SN

Ã¤r kostnadsmatrisen fÃ¶r sluttillstÃ¥ndet vid tiden

Ã¤ndlig tidshorisont N

N.

Systemets tillstÃ¥nd

beskrivs oftast som en avvikelse frÃ¥n det Ã¶nskade tillstÃ¥ndet och dÃ¥ ger
naden fÃ¶r denna avvikelse. Matriserna

SN

16

och

Q

Q

kost-

kan vÃ¤ljas godtyckligt med

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

restriktionen till positivt semidenita matriser sÃ¥ att termerna i (2.17) som innehÃ¥ller dessa matriser Ã¤r icke-negativa. Matrisen

R

Ã¤r kostnadsmatrisen fÃ¶r

styrningen och den vÃ¤ljs sÃ¥ att den Ã¤r positivt denit, vilket betyder att varje
term i 2.17 som innehÃ¥ller
Det

R

Ã¤r positiv.

linjÃ¤rkvadratiska reglerproblemet

eller kort

quadratic) gÃ¥r ut pÃ¥ att hitta en reglersignal

LQ-problemet

(eng. linear-

u fÃ¶r ett linjÃ¤rt system (2.7), sÃ¥dan

att den kvadratiska kostnadsfunktionalen (2.17) minimeras.
FÃ¶ljande sats enligt [HRS07, Sats 5.3.1], modierad fÃ¶r kostnadsfunktionalen (2.17), ger den optimala reglersignalen fÃ¶r deterministiska LTI-system, dÃ¥ vi

xk

har tillgÃ¥ng till systemets tillstÃ¥nd

Sats 2.18.
nalen

vid varje tidpunkt

Reglersignalen till systemet

(2.17),

(2.7),

k.

som minimerar kostnadsfunktio-

Ã¤r

uk = âˆ’Lk xk ,

k = 0, . . . , N âˆ’ 1,

dÃ¤r den tidsvarianta Ã¥terkopplingsmatrisen Lk ges av

Lk := (B T Sk+1 B + R)âˆ’1 B T Sk+1 A

(2.18)

och Sk Ã¤r lÃ¶sningen till ekvationen

Sk := AT Sk+1 A + Q âˆ’ AT Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 A
med begynnelsevÃ¤rdet SN i

(2.19)

(2.17).

Vidare gÃ¤ller att minimala kostnaden Ã¤r

min JN (x0 , u) = xT
0 S0 x0 ,

(2.20)

u

dÃ¤r S0 fÃ¥s ur
AnmÃ¤rkning

(2.19).
2.19. Om tillstÃ¥ndet

xk

inte Ã¤r tillgÃ¤ngligt vid varje tidpunkt kan

enligt separationsprincipen 2.3.1 en LuenbergerobservatÃ¶r konstrueras om systemet Ã¤r detekterbart. DÃ¥ kan skattningen
regleringen istÃ¤llet fÃ¶r tillstÃ¥ndet

AnmÃ¤rkning

Ì‚k
x

givet av observatÃ¶ren anvÃ¤ndas fÃ¶r

xk .

2.20. De tidsvarierande Ã¥terkopplingsmatriserna

Lk

Ã¤r oberoende

av tillstÃ¥ndet och utsignalen av systemet och kan alltsÃ¥ berÃ¤knas pÃ¥ fÃ¶rhand sÃ¥
snart systemmatriserna och kostnadsmatriserna Ã¤r kÃ¤nda.

17

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

FÃ¶r beviset av satsen behÃ¶vs fÃ¶rst tvÃ¥ lemman. Det fÃ¶rsta angÃ¥ende lÃ¶sningarna till den

dynamiska tidsdiskreta Riccatiekvationen

(2.19), vilket ofta utelÃ¤mnas

i litteraturen vid konstruktionen av den optimala reglersignalen. Det andra (som
ocksÃ¥ behÃ¶vs senare i det stokastiska fallet) hjÃ¤lper med omskrivningen av kostnadsfunktionalen (2.17).

Lemma 2.21.

LÃ¥t SN â‰¥ 0, Q â‰¥ 0 och R > 0. Deniera Sk genom

(2.19)

fÃ¶r

0 â‰¤ k â‰¤ N âˆ’ 1. DÃ¥ gÃ¤ller att Sk â‰¥ 0 fÃ¶r varje k = 0, . . . , N .
Vidare om Q > 0 sÃ¥ Ã¤r Sk > 0 fÃ¶r varje k = 0, . . . , N âˆ’ 1.
Bevis.
DÃ¥

Enligt antagandet Ã¤r

R>0

sÃ¥ Ã¤r

SN â‰¥ 0. Anta nu att Sk+1 â‰¥ 0 fÃ¶r nÃ¥got k â‰¤ N âˆ’ 1.

(B T Sk+1 B + R) > 0

och dÃ¤rmed inverterbar, varfÃ¶r (2.19) ger

att

Sk = AT Sk+1 A âˆ’ AT Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 A + Q.
1/2

Sk+1 . Vi kan skriva
(ï¸
1/2
1/2
1/2 )ï¸
1/2
Sk = (Sk+1 A)T I âˆ’ Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 (Sk+1 A) + Q.

LÃ¥t

DÃ¥

Sk+1

Qâ‰¥0

vara den positivt semidenita kvadratroten av

gÃ¤ller att

Sk â‰¥ 0

om

(ï¸
1/2
1/2
1/2 )ï¸
1/2
(Sk+1 A)T I âˆ’ Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 (Sk+1 A) â‰¥ 0.
Eftersom

LÃ¥t

T T P T â‰¥ 0 om P â‰¥ 0 enligt lemma A.3 rÃ¤cker det att
(ï¸
1/2
1/2 )ï¸
I âˆ’ Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 â‰¥ 0.

R1/2

vara den positivt denita kvadratroten av

R.

visa att

Vi kan skriva

I âˆ’ Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1
(ï¸
)ï¸âˆ’1
1/2
1/2
= I âˆ’ Sk+1 B(R1/2 )âˆ’1 (R1/2 )âˆ’1 B T Sk+1 B(R1/2 )âˆ’1 + I (R1/2 )âˆ’1 B T Sk+1 .
1/2

SÃ¤tt

1/2

1/2

W := Sk+1 B(R1/2 )âˆ’1

och dÃ¥ fÃ¥r vi med hjÃ¤lp av lemma A.1 att

I âˆ’ Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 = I âˆ’ W (W T W + I)âˆ’1 W T .
1/2

1/2

= (W W T + I)âˆ’1 ,
vilket Ã¤r en positivt denit matris eftersom det Ã¤r en invers. DÃ¥ har vi visat att

Sk = (Sk+1 A)T (W W T + I)âˆ’1 (Sk+1 A) + Q â‰¥ 0
1/2

och enligt induktionsprincipen fÃ¥s att
fÃ¥s att om

Q>0

1/2

Sk â‰¥ 0

sÃ¥ ser vi ur (2.21) att

fÃ¶r varje

Sk > 0

18

k = 0, . . . , N âˆ’ 1.

fÃ¶r varje

(2.21)
Vidare

k = 0, . . . , N âˆ’ 1.

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Lemma 2.22.

LÃ¥t Lk och Sk ges av

Tuomas Virtanen

(2.18)

respektive

(2.19)

och anta att matri-

serna Q och SN , Ã¤r positivt semidenita och R positivt denit. DÃ¥ gÃ¤ller fÃ¶ljande
likhet
T
T
(Axk + Buk )T Sk+1 (Axk + Buk ) âˆ’ xT
k Sk xk + xk Qxk + uk Ruk

= (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ).
Bevis.

(2.22)

Utveckling av vÃ¤nstra ledet i (2.22) ger

T
T T
T T
V L = xT
k A Sk+1 Axk + xk A Sk+1 Buk + uk B Sk+1 Axk
T
T
T
T
+ uT
k B Sk+1 Buk âˆ’ xk Sk xk + xk Qxk + uk Ruk

(2.23)

T
T T
= xT
k (A Sk+1 A + Q âˆ’ Sk )xk + xk A Sk+1 Buk
T
T
T
+ uT
k B Sk+1 Axk + uk (B Sk+1 B + R)uk .
Lemma 2.21 ger att

Sk â‰¥ 0

(B T Sk+1 B + R) > 0

fÃ¶r varje

k = 0, . . . , N

och dÃ¥

R > 0

sÃ¥ Ã¤r

och dÃ¤rmed inverterbar enligt sats A.2. Vi kan nu slÃ¥

ihop de tvÃ¥ sista termerna i (2.23), med anvÃ¤ndning av (2.18), pÃ¥ fÃ¶ljande sÃ¤tt

T
T
T
uT
k B Sk+1 Axk + uk (B Sk+1 B + R)uk
(ï¸
)ï¸
T
T
âˆ’1 T
= uT
k (B Sk+1 B + R) uk + (B Sk+1 B + R) B Sk+1 Axk
T
= uT
k (B Sk+1 B + R)(uk + Lk xk ).
Vidare gÃ¤ller att

T
T
âˆ’1
LT
k = A Sk+1 B(B Sk+1 B + R)

nering och det faktum att

R

och

Sk+1

(2.24)

enligt reglerna fÃ¶r transpo-

Ã¤r symmetriska. DÃ¥ Ã¤r

T
AT Sk+1 B = LT
k (B Sk+1 B + R).

(2.25)

InsÃ¤ttning av (2.24) och (2.25) i (2.23) ger att

T
T T
T
V L = xT
k (A Sk+1 A + Q âˆ’ Sk )xk + xk Lk (B Sk+1 B + R)uk
T
+ uT
k (B Sk+1 B + R)(uk + Lk xk ).
Om vi nu skriver om (2.19) som

T
Sk = AT Sk+1 A + Q âˆ’ LT
k (B Sk+1 B + R)Lk ,
sÃ¥ kan fÃ¶rsta termen i (2.26) skrivas

19

(2.26)

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

T
xT
k (A Sk+1 A + Q âˆ’ Sk )xk
(ï¸‚
(ï¸ T
)ï¸)ï¸‚
TS
T (B T S
= xT
A
A
+
Q
âˆ’
A
S
A
+
Q
âˆ’
L
B
+
R)L
xk
k+1
k+1
k+1
k
k
k
T
T
= xT
k Lk (B Sk+1 B + R)Lk xk .

(2.27)

InsÃ¤ttning av (2.27) i (2.26) ger nu att

T
T T
T
T
V L = xT
k Lk (B Sk+1 B + R)Lk xk + xk Lk (B Sk+1 B + R)uk
T
+ uT
k (B Sk+1 B + R)(uk + Lk xk )
T
T
T
T
= xT
k Lk (B Sk+1 B + R)(uk + Lk xk ) + uk (B Sk+1 B + R)(uk + Lk xk )

= (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ).

Bevis av Sats 2.18.

Vi gÃ¶r en omskrivning av kostnadsfunktionalen (2.17) enligt

fÃ¶ljande

JN (x0 , u) = xT
N SN xN +
= xT
0 S0 x0 +
= xT
0 S0 x0 +

N
âˆ’1
âˆ‘ï¸‚

T
(xT
k Qxk + uk Ruk )

k=0
N
âˆ’1
âˆ‘ï¸‚

T
(xT
k+1 Sk+1 xk+1 âˆ’ xk Sk xk ) +

k=0
N
âˆ’1
âˆ‘ï¸‚

N
âˆ’1
âˆ‘ï¸‚

T
(xT
k Qxk + uk Ruk )

k=0

(ï¸
)ï¸
(Axk + Buk )T Sk+1 (Axk + Buk ) âˆ’ xT
k Sk xk

k=0

+

N
âˆ’1
âˆ‘ï¸‚

T
(xT
k Qxk + uk Ruk ).

k=0
Nu kan vi anvÃ¤nda lemma 2.22 som ger att

JN (x0 , u) = xT
0 S0 x0 +

N
âˆ’1
âˆ‘ï¸‚

(uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )

k=0
och eftersom

(B T Sk+1 B + R) > 0

sÃ¥ Ã¤r varje term i summan stÃ¶rre Ã¤n eller

lika med noll. DÃ¥ och endast dÃ¥ styrsignalen vÃ¤ljs som

k = 0, . . . , N âˆ’ 1

uk = âˆ’Lk xk

fÃ¶r varje

sÃ¥ Ã¤r varje term i summan lika med noll och vi fÃ¥r att

min JN (x0 , u) = xT
0 S0 x0 .
u

20

KAPITEL 2.

2.4.1

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

OÃ¤ndlig tidshorisont

I vissa tillÃ¤mpningar kan det vara oklart hur man ska vÃ¤lja tidshorisonten

N

i

kostnadsfunktionalen (2.17), vilket motiverar enligt [HRS07, s. 61] att studera
reglerproblemet med en oÃ¤ndlig tidshorisont och fÃ¶ljande kostnadsfunktional

âˆ
âˆ‘ï¸‚
T
Jâˆ (x0 , u) :=
(xT
k Qxk + uk Ruk ).

(2.28)

k=0
Minimeringen av denna Ã¤r meningsfull endast dÃ¥ kostnaden kan gÃ¶ras Ã¤ndlig och
ett tillrÃ¤ckligt villkor fÃ¶r detta Ã¤r att systemet Ã¤r stabiliserbart [HRS07, s. 62].
Det visar sig att under vissa villkor kan man approximera problemet fÃ¶r
oÃ¤ndlig tidshorisont med problemet fÃ¶r Ã¤ndlig tidshorisont och lÃ¥ter
LÃ¥t

Q = H TH

fÃ¶r nÃ¥gon

N â†’ âˆ.

p Ã— n matris H . DÃ¥ Ã¤r Riccatiekvationen (2.19) av

formen

Sk = AT Sk+1 A + H T H âˆ’ AT Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 A

(2.29)

och enligt fÃ¶ljande sats konvergerar lÃ¶sningen mot ett grÃ¤nsvÃ¤rde som Ã¤r oberoende av valet av

Sats 2.23.

SN .

Anta att Q â‰¥ 0, R > 0, matrisparet (H, A) Ã¤r detekterbart, dÃ¤r

H T H = Q och matrisparet (A, B) Ã¤r stabiliserbart. Beteckna lÃ¶sningen vid
tidpunkt k = 0 till

(2.29)

startad frÃ¥n SN med S0 (SN ) och lÃ¥t

L(S) := (B T SB + R)âˆ’1 B T SA.

(2.30)

DÃ¥ gÃ¤ller fÃ¶ljande pÃ¥stÃ¥enden:
1. Det nns en entydig matris S+ â‰¥ 0, oberoende av SN , sÃ¥ att

lim S0 (SN ) = S+ .

N â†’âˆ

Matrisen S+ satiserar den

algebraiska Riccatiekvationen

S = AT SA + Q âˆ’ AT SB(B T SB + R)âˆ’1 B T SA

(2.31)

och matrisen A âˆ’ BL(S+ ) Ã¤r stabil. Vidare gÃ¤ller att om matrisparet

(A, B) Ã¤r styrbart sÃ¥ Ã¤r S+ > 0.
2. Reglersignalen till systemet

(2.7)

som minimerar

uk = âˆ’L(S+ )xk
och minimala kostnaden Ã¤r Jâˆ (x0 , u) = xT
0 S+ x0 .
21

(2.28)

ges av

KAPITEL 2.

LINJÃ„R SYSTEMTEORI

Tuomas Virtanen

FÃ¶rsta pÃ¥stÃ¥endet nns bevisat i [LR95, Sats 17.5.3, s. 382] och det andra
pÃ¥stÃ¥endet nns bevisat i [HRS07, Sats 5.3.2, s. 62].
Observera att Ã¥terkopplingsmatrisen i detta fall Ã¤r tidsinvariant. Den optimala reglersignalen till systemet (2.7) som minimerar kostnadsfunktionalen (2.28)
med oÃ¤ndlig tidshorisont Ã¤r alltsÃ¥ i form av statisk tillstÃ¥ndsÃ¥terkoppling.

AnmÃ¤rkning

2.24. Om

A Ã¤r inverterbar sÃ¥ gÃ¤ller enligt [LR95, Korollarium 13.5.3

s. 326] att bland alla positivt semidenita lÃ¶sningar till (2.31) Ã¤r
lÃ¶sningen fÃ¶r vilken matrisen
alla lÃ¶sningar

S

A âˆ’ BL(S+ )

S+ den entydiga

Ã¤r stabil. Det gÃ¤ller ocksÃ¥ att bland

till (2.31), fÃ¶r vilka matrisen

A âˆ’ BL(S)

entydiga positivt semidenita lÃ¶sningen. Vidare gÃ¤ller att

Ã¤r stabil, Ã¤r

S+

fÃ¶r matrisen

A

den

Ã¤r den maximala

positivt semidenita lÃ¶sningen till (2.31) i den mening att fÃ¶r varje
lÃ¶ser (2.31) sÃ¥ gÃ¤ller att matrisen

S+

S Ì¸= S+

som

S+ âˆ’S Ã¤r positivt semidenit. Inverterbarheten

Ã¤r inte nÃ¶dvÃ¤ndig enligt [DGG86].

22

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

Kapitel 3
Optimal stokastisk estimering
3.1 Denitioner och antaganden fÃ¶r diskreta stokastiska system
Repetition av grundlÃ¤ggande sannolikhetslÃ¤ra och normalfÃ¶rdelningen fÃ¶r stokastiska vektorer samt notationer och begrepp som anvÃ¤nds i denna avhandling
nns i bilaga B.
Detta kapitel bÃ¶rjar med nÃ¥gra denitioner fÃ¶r stokastiska processer enligt [LR95, s. 375] som Ã¤r viktiga grundlÃ¤ggande denitioner fÃ¶r stokastiska
system. Teorin fÃ¶r stokastiska system gÃ¶rs hÃ¤r endast i diskret tid, men en
metod fÃ¶r sampling av kontinuerliga system ges i avsnitt 3.4. Ett exempel
av samplingen och metoden fÃ¶r estimeringen av tillstÃ¥ndet som gÃ¥s igenom i
avsnitt 3.2 presenteras i slutet av kapitlet.

Denition 3.1.
(i) En

vk

stokastisk process
bestÃ¥r av

n

Ã¤r en fÃ¶ljd

{vk }kâ‰¥0

stycken stokastiska variabler.

vitt brus

xk

Ã¤r vitt brus, sÃ¥som denierat ovan, och varje vektor

vk

Ã¤r oberoende, vilket implicerar att

Gaussiskt vitt brus
Ã¤r dessutom

j Ì¸= k

och

(ii) En stokastisk process sÃ¤gs vara

(iii)

av stokastiska vektorer, dÃ¤r varje

n-dimensionellt

om fÃ¶r

Cov(vj , vk ) = 0

dÃ¥

gÃ¤ller att

xj

j Ì¸= k .

normalfÃ¶rdelad.

(iv) En stokastisk process sÃ¤gs ha

vÃ¤ntevÃ¤rdet noll

k â‰¥ 0.
23

om

E(vk ) = 0

fÃ¶r varje

KAPITEL 3.

Ett

OPTIMAL STOKASTISK ESTIMERING

stokastiskt LTI-system

Tuomas Virtanen

ges av tillstÃ¥ndsmodellen:

xk+1 = Axk + Buk + F vk ,

(3.1)

yk = Cxk + Gwk .

(3.2)

FÃ¶ljande antaganden gÃ¶rs fÃ¶r systemet (3.1)(3.2):
1.

Processbruset {vk }kâ‰¥0

och

mÃ¤tbruset {wk }kâ‰¥0

Ã¤r gaussiskt vitt brus med

vÃ¤ntevÃ¤rdet noll och givna kovariansmatriser

E(vk vkT ) = Pv â‰¥ 0,

E(wk wkT ) = Pw > 0,

fÃ¶r varje

k.

2. Processbruset och mÃ¤tbruset Ã¤r oberoende processer sÃ¥ att

E(vj wkT ) = 0,
3. StarttillstÃ¥det

G

j

och

k.

x0 antas vara normalfÃ¶rdelat med givna vÃ¤ntevÃ¤rdet m0 och

kovariansmatrisen
4. Matrisen

fÃ¶r varje

P0 > 0.

antas vara surjektiv.

5. Vidare antas att

x0

oberoende fÃ¶r varje

och

vj

Ã¤r oberoende fÃ¶r varje

j

samt

x0

och

wk

Ã¤r

k.

PÃ¥ samma sÃ¤tt som fÃ¶r deterministiska system fÃ¥s att lÃ¶sningen till (3.1) ges av

xn = An x0 +

nâˆ’1
âˆ‘ï¸‚

Anâˆ’1âˆ’k (Buk + F vk ),

(3.3)

k=0
men lÃ¶sningen hÃ¤r Ã¤r en stokastisk vektor som fÃ¶ljd av processbruset. Observera
att ekvation (3.2) kan ocksÃ¥ skrivas

yk = Cxk + fk ,
dÃ¤r

{fk }kâ‰¥0

Ã¤r gaussiskt vitt brus med vÃ¤ntevÃ¤rdet noll och kovariansmatrisen

Pf = GPw GT .

Lemma 3.2.

LÃ¥t x och v vara oberoende normalfÃ¶rdelade stokastiska vektorer

med kovariansmatriserna Cov(x) = Px och Cov(v) = Pv . LÃ¥t A och F vara sÃ¥dana konstanta matriser att multiplikationerna Ax och F v samt summan

Ax + F v Ã¤r denierade. DÃ¥ Ã¤r summan Ax + F v en normalfÃ¶rdelad stokastisk
vektor med vÃ¤ntevÃ¤rdet A E(x) + F E(v) och kovariansmatrisen

APx AT + F Pv F T .
24

KAPITEL 3.

Bevis.

OPTIMAL STOKASTISK ESTIMERING

Vi kan skriva

(ï¸‚

Ax + F v = A F
och dÃ¥

x

v

och

(ï¸„ )ï¸„
)ï¸‚ x
v

Ã¤r oberoende sÃ¥ fÃ¶ljer enligt pÃ¥stÃ¥ende

gaussisk och sÃ¥ledes Ã¤r

Ax + F v

Tuomas Virtanen

2

i sats B.20 att

( xv )

Ã¤r

gaussisk. VÃ¤ntevÃ¤rdet fÃ¥s direkt pÃ¥ grund av

lineariteten av vÃ¤ntevÃ¤rdet.
Kovariansmatrisen blir, tack vare att

x

och

v

Ã¤r oberoende,

)ï¸
(ï¸
E (Ax + F v âˆ’ E(Ax + F v))(Ax + F v âˆ’ E(Ax + F v))T
(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚
= E A(x âˆ’ E(x)) + F (v âˆ’ E(v)) A(x âˆ’ E(x)) + F (v âˆ’ E(v))
)ï¸
)ï¸
(ï¸
(ï¸
= E A(x âˆ’ E(x))(x âˆ’ E(x))T AT + E F (v âˆ’ E(v))(v âˆ’ E(v))T F T
= APx AT + F Pv F T .

Lemma 3.3.

LÃ¥t xn vara tillstÃ¥ndsvektorn, yn vara mÃ¤tsignalen och un vara

reglersignalen enligt

(3.1)(3.2)

fÃ¶r n = 0, . . . , k , dÃ¤r x0 , v0 , . . . , vk , w0 , . . . , wk

Ã¤r sinsemellan oberoende gaussiska vektorer. Anta att reglersignalen un Ã¤r en
funktion av tillstÃ¥ndet xn och mÃ¤tsignalen yn fÃ¶r varje n = 0, . . . , k . DÃ¥ gÃ¤ller
fÃ¶ljande pÃ¥stÃ¥enden:
1. vk Ã¤r oberoende av xn fÃ¶r varje n = 0, . . . , k ,
2. wk Ã¤r oberoende av xn fÃ¶r varje n = 0, . . . , k ,
3. vk Ã¤r oberoende av yn fÃ¶r varje n = 0, . . . , k .
Bevis.

LÃ¶sningen (3.3) ger att vektorn

v0 , . . . , vnâˆ’1 .

Enligt antagandet Ã¤r

xn

Ã¤r en funktion av

un = f (xn , yn )

av

wn .

eller med insÃ¤ttning av

x0 , u0 , . . . , unâˆ’1 , v0 , . . . , vnâˆ’1

lÃ¶sningen (3.3) och ekvation (3.2) en funktion av
och

Upprepad anvÃ¤ndning av antagandena ger att

x0 , v0 , . . . , vnâˆ’1 , w0 , . . . , wn .

x0 , v0 , . . . , vnâˆ’1 , w0 , . . . , wnâˆ’1 .
av

xn

dÃ¥

yn = Cxn + Gwn

fÃ¶r varje

AnmÃ¤rkning

Vidare fÃ¥s ocksÃ¥ att

xn

un

fÃ¶ljer att

Ã¤r oberoende av

vk

xn

Ã¤r oberoende av

yn

Ã¤r en funktion

Ã¤r en funktion av

Enligt antagandena fÃ¥s nu att

n = 0, . . . , k , wk

x0 , u0 , . . . , unâˆ’1 ,

fÃ¶r varje
fÃ¶r varje

vk

Ã¤r oberoende

n = 0, . . . , k ,

och

n = 0, . . . , k .

3.4. PÃ¥ grund av lineariteten hos systemet (3.1)(3.2) och lem-

man 3.2 och 3.3 fÃ¶ljer att
att processerna

{xk }kâ‰¥0

xn

samt

och

yn

Ã¤r gaussiska fÃ¶r varje

{yk }kâ‰¥0

n â‰¥ 0.

Ã¤r gaussiska processer.

25

Detta betyder

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

3.2 Estimering med Kalmanltret
Ofta Ã¤r bara en del av tillstÃ¥ndsvariablerna tillgÃ¤ngliga och mÃ¤tningen av dem
pÃ¥verkas dessutom av mÃ¤tfel eller mÃ¤tbrus. Detta ger upphov till ett estimeringsproblem att bestÃ¤mma den bÃ¤sta skattningen fÃ¶r hela tillstÃ¥ndsvektorn, utgÃ¥ende frÃ¥n den tillgÃ¤ngliga informationen om systemet. Ett nytt sÃ¤tt att lÃ¶sa detta
problem presenterades av den ungerskfÃ¶dda amerikanska matematikern Rudolf
E. Kalman i sin mycket inytelserika artikel [Kal60]. LÃ¶sningen har senare fÃ¥tt
namnet

Kalmanltret.

I denna avhandling ges en tvÃ¥delad formel av skattningen, Kalmanprediktorn
och Kalmankorrektorn. IdÃ©n med uppdelningen Ã¤r att systemets tillstÃ¥nd kan
estimeras av prediktorn vid varje tidpunkt Ã¤ven om mÃ¤tningar saknas ibland. DÃ¥
en ny mÃ¤tning Ã¤r tillgÃ¤nglig ger korrektorn en fÃ¶rbÃ¤ttrad skattning av tillstÃ¥ndet.
Prediktiorn Ã¤r en

a priori-skattning

eftersom den bestÃ¤ms

det nuvarande tillstÃ¥ndet har gjorts. Korrektorn Ã¤r en
Skattningen av tillstÃ¥ndet betecknas med

Ì‚
x

fÃ¶re

en mÃ¤tning av

a posteriori-skattning.

och kan bestÃ¤mmas enligt olika

kriterier. Ett sÃ¤tt att bestÃ¤mma skattningen Ã¤r sÃ¥ att det kvadratiska medelfelet

Ì‚ 2 ) minimeras. Detta ger en sÃ¥ kallad MMSE-skattning
E(âˆ¥x âˆ’ xâˆ¥

(eng. minimum

mean square error) och enligt fÃ¶ljande lemma ges denna skattning av det betingade vÃ¤ntevÃ¤rdet. Lemmat Ã¤r baserat pÃ¥ den fÃ¶r skalÃ¤ra fallet [Ros10, Proposition
6.1, s. 349] och skrivs hÃ¤r fÃ¶r reellvÃ¤rda stokastiska vektorer.

Lemma 3.5.

LÃ¥t x och y vara stokastiska vektorer pÃ¥ samma sannolikhetsrum

och lÃ¥t Ïƒ(y) vara Ïƒ -algebran genererad av y . DÃ¥ Ã¤r

Ì‚ := E(x | Ïƒ(y))
x

(3.4)

en Ïƒ(y)-mÃ¤tbar stokastisk vektor som uppfyller

)ï¸
(ï¸
)ï¸
(ï¸
Ì‚ 2 ,
min E âˆ¥x âˆ’ zâˆ¥2 = E âˆ¥x âˆ’ xâˆ¥

(3.5)

z

dÃ¤r z Ã¤r Ïƒ(y)-mÃ¤tbar.
Bevis.

FÃ¶r en godtycklig

Ïƒ(y)-mÃ¤tbar

stokastisk vektor

z

gÃ¤ller att

âˆ¥x âˆ’ zâˆ¥2 = âˆ¥x âˆ’ E(x | Ïƒ(y)) + E(x | Ïƒ(y)) âˆ’ zâˆ¥2
= âˆ¥x âˆ’ E(x | Ïƒ(y))âˆ¥2 + âˆ¥E(x | Ïƒ(y)) âˆ’ zâˆ¥2
(ï¸
)ï¸T (ï¸
)ï¸
+ 2 E(x | Ïƒ(y)) âˆ’ z
x âˆ’ E(x | Ïƒ(y)) .
26

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

DÃ¥ Ã¤r

(ï¸
)ï¸
(ï¸
)ï¸
(ï¸
)ï¸
E âˆ¥x âˆ’ zâˆ¥2 = E âˆ¥x âˆ’ E(x | Ïƒ(y))âˆ¥2 + E âˆ¥E(x | Ïƒ(y)) âˆ’ zâˆ¥2
(ï¸‚(ï¸
)ï¸T (ï¸
)ï¸)ï¸‚
+ 2 E E(x | Ïƒ(y)) âˆ’ z
x âˆ’ E(x | Ïƒ(y)) .

(3.6)

Vi kan anvÃ¤nda tredje egenskapen i sats B.18 fÃ¶r den sista termen

E

(ï¸‚(ï¸

)ï¸T (ï¸

)ï¸)ï¸‚
E(x | Ïƒ(y)) âˆ’ z
x âˆ’ E(x | Ïƒ(y))
(ï¸ƒ (ï¸‚
)ï¸‚)ï¸ƒ
(ï¸
)ï¸T (ï¸
)ï¸ âƒ“
âƒ“
= E E E(x | Ïƒ(y)) âˆ’ z
x âˆ’ E(x | Ïƒ(y)) Ïƒ(y)

och eftersom

(ï¸

E(x | Ïƒ(y)) âˆ’ z

)ï¸T

Ã¤r

Ïƒ(y)-mÃ¤tbar fÃ¥s enligt egenskap 5 i sats B.18

att

(ï¸ƒ
E

(ï¸‚(ï¸

)ï¸T (ï¸

)ï¸‚
)ï¸ âƒ“
x âˆ’ E(x | Ïƒ(y)) âƒ“ Ïƒ(y)

)ï¸ƒ

E(x | Ïƒ(y)) âˆ’ z
(ï¸‚(ï¸
)ï¸T (ï¸
)ï¸)ï¸‚
= E E(x | Ïƒ(y)) âˆ’ z E x âˆ’ E(x | Ïƒ(y)) | Ïƒ(y)
(ï¸‚(ï¸
)ï¸T (ï¸
)ï¸)ï¸‚
= E E(x | Ïƒ(y)) âˆ’ z
E(x | Ïƒ(y)) âˆ’ E(E(x | Ïƒ(y)) Â· 1 | Ïƒ(y))
(ï¸‚(ï¸
)ï¸T (ï¸
)ï¸)ï¸‚
= E E(x | Ïƒ(y)) âˆ’ z
E(x | Ïƒ(y)) âˆ’ E(x | Ïƒ(y))

E

= 0.
Den andra termen i (3.6) Ã¤r icke-negativ och dÃ¥ fÃ¥s fÃ¶r varje

Ïƒ(y)-mÃ¤tbar z

att

(ï¸
)ï¸
(ï¸
)ï¸
E âˆ¥x âˆ’ zâˆ¥2 â‰¥ E âˆ¥x âˆ’ E(x | Ïƒ(y))âˆ¥2 ,
dÃ¤r likhet gÃ¤ller dÃ¥ och endast dÃ¥

z = E(x | Ïƒ(y)).

SÃ¥ledes har vi visat pÃ¥stÃ¥en-

det (3.5), det vill sÃ¤ga det betingade vÃ¤ntevÃ¤rdet (3.4) minimerar det kvadratiska
medelfelet.
Enligt fÃ¶ljande sats gÃ¤ller att om
Ã¤ven den betingade fÃ¶rdelningen fÃ¶r

x och y

Ã¤r simultant normalfÃ¶rdelade sÃ¥ Ã¤r

x

y

givet

gaussisk. VÃ¤ntevÃ¤rdet och kova-

riansmatrisen bestÃ¤mmer entydigt fÃ¶rdelningen fÃ¶r en gaussisk vektor och dessa
ger explicita formler fÃ¶r att berÃ¤kna skattningen (3.4) som minimerar det kvadratiska medelfelet.

27

KAPITEL 3.

Sats 3.6.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

LÃ¥t x âˆˆ Rn och y âˆˆ Rp vara normalfÃ¶rdelade stokastiska vektorer

sÃ¥dana att deras simultana fÃ¶rdelning Ã¤r gaussisk. Anta att kovariansmatrisen

Py > 0. DÃ¥ Ã¤r den betingade fÃ¶rdelningen fÃ¶r x givet y gaussisk och det betingade
vÃ¤ntevÃ¤rdet ges av

(ï¸
)ï¸
E(x | Ïƒ(y)) = E(x) + Px,y Pyâˆ’1 y âˆ’ E(y) .

(3.7)

Den betingade kovariansmatrisen Ã¤r

Bevis.

dÃ¤r

Cov(x | Ïƒ(y)) = Px âˆ’ Px,y Pyâˆ’1 Py,x .

(3.8)

z = C1 x + C2 y,

(3.9)

SÃ¤tt

C1

och

C2

Vidare gÃ¤ller att

y

och

z

z
x

och

y

normalfÃ¶rdelad enligt lemma 3.2.

Ã¤r simultant normalfÃ¶rdelade eftersom

(ï¸„ )ï¸„
y

och

z

Ã¤r konstanta matriser. DÃ¥ Ã¤r

(ï¸„
=

0

I

)ï¸„ (ï¸„ )ï¸„
x
y

C1 C2

antas vara simultant normalfÃ¶rdelade.

Vi vill nu bestÃ¤mma

C1

och

C2

sÃ¥ att

y

och

z

Ã¤r oberoende, vilket fÃ¶r nor-

malfÃ¶rdelade vektorer Ã¤r ekvivalent med att de Ã¤r okorrelerade enligt pÃ¥stÃ¥ende

3

i sats B.20. SÃ¥ledes

0 = Cov(z, y)
(ï¸
)ï¸
= E (z âˆ’ E(z))(y âˆ’ E(y))T
(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚
= E C1 (x âˆ’ E(x)) + C2 (y âˆ’ E(y)) y âˆ’ E(y)
(ï¸
)ï¸
(ï¸
)ï¸
= C1 E (x âˆ’ E(x))(y âˆ’ E(y))T + C2 E (y âˆ’ E(y))(y âˆ’ E(y))T
= C1 Px,y + C2 Py .
Eftersom vi antar att

Py > 0 sÃ¥ kan vi multiplicera med inversen Pyâˆ’1

frÃ¥n hÃ¶ger

och fÃ¥r att

C2 = âˆ’C1 Px,y Pyâˆ’1 ,
dÃ¤r

C1

kan vÃ¤ljas godtyckligt. Vi kan vÃ¤lja

C1 = I

och dÃ¥ Ã¤r

C2 = âˆ’Px,y Pyâˆ’1 .

InsÃ¤ttning i (3.9) ger att

z = x âˆ’ Px,y Pyâˆ’1 y
28

(3.10)

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Ã¤r nu en vektor som Ã¤r oberoende av

y.

Tuomas Virtanen

Vi kan nu bestÃ¤mma det betingade

vÃ¤ntevÃ¤rdet och kovariansen fÃ¶r vektorn

x = z + Px,y Pyâˆ’1 y.
Eftersom

z

Ã¤r oberoende av

y

och

y

Ã¤r

Ïƒ(y)-mÃ¤tbar

har vi att

E(x | Ïƒ(y)) = E(z + Px,y Pyâˆ’1 y | Ïƒ(y))
= E(z | Ïƒ(y)) + Px,y Pyâˆ’1 E(y | Ïƒ(y))
= E(z) + Px,y Pyâˆ’1 y,
Vidare med insÃ¤ttning av (3.10) fÃ¥s att

E(z) + Px,y Pyâˆ’1 y = E(x âˆ’ Px,y Pyâˆ’1 y) + Px,y Pyâˆ’1 y
= E(x) âˆ’ Px,y Pyâˆ’1 E(y) + Px,y Pyâˆ’1 y
= E(x) + Px,y Pyâˆ’1 (y âˆ’ E(y)),
vilket visar (3.7).
Eftersom

z

Ã¤r bestÃ¤md sÃ¥ att

betingade kovariansmatrisen fÃ¶r

x

Cov(z, y) = 0

och

y

Ã¤r

Ïƒ(y)-mÃ¤tbar

fÃ¥s den

med hjÃ¤lp av sats B.18 som

Cov(x | Ïƒ(y)) = Cov(z + Px,y Pyâˆ’1 y | Ïƒ(y))
= Cov(z | Ïƒ(y)) + Px,y Pyâˆ’1 Cov(y, z | Ïƒ(y))
+ Cov(z, y | Ïƒ(y))Pyâˆ’1 Py,x + Px,y Pyâˆ’1 Cov(y | Ïƒ(y))Pyâˆ’1 Py,x
= Cov(z | Ïƒ(y))
= Cov(z).
Vidare har vi med insÃ¤ttning av (3.10) och enligt lemma B.16 att

Cov(z) = Cov(x âˆ’ Px,y Pyâˆ’1 y)
= Cov(x) âˆ’ Px,y Pyâˆ’1 Cov(y, x)
âˆ’ Cov(x, y)Pyâˆ’1 Py,x + Px,y Pyâˆ’1 Cov(y)Pyâˆ’1 Py,x
= Px âˆ’ Px,y Pyâˆ’1 Py,x âˆ’ Px,y Pyâˆ’1 Py,x + Px,y Pyâˆ’1 Py Pyâˆ’1 Py,x
= Px âˆ’ Px,y Pyâˆ’1 Py,x ,
vilket visar (3.8).

29

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Enligt anmÃ¤rkning 3.4 Ã¤r tillstÃ¥ndsvektorn

Gwk

gaussiska fÃ¶r varje

k â‰¥ 0.

xk

Tuomas Virtanen

och mÃ¤tningen

Eftersom mÃ¤tbruset

wk

yk = Cxk +

Ã¤r gaussiskt och obero-

xk enligt lemma 3.3 sÃ¥ fÃ¶ljer att vektorn
(ï¸„ )ï¸„ (ï¸„
)ï¸„ (ï¸„
)ï¸„ (ï¸„ )ï¸„
xk
xk
I 0
xk
=
=
yk
Cxk + Gwk
C G
wk

ende av tillstÃ¥ndet

Ã¤r ocksÃ¥ gaussisk. TillstÃ¥ndsvektorn

xk

och mÃ¤tningen

yk

Ã¤r alltsÃ¥ simultant

gaussiska och dÃ¥ Ã¤r enligt sats 3.6 den betingade fÃ¶rdelningen fÃ¶r
gaussisk. DÃ¥ en mÃ¤tning av

yk

xk

givet

yk

har gjorts kan det betingade vÃ¤ntevÃ¤rdet (3.7)

berÃ¤knas och kovariansmatrisen fÃ¥s genom (3.8).

l

infÃ¶rs be-

P0

fÃ¶r start-

FÃ¶r mÃ¤ngden av tillgÃ¤nglig information om systemet vid tiden
teckningen
tillstÃ¥ndet

Yl ,
x0

vilket innefattar vÃ¤ntevÃ¤rdet
samt alla

tillgÃ¤ngliga

Observera att fÃ¶r mÃ¤ngden
tidpunkt
sÃ¤ga

l = âˆ’1

Yk

m0

mÃ¤tningar

och kovariansen

yi

upp till och med tidpunkt

behÃ¶ver det inte gÃ¤lla att mÃ¤tningen

l.

yk âˆˆ Yk . Vid

Ã¤r endast information om starttillstÃ¥ndet tillgÃ¤ngligt, det vill

Yâˆ’1 = {m0 , P0 }. A priori - och a posteriori -skattningarna kan nu denieras

enligt fÃ¶ljande.

Denition 3.7.

Prediktionen

vet informationen

2

Ì‚ k âˆ¥ ).
E(âˆ¥xk âˆ’ x

Yl ,

dÃ¤r

vid tiden

l < k,

k

Ã¤r skattningen av tillstÃ¥ndet

xk

gi-

som minimerar det kvadratiska medelfelet

Prediktionen betecknas med

Ì‚ k|l
x

och ges av det betingade vÃ¤n-

tevÃ¤rdet, alltsÃ¥

Ì‚ k|l := E(xk | Yl ),
x

l < k.

(3.11)

Den tillhÃ¶rande betingade kovariansmatrisen betecknas med

Pk|l := Cov(xk | Yl ),
AnmÃ¤rkning
tillstÃ¥ndet

l < k.

(3.12)

3.8. Det fÃ¶ljer direkt frÃ¥n denna denition att prediktionen av start-

x0

Ã¤r

Ì‚ 0|âˆ’1 = m0 ,
x

P0|âˆ’1 = P0 .

(3.13)

Dessa Ã¤r startvÃ¤rden fÃ¶r bÃ¥de prediktorn och korrektorn.
FÃ¶r korrektionen gÃ¤ller utÃ¶ver prediktionen att mÃ¤tningen vid nuvarande tidpunkt Ã¤r tillgÃ¤nglig och denna mÃ¤tning skrivs dÃ¤rfÃ¶r ut explicit i det betingade
vÃ¤ntevÃ¤rdet och kovariansen.

30

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Denition 3.9.

Yl ,

informationen
ka medelfelet

Korrektionen
dÃ¤r

l<k
2

vid tiden

k

Ã¤r skattningen av tillstÃ¥ndet

samt mÃ¤tningen

Ì‚ k âˆ¥ ).
E(âˆ¥xk âˆ’ x

Tuomas Virtanen

yk ,

xk

givet

som minimerar det kvadratis-

Korrektionen betecknas med

Ì‚ k|k
x

och ges av det

betingade vÃ¤ntevÃ¤rdet, alltsÃ¥

Ì‚ k|k := E(xk | Yl , yk ),
x

l < k.

(3.14)

Den tillhÃ¶rande betingade kovariansmatrisen betecknas med

Pk|k := Cov(xk | Yl , yk ),
FÃ¶r

k â‰¥ 1

l < k.

(3.15)

fÃ¥s fÃ¶ljande resultat fÃ¶r prediktionen av tillstÃ¥ndet

xk

som en

rekursion.

Sats 3.10 (Kalmanprediktorn). LÃ¥t Yl , l < k vara den tillgÃ¤ngliga informationen
och ukâˆ’1 = f (Yl ) vara den kÃ¤nda delen av reglersignalen u i tidsintervallet [k âˆ’

Ì‚ kâˆ’1|l vara en given optimal skattning av tillstÃ¥ndet vid
1 âˆ’ âˆ†t, k âˆ’ 1). Vidare lÃ¥t x
tidpunkt k âˆ’ 1 och Pkâˆ’1|l vara den givna tillhÃ¶rande kovariansmatrisen.
DÃ¥ fÃ¥s prediktionen av tillstÃ¥ndet xk , k â‰¥ 1, fÃ¶r systemet

Ì‚ k|l = Ax
Ì‚ kâˆ’1|l + Bukâˆ’1 ,
x

(3.1)

enligt

l â‰¤kâˆ’1

(3.16)

och den tillhÃ¶rande betingade kovariansmatrisen ges av

Pk|l = APkâˆ’1|l AT + F Pv F T ,

(3.17)

dÃ¤r Pv Ã¤r kovariansmatrisen fÃ¶r processbruset.
Bevis.

Det stokastiska LTI-systemets tillstÃ¥ndsekvation (3.1) kan ocksÃ¥ skrivas

xk = Axkâˆ’1 + Bukâˆ’1 + F vkâˆ’1

(3.18)

och insÃ¤ttning av (3.18) i denitionen fÃ¶r prediktionen (3.11) ger

Ì‚ k|l = E(xk | Yl )
x
= E(Axkâˆ’1 + Bukâˆ’1 + F vkâˆ’1 | Yl )
= A E(xkâˆ’1 | Yl ) + B E(ukâˆ’1 | Yl ) + F E(vkâˆ’1 | Yl ).
Eftersom

vkâˆ’1

ukâˆ’1 = f (Yl ) Ã¤r Yl -mÃ¤tbar och som fÃ¶ljd av lemma 3.3 Ã¤r processbruset

oberoende av informationen

Yl

sÃ¥ fÃ¥r vi att

31

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

A E(xkâˆ’1 | Yl ) + B E(ukâˆ’1 | Yl ) + F E(vkâˆ’1 | Yl )
= A E(xkâˆ’1 | Yl ) + Bukâˆ’1 + F E(vkâˆ’1 ).
Vidare dÃ¥

E(vkâˆ’1 ) = 0

och omskrivning med denitionen (3.11) fÃ¥s

Ì‚ kâˆ’1|l + Bukâˆ’1 ,
A E(xkâˆ’1 | Yl ) + Bukâˆ’1 + F E(vkâˆ’1 ) = Ax
vilket visar (3.16).
Den tillhÃ¶rande kovariansmatrisen Ã¤r enligt denitionerna (3.12), (B.9)
och (3.11)

Pk|l = Cov(xk | Yl )
(ï¸‚(ï¸
)ï¸‚
)ï¸(ï¸
)ï¸T
= E xk âˆ’ E(xk | Yl ) xk âˆ’ E(xk | Yl ) | Yl
(ï¸
)ï¸
Ì‚ k|l )(xk âˆ’ x
Ì‚ k|l )T | Yl
= E (xk âˆ’ x
Med insÃ¤ttning av (3.16) och (3.18) fÃ¥s att

(ï¸
)ï¸
Ì‚ k|l )(xk âˆ’ x
Ì‚ k|l )T | Yl
E (xk âˆ’ x
(ï¸‚(ï¸
)ï¸‚
)ï¸(ï¸
)ï¸T
Ì‚ kâˆ’1|l ) + F vkâˆ’1 A(xkâˆ’1 âˆ’ x
Ì‚ kâˆ’1|l ) + F vkâˆ’1 | Yl .
= E A(xkâˆ’1 âˆ’ x
AnvÃ¤ndning av lemma 3.3 som sÃ¤ger att
informationen

E

Yl

vkâˆ’1

Ã¤r oberoende av

xkâˆ’1

samt av

ger att

(ï¸‚(ï¸
)ï¸‚
)ï¸(ï¸
)ï¸T
Ì‚ kâˆ’1|l ) + F vkâˆ’1 A(xkâˆ’1 âˆ’ x
Ì‚ kâˆ’1|l ) + F vkâˆ’1 | Yl
A(xkâˆ’1 âˆ’ x
(ï¸
)ï¸
T )F T .
Ì‚ kâˆ’1|l )(xkâˆ’1 âˆ’ x
Ì‚ kâˆ’1|l )T | Yl AT + F E(vkâˆ’1 vkâˆ’1
= A E (xkâˆ’1 âˆ’ x

Slutligen ger en omskrivning med hjÃ¤lp av denitionerna att

(ï¸
)ï¸
T )F T
Ì‚ kâˆ’1|l )(xkâˆ’1 âˆ’ x
Ì‚ kâˆ’1|l )T | Yl AT + F E(vkâˆ’1 vkâˆ’1
A E (xkâˆ’1 âˆ’ x
= A Cov(xkâˆ’1 | Yl )AT + F Cov(vkâˆ’1 )F T
= APkâˆ’1|l AT + F Pv F T ,
vilket visar (3.17).

32

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

FÃ¶r att kunna anvÃ¤nda prediktionen (3.16) i sats 3.10 fÃ¶r att bestÃ¤mma
korrektionen (3.14) behÃ¶vs fÃ¶rst nÃ¥gra hjÃ¤lpresultat. FÃ¶ljande lemma Ã¤r enligt [Ã…st70, Sats 7.3.3, s. 220].

Lemma 3.11.

LÃ¥t x, y1 och y2 vara simultant normalfÃ¶rdelade stokastiska vek-

torer. Anta att y1 och y2 Ã¤r oberoende och att kovariansmatriserna Py1 och Py2
Ã¤r positivt denita. DÃ¥ gÃ¤ller att

Bevis.

LÃ¥t

E(x | y1 , y2 ) = E(x | y1 ) + E(x | y2 ) âˆ’ E(x).
(ï¸„ )ï¸„
(ï¸„
)ï¸„
y1
E(y1 )
y=
, dÃ¥ Ã¤r E(y) =
och vi fÃ¥r att
y2
E(y2 )

(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚
x âˆ’ E(x) y âˆ’ E(y)
â›
(ï¸„
)ï¸„T â
(ï¸
)ï¸ y1 âˆ’ E(y1 )
â 
= E â x âˆ’ E(x)
y2 âˆ’ E(y2 )
(ï¸‚ (ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚ (ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚)ï¸‚
= E x âˆ’ E(x) y1 âˆ’ E(y1 )
, E x âˆ’ E(x) y2 âˆ’ E(y2 )
(ï¸‚
)ï¸‚
= Px,y1 Px,y2 .

Px,y = E

Eftersom

y1

och

y2

antas vara oberoende ges kovariansmatrisen fÃ¶r

(ï¸„
Py =

P y1

0

0

P y2

av

)ï¸„
,

observera att denna matris Ã¤r inverterbar eftersom vi antar att

Py2 > 0.

y

P y1 > 0

och

Sats 3.6 ger nu att

E(x | y1 , y2 ) = E(x | y)
(ï¸
)ï¸
= E(x) + Px,y Pyâˆ’1 y âˆ’ E(y)
(ï¸„
)ï¸„ (ï¸„
)ï¸„
(ï¸‚
)ï¸‚ P âˆ’1
0
y1 âˆ’ E(y1 )
y1
= E(x) + Px,y1 Px,y2
0 Pyâˆ’1
y2 âˆ’ E(y2 )
2
(ï¸
)ï¸
(ï¸
)ï¸
= E(x) + Px,y1 Pyâˆ’1
y1 âˆ’ E(y1 ) + Px,y2 Pyâˆ’1
y2 âˆ’ E(y2 )
1
2
(ï¸
)ï¸
(ï¸
)ï¸
âˆ’1
= E(x) + Px,y1 Pyâˆ’1
y
âˆ’
E(y
)
+
E(x)
+
P
P
y
âˆ’
E(y
)
âˆ’ E(x)
1
1
x,y
2
2
2
y
1
2
= E(x | y1 ) + E(x | y2 ) âˆ’ E(x).

33

KAPITEL 3.

Deniera

OPTIMAL STOKASTISK ESTIMERING

skattningsfelet

Ìƒ := x âˆ’ x
Ì‚,
x

enligt

Tuomas Virtanen

dÃ¤r skattningen

Ì‚
x

ges av (3.4).

DÃ¥ kan skattningsfelet fÃ¶r prediktionen respektive korrektionen betecknas med

Ìƒ k|l := xk âˆ’ x
Ì‚ k|l ,
x

l < k,

och

Ìƒ k|k := xk âˆ’ x
Ì‚ k|k .
x

FÃ¶ljande hjÃ¤lpresultat enligt [Ã…st70, s. 218220] ger nÃ¥gra anvÃ¤ndbara egenskaper
fÃ¶r skattningsfelet, bland annat att skattningsfelet

Ìƒ Ã¤r oberoende av mÃ¤tningen
x

y.

Lemma 3.12.

LÃ¥t x och y vara simultant normalfÃ¶rdelade stokastiska vektorer.

Ì‚ = E(x | Ïƒ(y)) och x
Ìƒ =xâˆ’x
Ì‚ . DÃ¥ gÃ¤ller att x
Ìƒ och y Ã¤r oberoende och att
LÃ¥t x
Ìƒ och x
Ì‚ Ã¤r oberoende. Vidare gÃ¤ller att
x
Ìƒ = 0 och
E(x)
Bevis.

Det Ã¤r klart att

Ìƒ = Cov(x | y) = Px|y .
Cov(x)

Ìƒ = E(x) âˆ’ E( E(x | y)) = 0.
E(x)

simultant gaussiska vektorer sÃ¥ Ã¤r ocksÃ¥

Ìƒ
x

och

y

Eftersom

x

och

y

Ã¤r

simultant gaussiska vektorer.

Eftersom de Ã¤r gaussiska rÃ¤cker det att visa att de Ã¤r okorrelerade. Kovariansmatrisen fÃ¶r

Ìƒ
x

och

y

blir med insÃ¤ttning av (3.7)

(ï¸‚ (ï¸
)ï¸T )ï¸‚
Ìƒ y âˆ’ E(y)
Px,y
Ìƒ = E x
(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚
= E x âˆ’ E(x | y) y âˆ’ E(y)
)ï¸ƒ
(ï¸ƒ(ï¸‚
(ï¸
)ï¸)ï¸‚
âˆ’1
T
= E x âˆ’ E(x) + Px,y Py (y âˆ’ E(y)) (y âˆ’ E(y))
(ï¸‚(ï¸
(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚
)ï¸(ï¸
)ï¸T )ï¸‚
= E x âˆ’ E(x) y âˆ’ E(y)
âˆ’ Px,y Pyâˆ’1 E y âˆ’ E(y) y âˆ’ E(y)
= Px,y âˆ’ Px,y Pyâˆ’1 Py
= 0.
SÃ¥ledes Ã¤r

y

och

Ìƒ
x

oberoende. Eftersom

egenskap 2 i sats B.18 sÃ¥ fÃ¶ljer att
och

Ìƒ
x

Ã¤r oberoende av

y

Ì‚
x

Ì‚ = E(x | y)
x

och

Ìƒ
x

Ã¤r en funktion av

Ã¤r oberoende.

Vidare dÃ¥

fÃ¥s att

Ìƒ = E(x
Ìƒx
Ìƒ T ) = E(x
Ìƒx
Ìƒ T | y)
Cov(x)
(ï¸
)ï¸
= E (x âˆ’ E(x | y))(x âˆ’ E(x | y))T | y
= Cov(x | y).

34

y

enligt

Ìƒ =0
E(x)

KAPITEL 3.

AnmÃ¤rkning

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

3.13. Observera att enligt lemma 3.12 fÃ¥s att kovariansmatriserna

fÃ¶r skattningsfelen av prediktionen och korrektionen Ã¤r lika med kovariansmatriserna (3.12) och (3.15), det vill sÃ¤ga

Ìƒ k|l ) = Pk|l ,
Cov(x
FÃ¶r en given prediktion

Ì‚ k|l
yÌ‚k|l := C x

Ì‚ k|l , l < k ,
x

Ìƒ k|k ) = Pk|k .
Cov(x
fÃ¶rvÃ¤ntas utsignalen vid tidpunkt

innovationen

och dÃ¥ denieras

l < k.

Innovationen Ã¤r alltsÃ¥ den nya informationen som mÃ¤tningen

Ì‚k
x

av tillstÃ¥ndet

xk

vara

som

yÌƒk|l := yk âˆ’ yÌ‚k|l ,

ningen

k

(3.19)

yk

ger till skatt-

[Kay93, s. 395]. FÃ¶ljande hjÃ¤lpresultat ger nÃ¥gra

egenskaper fÃ¶r innovationen.

Lemma 3.14.

LÃ¥t {wk }kâ‰¥0 vara gaussiskt vitt brus med vÃ¤ntevÃ¤rdet noll,

Cov(wk ) = Pw > 0 och wk Ã¤r oberoende av x0 fÃ¶r varje k . DÃ¥ gÃ¤ller fÃ¶r
innovationen

(3.19)

att

E(yÌƒk|l ) = 0.
Ì‚ k|l , fÃ¥s kovariansGivet den tillhÃ¶rande kovariansmatrisen Pk|l fÃ¶r prediktionen x
matrisen fÃ¶r innovationen som

PyÌƒk|l = CPk|l C T + GPw GT

(3.20)

och denna kovariansmatris Ã¤r inverterbar dÃ¥ G antas vara surjektiv. Vidare Ã¤r

yÌƒk|l oberoende av alla mÃ¤tningar y0 , . . . , yl , l < k .
Bevis.

Med hjÃ¤lp av ekvationen (3.2) fÃ¶r utsignalen kan vi skriva innovatio-

nen (3.19) som

Ì‚ k|l
yÌƒk|l = Cxk + Gwk âˆ’ C x
Ì‚ k|l ) + Gwk
= C(xk âˆ’ x

(3.21)

Ìƒ k|l + Gwk .
= Cx
Lemma 3.12 ger att vÃ¤ntevÃ¤rdet fÃ¶r skattningsfelet
Ã¤r

E(wk ) = 0,

Ìƒ k|l Ã¤r 0 och enligt antagandet
x

dÃ¥ Ã¤r

Ìƒ k|l ) + G E(wk ) = 0.
E(yÌƒk|l ) = C E(x

35

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Enligt lemma 3.3 Ã¤r

wk

wk

och

xn

oberoende fÃ¶r varje

Ìƒ k|l = xk âˆ’ x
Ì‚ k|l
x

ocksÃ¥ oberoende av

fÃ¶r varje

Tuomas Virtanen

n, 0 â‰¤ n â‰¤ k , och dÃ¤rmed Ã¤r

l < k . DÃ¥ fÃ¥s kovariansmatrisen

fÃ¶r innovationen som

PyÌƒk|l

)ï¸(ï¸
)ï¸T )ï¸‚
= E yÌƒk|l âˆ’ E(yÌƒk|l ) yÌƒk|l âˆ’ E(yÌƒk|l )
(ï¸
)ï¸
Ìƒ k|l + Gwk )(C x
Ìƒ k|l + Gwk )T
= E (C x
(ï¸‚(ï¸

T
T T
Ìƒ k|l x
ÌƒT
= C E(x
k|l )C + G E(wk wk )G

= CPk|l C T + GPw GT .
Vidare ger lemma 3.12 att

Ìƒ k|l
x

Ã¤r oberoende av mÃ¤tningarna

y0 , . . . , yl

och dÃ¥

fÃ¶ljer att innovationen (3.21) ocksÃ¥ Ã¤r oberoende av dessa mÃ¤tningar.
Kovariansmatrisen (3.20) fÃ¶r innovationen Ã¤r inverterbar eftersom

y T (CPk|l C T + GPw GT )y = (C T y)T Pk|l (C T y) + (GT y)T Pw (GT y)
â‰¥ (GT y)T Pw (GT y)
â‰¥ 0,
dÃ¤r likhet gÃ¤ller dÃ¥ och endast dÃ¥
endast dÃ¥

yâ‰¡0

GT y = 0.

PyÌƒk|l > 0

och det fÃ¶ljer att

DÃ¥

G

Ã¤r surjektiv gÃ¤ller detta

och sÃ¥ledes inverterbar.

FÃ¶ljande resultat ger korrektionssteget fÃ¶r Kalmanltret.

Sats 3.15

.

(Kalmankorrektorn)

Ì‚ k|l vara den optimala skattFÃ¶r l < k , lÃ¥t x

ningen av tillstÃ¥ndet xk givet informationen Yl och lÃ¥t Pk|l vara den tillhÃ¶rande
kovariansmatrisen. Dessa ges av Kalmanprediktorn i sats 3.10. LÃ¥t yk vara en
mÃ¤tning vid nuvarande tidpunkt k . DÃ¥ ges korrektionen av

Ì‚ k|k = x
Ì‚ k|l + Kk yÌƒk|l ,
x
dÃ¤r Kk Ã¤r

KalmanfÃ¶rstÃ¤rkningen

(3.22)

denierad enligt

Kk := Pk|l C T (CPk|l C T + GPw GT )âˆ’1
och yÌƒk|l Ã¤r innovationen

(3.19).

(3.23)

Den tillhÃ¶rande kovariansmatrisen Ã¤r

Pk|k = (I âˆ’ Kk C)Pk|l .
Bevis.
av

DÃ¥ prediktionen

(Yl , yk )

enligt

Ì‚ k|l = E(xk | Yl )
x

Ì‚ k|l
yÌƒk|l = yk âˆ’ C x

Ã¤r given sÃ¥ bestÃ¤ms

(3.24)

(Yl , yÌƒk|l )

och dÃ¥ gÃ¤ller fÃ¶ljande likhet

Ì‚ k|k = E(xk | Yl , yk ) = E(xk | Yl , yÌƒk|l ).
x
36

entydigt

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Lemma 3.14 sÃ¤ger att innovationen
kovariansmatrisen

PyÌƒk|l

yÌƒk|l

Tuomas Virtanen

Ã¤r oberoende av informationen

Yl

och

Ã¤r inverterbar. DÃ¥ kan vi anvÃ¤nda lemma 3.11, sats 3.6

och lemma 3.14 fÃ¶r att fÃ¥

E(xk | Yl , yÌƒk|l ) = E(xk | Yl ) + E(xk | yÌƒk|l ) âˆ’ E(xk )
(ï¸
)ï¸
Ì‚ k|l + E(xk ) + Pxk ,yÌƒk|l PyÌƒâˆ’1
Ìƒ
Ìƒ
=x
y
âˆ’
E(
y
)
âˆ’ E(xk )
k|l
k|l
k|l

(3.25)

Ì‚ k|l + Pxk ,yÌƒk|l PyÌƒâˆ’1
=x
yÌƒk|l .
k|l
Lemma 3.14 ger kovariansmatrisen

PyÌƒk|l

och vi behÃ¶ver Ã¤nnu bestÃ¤mma

Pxk ,yÌƒk|l .

Ì‚ k|l + xk âˆ’ x
Ì‚ k|l = x
Ì‚ k|l + x
Ìƒ k|l och eftersom E(yÌƒk|l ) = 0
xk = x
(ï¸‚(ï¸
)ï¸ T )ï¸‚
Pxk ,yÌƒk|l = E xk âˆ’ E(xk ) yÌƒk|l
(ï¸‚(ï¸
)ï¸ T )ï¸‚
Ì‚ k|l + x
Ìƒ k|l âˆ’ E(x
Ì‚ k|l + x
Ìƒ k|l ) yÌƒk|l
=E x
(ï¸‚(ï¸
(ï¸‚(ï¸
)ï¸ T )ï¸‚
)ï¸ T )ï¸‚
Ì‚ k|l âˆ’ E(x
Ì‚ k|l ) yÌƒk|l
Ìƒ k|l âˆ’ E(x
Ìƒ k|l ) yÌƒk|l
.
+E x
=E x

Vi kan skriva

fÃ¥s att

(3.26)

Ì‚ k|l Ã¤r en funktion av Yl
yÌƒk|l oberoende
av Yl och eftersom x
(ï¸‚(ï¸
)ï¸ T )ï¸‚
Ìƒ k|l ) = 0 och dÃ¥ Ã¤r
Ì‚ k|l âˆ’ E(x
Ì‚ k|l ) yÌƒk|l = 0. Enligt lemma 3.12 Ã¤r E(x
E x

Enligt lemma 3.14 Ã¤r
fÃ¥s att

E

(ï¸‚(ï¸
(ï¸‚(ï¸
(ï¸
)ï¸ T )ï¸‚
)ï¸ T )ï¸‚
T )ï¸.
Ìƒ k|l yÌƒk|l
Ì‚ k|l âˆ’ E(x
Ì‚ k|l ) yÌƒk|l
Ìƒ k|l âˆ’ E(x
Ìƒ k|l ) yÌƒk|l
=E x
+E x
x
wk

Enligt lemma 3.3 Ã¤r
Ã¤r

wk

och

ocksÃ¥ oberoende av

xn

oberoende fÃ¶r varje

Ìƒ k|l = xk âˆ’ x
Ì‚ k|l
x

fÃ¶r varje

n, 0 â‰¤ n â‰¤ k ,

l < k.

(3.27)

och dÃ¤rmed

Nu fÃ¥s med insÃ¤ttning

av (3.21) i (3.27) att

(ï¸
)ï¸
T )ï¸ = E (ï¸x
Ìƒ k|l yÌƒk|l
Ìƒ k|l (C x
Ìƒ k|l + Gwk )T
E x
T
Ìƒ k|l x
ÌƒT
Ìƒ k|l wkT )GT
= E(x
k|l )C + E(x

(3.28)

= Pk|l C T .
InsÃ¤ttning av (3.20) och (3.28) i (3.25) ger att

Ì‚ k|k = x
Ì‚ k|l + Pk|l C T (CPk|l C T + GPw GT )âˆ’1 yÌƒk|l
x
Ì‚ k|l + Kk yÌƒk|l .
=x
Den tillhÃ¶rande kovariansmatrisen blir enligt (3.22), (3.21), lemma 3.12 och an-

37

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

mÃ¤rkning 3.13

Ìƒ k|k )
Pk|k = Cov(x
)ï¸
(ï¸
Ì‚ k|k )(xk âˆ’ x
Ì‚ k|k )T
= E (xk âˆ’ x
(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚
Ì‚
Ìƒ
Ì‚
Ìƒ
= E xk âˆ’ (xk|l + Kk yk|l ) xk âˆ’ (xk|l + Kk yk|l )
(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚
Ìƒ k|l âˆ’ Kk (C x
Ìƒ k|l + Gwk ) x
Ìƒ k|l âˆ’ Kk (C x
Ìƒ k|l + Gwk )
=E x
(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T )ï¸‚
Ìƒ k|l âˆ’ Kk Gwk (I âˆ’ Kk C)x
Ìƒ k|l âˆ’ Kk Gwk
= E (I âˆ’ Kk C)x

(3.29)

T
T
T
Ìƒ k|l x
ÌƒT
= (I âˆ’ Kk C) E(x
k|l )(I âˆ’ Kk C) + Kk G E(wk wk )(Kk G)

= (I âˆ’ Kk C)Pk|l (I âˆ’ Kk C)T + Kk GPw GT KkT .
Vidare kan vi fÃ¶renkla detta uttryck enligt

(I âˆ’ Kk C)Pk|l (I âˆ’ Kk C)T + Kk GPw GT KkT
= (I âˆ’ Kk C)Pk|l âˆ’ (I âˆ’ Kk C)Pk|l C T KkT + Kk GPw GT KkT
= (I âˆ’ Kk C)Pk|l âˆ’ Pk|l C T KkT + Kk CPk|l C T KkT + Kk GPw GT KkT
= (I âˆ’ Kk C)Pk|l + (Kk CPk|l C T + Kk GPw GT âˆ’ Pk|l C T )KkT
(ï¸
)ï¸
= (I âˆ’ Kk C)Pk|l + Kk (CPk|l C T + GPw GT ) âˆ’ Pk|l C T KkT
= (I âˆ’ Kk C)Pk|l + (Pk|l C T âˆ’ Pk|l C T )KkT
= (I âˆ’ Kk C)Pk|l ,
vilket visar (3.24).

AnmÃ¤rkning

3.16. Observera att

Kk CPk|l = Pk|l C T (CPk|l C T + GPw GT )âˆ’1 CPk|l â‰¥ 0
och dÃ¥ Ã¤r

Pk|k = Pk|l âˆ’ Kk CPk|l â‰¤ Pk|l .
Den tillhÃ¶rande kovariansmatrisen fÃ¶r korrektionen Ã¤r alltsÃ¥ mindre Ã¤n den tillhÃ¶rande kovariansmatrisen fÃ¶r prediktionen.
Kovariansmatrisen pÃ¥ sista raden i (3.29) Ã¤r i

Josephs form

och den Ã¤r att

Pk|k

Ã¤r symmetrisk.

fÃ¶redra i numeriska berÃ¤kningar eftersom den sÃ¤kerstÃ¤ller att

Avrundningsfel kan fÃ¶rorsaka att (3.24) inte Ã¤r positivt denit, vilket krÃ¤vs av
teorin. En anmÃ¤rkningsvÃ¤rd egenskap fÃ¶r prediktionen och korrektionen samt
deras tillhÃ¶rande kovariansmatriser Ã¤r att de kan ges som rekursiva formler, enligt

38

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

satserna 3.10 och 3.15. SÃ¥ledes behÃ¶vs alltsÃ¥ endast den fÃ¶regÃ¥ende optimala
skattningen fÃ¶r att berÃ¤kna nÃ¤sta optimala skattning och tidigare skattningar
ger ingen ytterligare information. Detta Ã¤r den sÃ¥ kallade

Markovegenskapen

fÃ¶r

stokastiska processer.

3.3 StationÃ¤rt Kalmanlter
Som pÃ¥pekades i anmÃ¤rkning 3.8 behÃ¶vs

E(x0 ) = m0

och

Cov(x0 ) = P0

som

startvÃ¤rden fÃ¶r att kunna rekursivt berÃ¤kna den optimala skattningen av tillstÃ¥ndet. Under vissa villkor konvergerar kovariansmatrisen och sÃ¥ledes KalmanfÃ¶rstÃ¤rkningen till stationÃ¤ra vÃ¤rden, som Ã¤r oberoende av startvÃ¤rdet

P0 .

Den

tillhÃ¶rande kovariansmatrisen fÃ¶r korrektorn enligt (3.24) Ã¤r

Pk|k = (I âˆ’ Kk C)Pk|kâˆ’1 .
Med insÃ¤ttning av KalmanfÃ¶rstÃ¤rkningen (3.23) fÃ¥s att

Pk|k = Pk|kâˆ’1 âˆ’ Pk|kâˆ’1 C T (CPk|kâˆ’1 C T + GPw GT )âˆ’1 CPk|kâˆ’1 .
Prediktorns kovariansmatris (3.17) vid tidpunkt

k+1

(3.30)

Ã¤r

Pk+1|k = APk|k AT + F Pv F T .
Beteckna

Î£k := Pk+1|k

och lÃ¥t

Pv = J J T

fÃ¶r nÃ¥gon

(3.31)

nÃ—q

matris

J.

DÃ¥ ger

insÃ¤ttning av (3.30) i (3.31) att

(ï¸
)ï¸
Î£k = A Pk|kâˆ’1 âˆ’ Pk|kâˆ’1 C T (CPk|kâˆ’1 C T + GPw GT )âˆ’1 CPk|kâˆ’1 AT + F Pv F T
= AÎ£kâˆ’1 AT + F JJ T F T âˆ’ AÎ£kâˆ’1 C T (CÎ£kâˆ’1 C T + GPw GT )âˆ’1 CÎ£kâˆ’1 AT .
(3.32)
Detta liknar Riccatiekvationen (2.29) fÃ¶r deterministisk optimal reglering och ett
liknande resultat fÃ¥s fÃ¶r konvergens. Resultatet fÃ¶ljer direkt frÃ¥n sats 2.23.

Sats 3.17.

Anta att Pv â‰¥ 0, Pw > 0, G surjektiv, matrisparet (C, A) Ã¤r de-

tekterbart och matrisparet (A, F J ) Ã¤r stabiliserbart, dÃ¤r J J T = Pv . Beteckna
lÃ¶sningen vid tidpunkt k = N till

(3.32),

startad frÃ¥n Î£0 , med Î£N (Î£0 ) och lÃ¥t

K(Î£) := Î£C T (CÎ£C T + GPw GT )âˆ’1 .
DÃ¥ gÃ¤ller fÃ¶ljande pÃ¥stÃ¥enden:
39

(3.33)

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

Det nns en entydig matris Î£âˆ’ â‰¥ 0, oberoende av Î£0 , sÃ¥ att

lim Î£N (Î£0 ) = Î£âˆ’ .

N â†’âˆ

Matrisen Î£âˆ’ satiserar den algebraiska Riccatiekvationen

Î£ = AÎ£AT + F Pv F T âˆ’ AÎ£C T (CÎ£C T + GPw GT )âˆ’1 CÎ£AT

(3.34)

och matrisen Aâˆ’AK(Î£âˆ’ )C Ã¤r stabil. Vidare gÃ¤ller att om matrisparet (A, F J )
Ã¤r styrbart sÃ¥ Ã¤r Î£âˆ’ > 0.
Eftersom matrisen

Î£âˆ’

Ã¤r entydig sÃ¥ kan vi beteckna den tidsinvarianta Kal-

manfÃ¶rstÃ¤rkningen med

K := K(Î£âˆ’ ) = Î£âˆ’ C T (CÎ£âˆ’ C T + GPw GT )âˆ’1
och det

stationÃ¤ra Kalmanltret

ges av

Ì‚ k+1 = Ax
Ì‚ k + Buk + AK yÌƒk ,
x
dÃ¤r

(3.35)

(3.36)

Ì‚ k . Detta system har en liknande form som LuenbergerobservatÃ¶yÌƒk = yk âˆ’C x

ren (2.13) och Ã¤r en tillstÃ¥ndsobservatÃ¶r till det stokastiska systemet (3.1)(3.2),
det vill sÃ¤ga

xk+1 = Axk + Buk + F vk ,
yk = Cxk + Gwk .
Skattningsfelet ges av

Ìƒ k = xk âˆ’ x
Ì‚k,
x

(3.1)
(3.2)

sÃ¥ felsystemet Ã¤r

Ìƒ k+1 = xk+1 âˆ’ x
Ì‚ k+1
x
Ì‚ k + Buk + AK yÌƒk )
= Axk + Buk + F vk âˆ’ (Ax
Ì‚ k ) + F vk âˆ’ AK(yk âˆ’ C x
Ì‚k)
= A(xk âˆ’ x
Ì‚ k ) + F vk âˆ’ AK(Cxk + Gwk âˆ’ C x
Ì‚k)
= A(xk âˆ’ x
Ìƒ k + F vk âˆ’ AKGwk .
= (A âˆ’ AKC)x
Skattningsfelet Ã¤r alltsÃ¥ ett stokastiskt system med systemmatrisen
och vitt brus

F Pv F

T

matrisen

Î·k = F vk âˆ’ AKGwk

+ AKGPw (AKG)
A âˆ’ AKC

T

A âˆ’ AKC

med vÃ¤ntevÃ¤rdet noll och kovariansen

. Om antagandena i sats 3.17 Ã¤r uppfylda sÃ¥ Ã¤r

stabil och dÃ¥ kovergerar det stationÃ¤ra Kalmanltrets till-

stÃ¥nd (3.36) mot det stokastiska systemets tillstÃ¥nd.

40

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

Sambandet mellan det stationÃ¤ra Kalmanltret och prediktorn samt korrektorn i denna avhandling fÃ¥s enligt fÃ¶ljande. Prediktionen som ges av sats 3.10
Ã¤r

Ì‚ k+1|k = Ax
Ì‚ k|k + Buk ,
x
dÃ¤r

Ì‚ k|k
x

(3.37)

Ã¤r korrektionen i sats 3.15, det vill sÃ¤ga

Ì‚ k|k = x
Ì‚ k|kâˆ’1 + Kk yÌƒk|kâˆ’1 .
x

(3.38)

InsÃ¤ttning av (3.38) i (3.37) ger

Ì‚ k+1|k = A(x
Ì‚ k|kâˆ’1 + Kk yÌƒk|kâˆ’1 ) + Buk
x
Ì‚ k|kâˆ’1 + Buk + AKk yÌƒk|kâˆ’1 ,
= Ax
vilket Ã¤r pÃ¥ formen (3.36), med tidsvariant KalmanfÃ¶rstÃ¤rkning. DÃ¥ villkoren i
sats 3.17 Ã¤r uppfylda kovergerar

Kk

mot den tidsinvarianta (3.35).

3.4 Sampling av tidskontinuerliga stokastiska system
Hittills har tidsdiskreta stokastiska system behandlats och hur man bestÃ¤mmer
en optimal skattning av tillstÃ¥ndet baserat pÃ¥ tillgÃ¤nglig information, sÃ¥ att det
kvadratiska medelfelet minimeras. Modellerna fÃ¶r systemen Ã¤r fÃ¶rstÃ¥s oftast tidskontinuerliga. FrÃ¥gan Ã¤r om ett tidskontinuerligt stokastiskt system kan samplas
sÃ¥ att det diskretiserade systemet har samma stokastiska egenskaper som det
tidskontinuerliga systemet vid samplingstidpunkterna. Bruset fÃ¶r tidskontinuerliga system modelleras med

Denition 3.18.
(â„¦, A, P)
1.

En

Brownsk rÃ¶relse,

ocksÃ¥ kallad

Wienerprocess {Wt }tâ‰¥0

i

R

pÃ¥

Wienerprocess.
sannolikhetsrummet

uppfyller fÃ¶ljande villkor:

W0 = 0.
Wt âˆ¼ N (0, Ïƒ 2 t).

2. FÃ¶r

t>0

3. FÃ¶r

0 â‰¤ t1 < . . . < tk

Ã¤r

gÃ¤ller att inkrementen

oberoende.
4. FÃ¶r varje

Ï‰âˆˆâ„¦

Ã¤r

t â†’ Wt (Ï‰)

kontinuerlig.

41

Wtk âˆ’ Wtkâˆ’1 , . . . , Wt2 âˆ’ Wt1

Ã¤r

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Denition 3.19.
en

En process

{Wt }tâ‰¥0 := {( Wt(1)

n-dimensionell Wienerprocess

(2)

Wt

Tuomas Virtanen

(n)

... Wt

)T }tâ‰¥0

i

Rn

kallas

(1)
(2)
(n)
om dess komponenter Wt , Wt , . . . , Wt
Ã¤r

oberoende endimensionella Wienerprocesser.

Denition 3.20. Autokorrelationsfunktionen rX (t1 , t2 ) fÃ¶r en stokastisk process
{Xt }tâ‰¥0 denieras som
rX (t1 , t2 ) := E(Xt1 XT
t2 ).
Om autokorrelationsfunktionen beror endast pÃ¥ skillnaden

Ï„ = tâˆ’s, s < t, skrivs

rX (Ï„ ) := E(Xs XT
s+Ï„ ).

Denition 3.21.
nÃ¤r

En tidskontinuerlig stokastisk process

{Xt }tâˆˆR

Ã¤r

svagt statio-

om det gÃ¤ller att

1.

E(Xt ) = Âµ,

2.

rX (t1 , t2 ) = rX (t1 âˆ’ t2 ),

fÃ¶r varje

t âˆˆ R,
fÃ¶r varje

t1 , t2 âˆˆ R.

FÃ¶ljande lemma fÃ¶ljer direkt frÃ¥n denitionerna.

Lemma 3.22.
(i)

dÃ¤r Wt

LÃ¥t {Wt }tâ‰¥0 = {( Wt(1)

(2)

Wt

â›

Ïƒ12 Ï„

âœ
rW (Ï„ ) = Cov(WÏ„ ) = âœ
â
0
Matrisen PW kallas ibland

dÃ¤r

)T }tâ‰¥0 vara en Wienerprocess,

âˆ¼ N (0, Ïƒi2 t). DÃ¥ Ã¤r {Wt }tâ‰¥0 en svagt stationÃ¤r process och det gÃ¤ller

att

En

(n)

... Wt

0
..

.

Ïƒn2 Ï„

â
âŸ
âŸ = PW Ï„.
â 

inkrementella kovariansmatrisen

fÃ¶r processen Wt .

stokastisk dierentialekvation Ã¤r av formen
â§
(ï¸
)ï¸
â¨dx(t) = Ax(t) + Bu(t) dt + HdWt ,
â© x(0) = x0 ,

Wt

(3.39)

Ã¤r en Wienerprocess. LÃ¶sningen ges av en stokastisk integral, fÃ¶r vilken

teorin Ã¤r omfattande och kan lÃ¤sas i till exempel [Ã˜ks98]. HÃ¤r ges endast resultatet som diskretiserar systemet, enligt [Ã…st70, Sats 3.10.1, s. 84], modierad att
ocksÃ¥ innehÃ¥lla styrsignalen

u(t)

som samplas med nollte ordningens hÃ¥llkrets

pÃ¥ samma sÃ¤tt som i avsnitt 2.1.1 fÃ¶r deterministiska system.

42

KAPITEL 3.

Sats 3.23.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

LÃ¥t T > 0 vara samplingstiden. DÃ¥ fÃ¥s att systemets

(3.39)

tillstÃ¥nd

vid de diskreta tidpunkterna tk = kT , k = 0, 1, . . . Ã¤r

xk+1 = Ad (T )xk + Bd (T )uk + vk ,
dÃ¤r

Ad (T ) := e

AT

och Bd (T ) :=

(3.40)

T

âˆ«ï¸‚

eAÏ„ dÏ„ B.

0

Processen {vk }kâ‰¥0 Ã¤r gaussiskt vitt brus med vÃ¤ntevÃ¤rdet noll och kovariansmatrisen

T

âˆ«ï¸‚
Pv =

Ad (Ï„ )HPW H T AT
d (Ï„ )dÏ„,

(3.41)

0

dÃ¤r PW Ã¤r inkrementella kovariansmatrisen fÃ¶r Wienerprocessen Wt i

(3.39).

Exempel 2.1 kan nu utvidgas med stÃ¶rningar. Systemet samplas och sedan
kan Kalmanltret tillÃ¤mpas pÃ¥ det diskretiserade systemet fÃ¶r att estimera positionen.

Exempel 3.24.
y

LÃ¥t stÃ¶rningarna vara i form av Ã¤ndringar i acceleration i

x och

riktningar. DÃ¥ kan tillstÃ¥ndsekvationen i exempel 2.3 skrivas som en stokastisk

dierentialekvation

â›
ââ› â
0
x1
0 0 1 0
âœ
âŸâœ âŸ
âœ âŸ âœ
âœ
âœdx2 âŸ âœ0 0 0 1âŸ âœx2 âŸ
âŸ âœ âŸ dt + âœ0
âœ âŸ=âœ
âœ1
âœdx âŸ âœ0 0 0 0âŸ âœx âŸ
â
â  â 3â 
â 3â  â
0
x4
0 0 0 0
dx4
â›

dÃ¤r

x1 , x2

Ã¤r

riktning och

dx1

x

â

â›

respektive

dv1 , dv2

y

positionen,

x3 , x4

0

â

âŸ (ï¸„ )ï¸„
0âŸ
âŸ dv1 ,
0âŸ
â  dv2
1

Ã¤r hastigheten i

x

(3.42)

respektive

y

Ã¤r stÃ¶rningar. Ekvation (3.42) Ã¤r pÃ¥ formen

dx(t) = Ax(t)dt + HdVt
och detta diskretiseras med samplingsperioden

âˆ†t.

Den diskreta tillstÃ¥ndsekva-

tion fÃ¶r systemet Ã¤r

xk+1 = As xk + vk .
Sats 3.23 ger att

As = e

Aâˆ†t

â›
1
âœ
âœ0
=âœ
âœ0
â
0
43

0 âˆ†t
1

0

0

1

0

0

0

â

âŸ
âˆ†tâŸ
âŸ,
0âŸ
â 
1

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

vilken berÃ¤knades redan i exempel 2.3 . Processen

{vk }kâ‰¥0

Tuomas Virtanen

Ã¤r gaussiskt vitt brus

med vÃ¤ntevÃ¤rdet noll och kovariansen blir enligt sats 3.23

âˆ†t

âˆ«ï¸‚
Pv =

eAs HPV (eAs H)T ds.

0
Vi har att

LÃ¥t

Vt ,

PV =

ââ›
0
1 0 s 0
âŸâœ
âœ
âœ0 1 0 s âŸ âœ0
âŸâœ
eAs H = âœ
âœ0 0 1 0âŸ âœ1
â â
â
0
0 0 0 1
)ï¸‚
â›

(ï¸‚

2
Ïƒv1
0
2
0 Ïƒv2

â
s 0
âŸ
âŸ âœ
âœ0 sâŸ
0âŸ
âŸ
âŸ=âœ
âœ1 0âŸ .
0âŸ
â 
â  â
0 1
1
0

â

â›

vara inkrementella kovariansmatrisen fÃ¶r Wienerprocessen

dÃ¥ Ã¤r

â
s 0
)ï¸„ (ï¸„
)ï¸„
âŸ (ï¸„ 2
âˆ«ï¸‚ âˆ†t âœ
âœ0 sâŸ Ïƒv1
0
s
0
1
0
âŸ
âœ
ds
Pv =
âœ1 0 âŸ 0 Ïƒ 2
0
s
0
1
0
â 
â
v2
0 1
â
â›
2 2
2
Ïƒv1
s
0
Ïƒv1
s 0
âŸ
âˆ«ï¸‚ âˆ†t âœ
2 âŸ
2 2
âœ 0
sâŸ
s
0 Ïƒv2
Ïƒv2
âœ
ds
=
âœ Ïƒ2 s
2
0
Ïƒv1
0 âŸ
0
â 
â v1
2
2
s
0
Ïƒv2
0
Ïƒv2
â› Ïƒ2 (âˆ†t)3
â
2 (âˆ†t)2
Ïƒv1
v1
0
0
2
2 (âˆ†t)3
2 (âˆ†t)2 âŸ
âœ 3
Ïƒv2
Ïƒv2
âœ 0
âŸ
0
3
2
âŸ.
=âœ
2
2
âœ Ïƒv1 (âˆ†t)
âŸ
2
0
Ïƒ
âˆ†t
0
â 2
â 
v1
2 (âˆ†t)2
Ïƒv2
2
0
0
Ïƒv2 âˆ†t
2
â›

(3.43)

LÃ¥t mÃ¤tbruset vara gaussiskt vitt brus med vÃ¤ntevÃ¤rdet noll och kovariansmatrisen

Pw =

(ï¸‚

2
Ïƒw1
0
2
0 Ïƒw2

)ï¸‚

.

(3.44)

Den tidsdiskreta modellen av systemet ges nu av

xk+1 = Axk + vk ,

vk âˆ¼ N4 (0, Pv ),

yk = Cxk + wk ,

wk âˆ¼ N2 (0, Pw ),

44

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

dÃ¤r systemmatriserna ges av

â›

1 0 âˆ†t

âœ
âœ0 1
A=âœ
âœ0 0
â
0 0

0
1
0

0

â

âŸ
âˆ†tâŸ
âŸ,
0âŸ
â 
1

(ï¸„
C=

)ï¸„
1 0 0 0
0 1 0 0

och kovariansmatriserna ges av (3.43) samt (3.44).
FÃ¶r att se hur estimatet som ges av prediktionen eller korrektionen avviker
sig frÃ¥n det verkliga tillstÃ¥ndet och mÃ¤tningen har detta simulerats med Mat-

lab. Koden fÃ¶r programmet nns i bilaga C.1. Bilens starttillstÃ¥nd
slumpmÃ¤ssigt enligt en normalfÃ¶rdelning med vÃ¤ntevÃ¤rdet
ansmatrisen

P0 = I .

m0 = 0

x0

vÃ¤ljs

och kovari-

Varianserna fÃ¶r processbruset och mÃ¤tbruset antas vara

2
2
2
2
= 14 , Ïƒv1
=1
= Ïƒw2
= Ïƒv2
Ïƒw1

och samplingsintervallet sÃ¤tts till

âˆ†t =

1
. DÃ¥ Ã¤r
10

processbrusets kovariansmatris fÃ¶r det diskretiserade systemet

â›

0.0003

0

0.0050

0

â

âŸ
âœ
âŸ
âœ 0
0.0003
0
0.0050
âŸ.
Pv = âœ
âœ0.0050
0
0.1000
0 âŸ
â 
â
0
0.0050
0
0.1000
Prediktionen ges av

Ì‚ k|l = Ax
Ì‚ kâˆ’1|l ,
x

Pk|l = APkâˆ’1|l AT + Pv

och korrektionen ges av

Ì‚ k|k = x
Ì‚ k|kâˆ’1 + Kk (yk âˆ’ C x
Ì‚ k|kâˆ’1 ),
x
Kk = Pk|kâˆ’1 C T (CPk|kâˆ’1 C T + Pw )âˆ’1 ,
Pk|k = Pk|kâˆ’1 âˆ’ Kk CPk|kâˆ’1 .
Prediktionen av starttillstÃ¥ndet Ã¤r

Ì‚ 0|âˆ’1 = m0 = 0,
x

P0|âˆ’1 = P0 = I.

En simulering med dessa parametrar dÃ¤r mÃ¤tningen gÃ¶rs vid varje steg och
sÃ¥ledes kan korrektionen anvÃ¤ndas vid varje steg illustreras i gur 3.1. AnvÃ¤ndning av samma bana och stÃ¶rningar men mÃ¤tningar vid var fjÃ¤rde steg, sÃ¥ att
prediktionen anvÃ¤nds fÃ¶r att estimera positionen vid de Ã¶vriga tidpunkterna,
illustreras i gur 3.2.

45

KAPITEL 3.

OPTIMAL STOKASTISK ESTIMERING

Tuomas Virtanen

Figur 3.1: Simulerad bana, mÃ¤tningar och skattning av positionen enligt Kalmankorrektorn.

Figur 3.2: Samma bana som gur 3.1, mÃ¤tningar vid var fjÃ¤rde steg och skattning
av positionen enligt Kalmanprediktorn.

46

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

Kapitel 4
Optimal stokastisk reglering
4.1 Reglerproblemet
I avsnitt 2.4 bestÃ¤mdes den optimala reglersignalen som minimerar den kvadratiska kostnadsfunktionalen (2.17) fÃ¶r det linjÃ¤ra deterministiska systemet (2.7).
Motsvarande fundamentala stokastiska reglerproblem Ã¤r det

gaussiska reglerproblemet

eller

LQG-problemet

linjÃ¤rkvadratiska

(eng. linear-quadratic-Gaussian

control problem) fÃ¶r linjÃ¤ra system med vitt brus. DÃ¥ tillstÃ¥ndsvektorn Ã¤r stokastisk blir kostnaden (2.17) en stokastisk variabel och kostnadsfunktionalen fÃ¶r
stokastiska reglerproblemet vÃ¤ljs att vara vÃ¤ntevÃ¤rdet av (2.17) och betecknas
med

(ï¸„
)ï¸„
N
âˆ’1
âˆ‘ï¸‚
(ï¸
)ï¸
T
(xT
lN (x0 , u) := E JN (x0 , u) = E xT
N SN xN +
k Qxk + uk Ruk ) .

(4.1)

k=0
Analogt med deterministiska fallet anvÃ¤nds hÃ¤r en kostnadsfunktional med Ã¤ndlig
tidshorisont.
FÃ¶r att bestÃ¤mma den optimala reglersignalen, som minimerar (4.1), Ã¤r det
viktigt att specicera vad som Ã¤r en

tillÃ¥ten reglersignal.

Den tillgÃ¤ngliga in-

formationen som anvÃ¤nds fÃ¶r att reglera systemet Ã¤r mÃ¤tningar av systemets
tillstÃ¥nd vid olika tidpunkter, som pÃ¥verkas av brus. Om
vation (3.2) fÃ¶r mÃ¤tsignalen sÃ¥ Ã¤r
vÃ¤rdet pÃ¥ tillstÃ¥ndsvektorn

xk .

C=I

och

G = 0 i ek-

yk = xk , det vill sÃ¤ga mÃ¤tningen ger det exakta

I detta fall ger inga andra fÃ¶regÃ¥ende mÃ¤tningar

nÃ¥gon ytterligare information om tillstÃ¥ndet och dÃ¥ kan den tillÃ¥tna reglersignalen vara en funktion av tillstÃ¥ndet,
form av

tillstÃ¥ndsÃ¥terkoppling

uk = f (xk ),

det vill sÃ¤ga regleringen Ã¤r i

och behandlas i avsnitt 4.2.

47

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Vanligtvis Ã¤r matriserna

C Ì¸= I

och

G Ì¸= 0, det vill sÃ¤ga det exakta vÃ¤rdet pÃ¥

tillstÃ¥ndet Ã¤r inte tillgÃ¤ngligt. DÃ¥ ger mÃ¤ngden
fÃ¶rdelning

ji â‰¤ k ,

(m0 , P0 )

Tuomas Virtanen

Yk ,

samt tillgÃ¤ngliga mÃ¤tningar

det vill sÃ¤ga starttillstÃ¥ndets

yj0 , . . . , yji , 0 â‰¤ j0 < . . . <
k.

den tillgÃ¤ngliga informationen fÃ¶r systemet vid tidpunkt

DÃ¥ kan den

tillÃ¥tna reglersignalen endast vara en funktion av denna information, det vill sÃ¤ga

uk = f (Yk ).
Den optimala skattningen av tillstÃ¥ndet

Ì‚k,
x

som ges av Kalmanprediktorn

i sats 3.10 eller Kalmankorrektorn i sats 3.15, anvÃ¤nder ocksÃ¥ den tillgÃ¤ngliga
informationen

Ì‚ k = g(Yk ). Reglersignaler av formen uk = f (x
Ì‚k)
Yk , det vill sÃ¤ga x

Ã¤r alltsÃ¥ tillÃ¥tna. I detta fall anvÃ¤nds

Ã¥terkoppling av det skattade tillstÃ¥ndet

och

behandlas i avsnitt 4.3.
Det stokastiska reglerproblemet kan enligt [Ã…st70, s. 258] formuleras pÃ¥ fÃ¶ljande sÃ¤tt:

Problem 4.1.

Hitta en tillÃ¥ten reglersignal fÃ¶r det stokastiska systemet

xk+1 = Axk + Buk + F vk ,

(3.1)

yk = Cxk + Gwk ,

(3.2)

sÃ¥ att den kvadratiska kostnadsfunktionalen

(ï¸„

lN (x0 , u) = E xT
N SN xN +

)ï¸„

N
âˆ’1
âˆ‘ï¸‚

T
(xT
k Qxk + uk Ruk )

(4.1)

k=0
minimeras.
Analogt med deterministiska fallet gÃ¶rs fÃ¶rst en omskrivning av uttrycket
innanfÃ¶r vÃ¤ntevÃ¤rdet i kostnadsfunktionalen (4.1) enligt fÃ¶ljande lemma [Ã…st70,
Lemma 8.6.1, s. 278].

Lemma 4.2.

Anta att SN â‰¥ 0, Q â‰¥ 0 och R > 0 i

(4.1)

och lÃ¥t

Lk = (B T Sk+1 B + R)âˆ’1 B T Sk+1 A,

(2.18)

Sk = AT Sk+1 A + Q âˆ’ AT Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 A,

(2.19)

fÃ¶r varje k = 0, . . . , N âˆ’ 1.

48

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

DÃ¥ gÃ¤ller att

xT
N SN xN +
+

N
âˆ’1
âˆ‘ï¸‚

T
T
(xT
k Qxk + uk Ruk ) = x0 S0 x0 +

k=0
N
âˆ’1
âˆ‘ï¸‚

N
âˆ’1
âˆ‘ï¸‚

vkT F T Sk+1 F vk

k=0

(uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )

k=0
N
âˆ’1
âˆ‘ï¸‚

+2

(4.2)

vkT F T Sk+1 (Axk + Buk ).

k=0

Bevis.

Analogt med beviset av sats 2.18 skriver vi om uttrycket i vÃ¤nstra ledet

av (4.2) med anvÃ¤ndning av tillstÃ¥ndsekvation (3.1). DÃ¥ fÃ¥s att

xT
N SN xN +

N
âˆ’1
âˆ‘ï¸‚

T
(xT
k Qxk + uk Ruk )

k=0

= xT
0 S0 x0 +
= xT
0 S0 x0 +

N
âˆ’1
âˆ‘ï¸‚
k=0
N
âˆ’1
âˆ‘ï¸‚

(ï¸ T
)ï¸
T
T
xk+1 Sk+1 xk+1 âˆ’ xT
k Sk xk + xk Qxk + uk Ruk
(ï¸ T
)ï¸
T
xk Qxk + uT
k Ruk âˆ’ xk Sk xk

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(Axk + Buk + F vk )T Sk+1 (Axk + Buk + F vk )

k=0

= xT
0 S0 x0 +

N
âˆ’1
âˆ‘ï¸‚

(ï¸ T
T S x )ï¸
xk Qxk + uT
Ru
âˆ’
x
k
k
k k k

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(Axk + Buk )T Sk+1 (Axk + Buk ) +

k=0
N
âˆ’1
âˆ‘ï¸‚

+2

N
âˆ’1
âˆ‘ï¸‚

vkT F T Sk+1 F vk

k=0

vkT F T Sk+1 (Axk + Buk ).

k=0
Eftersom

SN â‰¥ 0, Q â‰¥ 0

och

R>0

sÃ¥ Ã¤r

Sk â‰¥ 0

fÃ¶r varje

k = 0, . . . , N

lemma 2.21 . DÃ¥ ger lemma 2.22 att

xT
N SN xN +
+

N
âˆ’1
âˆ‘ï¸‚

N
âˆ’1
âˆ‘ï¸‚

k=0
N
âˆ’1
âˆ‘ï¸‚

k=0

T
T
(xT
k Qxk + uk Ruk ) = x0 S0 x0 +

vkT F T Sk+1 F vk

(uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )

k=0
N
âˆ’1
âˆ‘ï¸‚

+2

vkT F T Sk+1 (Axk + Buk ).

k=0

49

enligt

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

De esta termer i (4.2) Ã¤r av kvadratisk form och fÃ¶ljande lemma enligt [Ã…st70, Lemma 8.3.3, s. 262] ger en enkel formel fÃ¶r att berÃ¤kna vÃ¤ntevÃ¤rdet
av kvadratisk form.

Lemma 4.3.

LÃ¥t x âˆˆ Rn vara en stokastisk vektor med vÃ¤ntevÃ¤rdet mx och

kovariansmatrisen Px . FÃ¶r en godtycklig matris S âˆˆ RnÃ—n gÃ¤ller att

E(xT Sx) = mT
x Smx + tr(SPx ).
Bevis.

Vi har att

(ï¸
)ï¸
T
T
E(xT Sx) = E (x âˆ’ mx )T S(x âˆ’ mx ) + mT
x Sx + x Smx âˆ’ mx Smx
(ï¸
)ï¸
T
T
= E (x âˆ’ mx )T S(x âˆ’ mx ) + mT
x S E(x) + E(x )Smx âˆ’ mx Smx
)ï¸
(ï¸
= E (x âˆ’ mx )T S(x âˆ’ mx ) + mT
x Smx .
(4.3)
Eftersom

(x âˆ’ mx )T S(x âˆ’ mx )

Ã¤r en skalÃ¤r sÃ¥ gÃ¤ller att

(ï¸‚ (ï¸
(ï¸
)ï¸
)ï¸)ï¸‚
T
T
E (x âˆ’ mx ) S(x âˆ’ mx ) = E tr (x âˆ’ mx ) S(x âˆ’ mx ) .
Vidare eftersom

tr(AB) = tr(BA)

sÃ¥ Ã¤r

(ï¸‚ (ï¸
(ï¸‚ (ï¸
)ï¸)ï¸‚
)ï¸)ï¸‚
E tr (x âˆ’ mx )T S(x âˆ’ mx ) = E tr S(x âˆ’ mx )(x âˆ’ mx )T .
SpÃ¥ret Ã¤r summan av diagonalelementen och vÃ¤ntevÃ¤rdet av en summa Ã¤r summan av vÃ¤ntevÃ¤rdena sÃ¥ kan vi byta ordningen pÃ¥ operationerna fÃ¶r vÃ¤ntevÃ¤rdet
och spÃ¥ret fÃ¶r att fÃ¥

(ï¸‚ (ï¸
(ï¸‚ (ï¸
)ï¸)ï¸‚
)ï¸)ï¸‚
E tr S(x âˆ’ mx )(x âˆ’ mx )T = tr E S(x âˆ’ mx )(x âˆ’ mx )T
(ï¸‚
(ï¸
)ï¸)ï¸‚
= tr S E (x âˆ’ mx )(x âˆ’ mx )T

(4.4)

= tr(SPx ).
InsÃ¤ttning av (4.4) i (4.3) ger pÃ¥stÃ¥endet.

AnmÃ¤rkning

4.4. Observera att lemma 4.3 kan ocksÃ¥ anvÃ¤ndas fÃ¶r betingade

vÃ¤ntevÃ¤rdet av kvadratisk form. Om

E(x | y) = mx|y

och

Cov(x | y) = Px|y

gÃ¤ller att

E(xT Sx | y) = mT
x|y Smx|y + tr(SPx|y ),
vilket bevisas pÃ¥ samma sÃ¤tt som beviset fÃ¶r lemma 4.3.

50

sÃ¥

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

4.2 Minimering av kostnaden genom tillstÃ¥ndsÃ¥terkoppling
I detta avsnitt bestÃ¤ms reglersignalen som minimerar kostnaden dÃ¥ den tillÃ¥tna
reglersignalen Ã¤r en funktion av tillstÃ¥ndet, det vill sÃ¤ga dÃ¥ mÃ¤tsignalen ger
exakta vÃ¤rdet pÃ¥ alla tillstÃ¥ndsvariabler och pÃ¥verkas inte av nÃ¥got mÃ¤tbrus.
FÃ¶ljande lemma, som Ã¤r en omformulering av [Ã…st70, Lemma 8.3.1, s.260],
implicerar att operationen att minimera kostnaden (4.1) med avseende pÃ¥ reglersignaler som Ã¤r funktioner av tillstÃ¥ndet kommuterar med operationen att ta
vÃ¤ntevÃ¤rdet av kostnaden.

Lemma 4.5.

Anta att den tillÃ¥tna reglersignalen Ã¤r en funktion av tillstÃ¥ndet,

u = f (x), f : Rn â†’ Rm . Vidare anta att funktionen l(x, u) antar ett minsta
vÃ¤rde i ett entydigt u fÃ¶r varje givet vÃ¤rde pÃ¥ den stokastiska vektorn x.
DÃ¥ gÃ¤ller att

(ï¸
)ï¸
(ï¸
)ï¸
min E l(x, u) = E min l(x, u) .
u

Bevis.

u

FÃ¶r varje tillÃ¥ten reglersignal

f (x)

gÃ¤ller att

l(x, f (x)) â‰¥ min l(x, u),
u

varfÃ¶r enligt (B.11) fÃ¥s att

(ï¸
)ï¸
(ï¸
)ï¸
E l(x, f (x)) â‰¥ E min l(x, u) .
u

Minimering av vÃ¤nstra ledet med avseende pÃ¥ alla tillÃ¥tna reglersignaler

f (x)

ger att

(ï¸
)ï¸
(ï¸
)ï¸
min E l(x, u) â‰¥ E min l(x, u) .
u

LÃ¥t

uâˆ— (x)

tersom

beteckna vÃ¤rdet pÃ¥

uâˆ— (x)

u

u

fÃ¶r vilken

l(x, u)

(4.5)

antar sitt minsta vÃ¤rde. Ef-

Ã¤r en tillÃ¥ten reglersignal gÃ¤ller samtidigt att

(ï¸
)ï¸
(ï¸
)ï¸
(ï¸
)ï¸
E min l(x, u) = E l(x, uâˆ— (x)) â‰¥ min E l(x, u) .
u

u

(4.6)

PÃ¥stÃ¥endet fÃ¶ljer frÃ¥n olikheterna (4.5) och (4.6).
Nu kan reglersignalen som minimerar kostnadsfunktionalen (4.1) fÃ¶r det stokastiska systemet med tillstÃ¥ndsekvationen (3.1) bestÃ¤mmas enligt [Ã…st70, Sats
8.6.2, s. 281], dÃ¥ det exakta tillstÃ¥ndet Ã¤r tillgÃ¤ngligt.

51

KAPITEL 4.

Sats 4.6.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

âˆ’1
Det nns en fÃ¶ljd av entydigt bestÃ¤mda reglersignaler uâˆ— = {uâˆ—k }N
k=0 ,

dÃ¤r uâˆ—k = f (xk ) fÃ¶r varje k , till systemet

xk+1 = Axk + Buk + F vk ,
yk = xk ,
som minimerar kostnadsfunktionalen

(4.1).

uâˆ—k = âˆ’Lk xk ,
dÃ¤r Lk ges av

(2.18)(2.19),

Vidare gÃ¤ller att

k = 0, . . . , N âˆ’ 1,

som i sats 2.18 fÃ¶r deterministiska fallet. Den mi-

nimala kostnaden Ã¤r

min lN (x0 , u) = mT
0 S0 m0 + tr(S0 P0 ) +
u

N
âˆ’1
âˆ‘ï¸‚

tr(F T Sk+1 F Pv ),

k=0

dÃ¤r m0 och P0 Ã¤r vÃ¤ntevÃ¤rdet respektive kovariansmatrisen fÃ¶r x0 och Pv Ã¤r
kovariansmatrisen fÃ¶r processbruset {vk }kâ‰¥0 .
Bevis.

Enligt lemma 4.2 Ã¤r kostnaden (4.1) lika med

lN (x0 , u) = E(xT
0 S0 x0 ) +

N
âˆ’1
âˆ‘ï¸‚

E(vkT F T Sk+1 F vk )

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(ï¸
)ï¸
E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )

k=0
N
âˆ’1
âˆ‘ï¸‚

+2

(4.7)

(ï¸
)ï¸
E vkT F T Sk+1 (Axk + Buk ) .

k=0
LÃ¥t

uk = f (xk )

fÃ¶r nÃ¥gon funktion

lemma 3.3 sÃ¥ Ã¤r ocksÃ¥

vk

N
âˆ’1
âˆ‘ï¸‚

f.

oberoende av

Eftersom

uk ,

vk

Ã¤r oberoende av

xk

enligt

vilket ger att

)ï¸
(ï¸
E vkT F T Sk+1 (Axk + Buk ) = 0.

(4.8)

k=0
FÃ¶r de Ã¶vriga termerna kan vi anvÃ¤nda lemma 4.3. FÃ¶r det fÃ¶rsta har vi att

T
E(xT
0 S0 x0 ) = m0 S0 m0 + tr(S0 P0 ).
DÃ¥

E(vk ) = 0

och

Cov(vk ) = Pv

(4.9)

fÃ¥s fÃ¶r det andra att

(ï¸
)ï¸
E(vkT F T Sk+1 F vk ) = E(vk )T F T Sk+1 F E(vk ) + tr F T Sk+1 F Cov(vk )
= tr(F T Sk+1 F Pv ).
52

(4.10)

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Vidare eftersom

(B T Sk+1 B + R) > 0

Tuomas Virtanen

sÃ¥ gÃ¤ller att

lk (xk , uk ) = (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ) â‰¥ 0
och likhet gÃ¤ller dÃ¥ och endast dÃ¥

uk + Lk xk = 0,

eller

uk = âˆ’Lk xk .
DÃ¥ har

lk (xk , uk )

ett minsta vÃ¤rde

bestÃ¤ms entydigt fÃ¶r varje vÃ¤rde pÃ¥

0
xk

(4.11)

med avseende pÃ¥

uk

fÃ¶r varje

k

och

uk

enligt (4.11). Lemma 4.5 ger att

(ï¸
)ï¸
min E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )
uk
(ï¸
)ï¸
= E min(uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ) = 0.
uk

(4.12)

Minimala kostnaden fÃ¥s med instÃ¤ttning av (4.8), (4.9), (4.10) och (4.12)
i (4.7) som

min lN (x0 , u) = mT
0 S0 m0 + tr(S0 P0 ) +
u

N
âˆ’1
âˆ‘ï¸‚

tr(F T Sk+1 F Pv )

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(ï¸
)ï¸
min E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )

k=0

uk

= mT
0 S0 m0 + tr(S0 P0 ) +

N
âˆ’1
âˆ‘ï¸‚

tr(F T Sk+1 F Pv ),

k=0
dÃ¥

uk = âˆ’Lk xk ,

fÃ¶r varje

k = 0, . . . , N âˆ’ 1.

Observera att reglersignalen hÃ¤r Ã¤r densamma som i deterministiska fallet,
det vill sÃ¤ga processbruset pÃ¥verkar inte valet av reglersignalen ifall systemets
tillstÃ¥nd kan mÃ¤tas exakt vid varje tidpunkt. Processbruset ger dock upphov
till en stÃ¶rre minimal kostnad Ã¤n om systemet inte hade brus, jÃ¤mfÃ¶r med den
minimala kostnaden fÃ¶r deterministisk optimal reglering (2.20).

53

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

4.3 Minimering av kostnaden genom Ã¥terkoppling
av det skattade tillstÃ¥ndet
Kalmankorrektorn

ger

den

optimala

skattningen

yk

minstakvadratmening, givet mÃ¤tningen

saknas.

PÃ¥

basen

av

den

bÃ¤sta

av

Ì‚k
x

av tillstÃ¥ndet

tillgÃ¤ngliga

optimala

xk

tillstÃ¥ndet

och informationen

manprediktorn ger den optimala skattningen

yk

Ì‚k
x

Yl , l < k .
xk

i

Kal-

dÃ¥ mÃ¤tningen

skattningen

kan

reglersignalen som minimerar kostnadsfunktionalen

(ï¸„

lN (x0 , u) = E xT
N SN xN +

)ï¸„

N
âˆ’1
âˆ‘ï¸‚

T
(xT
k Qxk + uk Ruk )

(4.1)

k=0
bestÃ¤mmas. Reglersignalen
informationen

Yk

uk

i detta fall Ã¤r en funktion av den tillgÃ¤ngliga

upp till och med tidpunkten

k.

Enligt lemma 4.2 och ekvatio-

nerna (4.9) och (4.10) kan kostnadsfunktionalen (4.1) skrivas som

T
lN (x0 , u) = mT
0 S0 m0 + tr(S0 P0 ) +

N
âˆ’1
âˆ‘ï¸‚

tr(F T Sk+1 F Pv )

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(ï¸
)ï¸
E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )

k=0
N
âˆ’1
âˆ‘ï¸‚

+2

(ï¸
)ï¸
E vkT F T Sk+1 (Axk + Buk ) .

k=0
Enligt antagandena fÃ¶r processbruset och mÃ¤tbruset Ã¤r

uk = f (Yk ),

som visades i lemma 3.3. Vidare eftersom

f (y0 , . . . , yk ),

och

ma 3.3, fÃ¶ljer att

vk

vk

Ã¤r oberoende av

Ã¤r oberoende av

yn

uk .

fÃ¶r varje

vk

oberoende av

det vill sÃ¤ga

n = 0, . . . , k ,

xk

uk =

enligt lem-

DÃ¤rmed Ã¤r alla termer i sista summan

lika med noll, det vill sÃ¤ga

T
lN (x0 , u) = mT
0 S0 m0 + tr(S0 P0 ) +

N
âˆ’1
âˆ‘ï¸‚

tr(F T Sk+1 F Pv )

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(4.13)

(ï¸
)ï¸
E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ) .

k=0
FÃ¶r att minimera termerna

(ï¸
)ï¸
E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ) sÃ¥

behÃ¶vs ett liknande hjÃ¤lpresultat till lemma 4.5 dÃ¤r minimeringen nu gÃ¶rs med
avseende pÃ¥ reglersignaler som Ã¤r funktioner av det skattade tillstÃ¥ndet. FÃ¶ljande
lemma Ã¤r en omformulering av [Ã…st70, Lemma 8.3.2, s. 261].

54

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

Lemma 4.7.

Anta att den tillÃ¥tna reglersignalen Ã¤r en funktion av mÃ¤tsignalen,
(ï¸
)ï¸
u = f (y), f : Rp â†’ Rm . Vidare anta att funktionen g(y, u) = E l(x, y, u) | y
antar ett minsta vÃ¤rde i ett entydigt u fÃ¶r varje givet vÃ¤rde pÃ¥ den stokastiska
vektorn y .
DÃ¥ gÃ¤ller att

(ï¸
)ï¸
(ï¸
)ï¸
(ï¸
(ï¸
)ï¸)ï¸
min E l(x, y, u) = E min g(y, u) = E min E l(x, y, u) | y .
u

Bevis.

u

FÃ¶r varje tillÃ¥ten reglersignal

u

f (y)

gÃ¤ller att

(ï¸
)ï¸
g y, f (y) â‰¥ min g(y, u),
u

eller

(ï¸
)ï¸
(ï¸
)ï¸
E l(x, y, f (y)) | y â‰¥ min E l(x, y, u) | y ,
u

varfÃ¶r enligt B.11 fÃ¥s att

(ï¸‚ (ï¸
(ï¸‚
)ï¸)ï¸‚
(ï¸
)ï¸)ï¸‚
E E l(x, y, f (y)) | y â‰¥ E min E l(x, y, u) | y ,
u

dÃ¤r vÃ¤nstra ledet kan skrivas med egenskap 3 i sats B.18 som

(ï¸
)ï¸
(ï¸
)ï¸)ï¸‚
E E l(x, y, f (y)) | y = E l(x, y, f (y)) .
(ï¸‚

Minimering av vÃ¤nstra ledet med avseende pÃ¥ alla tillÃ¥tna reglersignaler

f (y)

ger att

(ï¸
)ï¸
(ï¸
)ï¸
min E l(x, y, u) â‰¥ E min E(l(x, y, u) | y) .
u

LÃ¥t

uâˆ— (y)

tersom

u

beteckna vÃ¤rdet pÃ¥

uâˆ— (y)

u

fÃ¶r vilken

g(y, u)

(4.14)

antar sitt minsta vÃ¤rde. Ef-

Ã¤r en tillÃ¥ten reglersignal gÃ¤ller samtidigt att

(ï¸
)ï¸
(ï¸
)ï¸
E g(y, uâˆ— (y)) â‰¥ min E g(y, u) ,
u

det vill sÃ¤ga

(ï¸
)ï¸
(ï¸
)ï¸
E min E(l(x, y, u) | y) â‰¥ min E l(x, y, u) .
u

u

(4.15)

PÃ¥stÃ¥endet fÃ¶ljer frÃ¥n olikheterna (4.14) och (4.15).
Vi kan nu bevisa fÃ¶ljande sats som Ã¤r huvudresultatet i denna avhandling.
Satsen Ã¤r en generalisering av [Ã…st70, Sats 8.6.3, s.282] och ger lÃ¶sningen till
LQG problemet ocksÃ¥ ifall mÃ¤tningar inte gÃ¶rs vid varje tidpunkt.

55

KAPITEL 4.

Sats 4.8.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

âˆ’1
âˆ—
Det nns en entydig reglersignal uâˆ— = {uâˆ—k }N
k=0 , dÃ¤r uk = f (Yk ) fÃ¶r

varje k , till systemet

xk+1 = Axk + Buk + F vk ,
yk = Cxk + Gwk ,

(3.1)
(3.2)

dÃ¤r x0 âˆ¼ Nn (m0 , P0 ), vk âˆ¼ Nn (0, Pv ) och wk âˆ¼ Np (0, Pw ), som minimerar
kostnadsfunktionalen

(4.1).

Reglersignalen ges av

uâˆ—k =

â§
â¨âˆ’Lk x
Ì‚ k|k ,

om yk âˆˆ Yk ,

â© âˆ’Lk x
Ì‚ k|l ,

om yk âˆˆ
/ Yk ,

Ì‚ k|l ges av Kalmanprediktorn
dÃ¤r x

(3.16)

Ì‚ k|k ges av Kalmankorrektorn
och x

(3.22).

Ã…terkopplingsmatrisen fÃ¥s som i deterministiska fallet enligt

Lk = (B T Sk+1 B + R)âˆ’1 B T Sk+1 A,

k = 0, . . . , N âˆ’ 1

(2.18)

dÃ¤r Sk , k = 0, . . . , N âˆ’ 1 Ã¤r lÃ¶sningen till Riccatiekvationen

Sk = AT Sk+1 A + Q âˆ’ AT Sk+1 B(B T Sk+1 B + R)âˆ’1 B T Sk+1 A

(2.19)

med begynnelsevÃ¤rdet SN , och matriserna SN â‰¥ 0, Q â‰¥ 0 och R > 0 Ã¤r givna i
kostnadsfunktionalen

(4.1).

Vidare Ã¤r den minimala kostnaden

min lN (x0 , u) = mT
0 S0 m0 + tr(S0 P0 ) +
u

N
âˆ’1
âˆ‘ï¸‚

tr(F T Sk+1 F )

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(4.16)

T
tr(Pk|l LT
k B Sk+1 A),

k=0

dÃ¤r kovariansmatrisen Pk|l ges av

(3.17)

fÃ¶r prediktorn om l < k eller av

(3.24)

fÃ¶r korrektorn om l = k och yk Ã¤r tillgÃ¤nglig.
(ï¸
)ï¸
Bevis. LÃ¥t f (Yl , uk ) = E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ) | Yl ,

uk = g(Yl )

och

l â‰¤ k.

dÃ¤r

DÃ¥ fÃ¥r vi enligt anmÃ¤rkning 4.4 att

(ï¸
)ï¸
E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ) | Yl
= E(uk + Lk xk | Yl )T (B T Sk+1 B + R) E(uk + Lk xk | Yl )
(ï¸
)ï¸
+ tr (B T Sk+1 B + R) Cov(uk + Lk xk | Yl ) .
56

(4.17)

KAPITEL 4.

Eftersom

uk

OPTIMAL STOKASTISK REGLERING

Ã¤r

Yl -mÃ¤tbar

Tuomas Virtanen

ger sats B.18 att

Ì‚ k|l
E(uk + Lk xk | Yl ) = uk + Lk E(xk | Yl ) = uk + Lk x

(4.18)

Cov(uk + Lk xk | Yl ) = Lk Cov(xk | Yl )Lk = Lk Pk|l Lk .

(4.19)

och

Nu fÃ¥s att

(ï¸
)ï¸
E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk ) | Yl
Ì‚ k|l )T (B T Sk+1 B + R)(uk + Lk x
Ì‚ k|l )
= (uk + Lk x
)ï¸
(ï¸
+ tr (B T Sk+1 B + R)Lk Pk|l LT
k .
Eftersom

(B T Sk+1 B + R) > 0

(4.20)

sÃ¥ Ã¤r

Ì‚ k|l )T (B T Sk+1 B + R)(uk + Lk x
Ì‚ k|l ) â‰¥ 0,
(uk + Lk x
dÃ¤r likhet fÃ¥s dÃ¥ och endast dÃ¥

Ì‚ k|l = 0,
uk + Lk x

eller

Ì‚ k|l .
uk = âˆ’Lk x
Funktionen

k

f (Yl , uk )

(4.21)

har alltsÃ¥ ett minsta vÃ¤rde med avseende pÃ¥

och bestÃ¤ms entydigt fÃ¶r varje vÃ¤rde pÃ¥

Ì‚ k|l
x

uk

fÃ¶r varje

enligt (4.21). Vi kan nu anvÃ¤nda

lemma 4.7 och fÃ¥r att

(ï¸
)ï¸
min E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )
uk
(ï¸‚
(ï¸
)ï¸)ï¸‚
T
T
= E min E (uk + Lk xk ) (B Sk+1 B + R)(uk + Lk xk ) | Yl .
uk

(4.22)

InsÃ¤ttning av (4.20) i (4.22) ger nu att

(ï¸
)ï¸
min E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )
uk
(ï¸
)ï¸
Ì‚ k|l )T (B T Sk+1 B + R)(uk + Lk x
Ì‚ k|l )
= E min(uk + Lk x
uk
(ï¸
)ï¸
+ tr (B T Sk+1 B + R)Lk Pk|l LT
k
(ï¸
)ï¸
= tr (B T Sk+1 B + R)Lk Pk|l LT
k ,
dÃ¤r minsta vÃ¤rdet antas dÃ¥
av

Lk

och dÃ¥

(4.23)

Ì‚ k|l . Vi kan skriva om (4.23) med insÃ¤ttning
uk = âˆ’Lk x

tr(AB) = tr(BA)

fÃ¥r vi att

)ï¸
tr (B T Sk+1 B + R)Lk Pk|l LT
k
(ï¸
)ï¸
T
= tr Pk|l LT
k (B Sk+1 B + R)Lk
(ï¸
)ï¸
T
T
âˆ’1 T
= tr Pk|l LT
k (B Sk+1 B + R)(B Sk+1 B + R) B Sk+1 A
)ï¸
(ï¸
TS
= tr Pk|l LT
B
A
.
k+1
k
(ï¸

57

(4.24)

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

Minsta vÃ¤rdet (4.16) pÃ¥ kostnadsfunktionalen (4.1) fÃ¥s nu enligt (4.13), (4.23)
och (4.24) som

T
min lN (x0 , u) = mT
0 S0 m0 + tr(S0 P0 ) +
u

N
âˆ’1
âˆ‘ï¸‚

tr(F T Sk+1 F )

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(ï¸
)ï¸
min E (uk + Lk xk )T (B T Sk+1 B + R)(uk + Lk xk )
uk

k=0

T
= mT
0 S0 m0 + tr(S0 P0 ) +

N
âˆ’1
âˆ‘ï¸‚

tr(F T Sk+1 F )

k=0

+

N
âˆ’1
âˆ‘ï¸‚

(ï¸
)ï¸
TS
tr Pk|l LT
B
A
,
k+1
k

k=0
dÃ¥

Ì‚ k|l
uk = âˆ’Lk x

AnmÃ¤rkning

fÃ¶r varje

k = 0, . . . , N âˆ’ 1

och

l â‰¤ k.

4.9. Som pÃ¥pekades i anmÃ¤rkning 3.16 Ã¤r den tillhÃ¶rande kovarians-

matrisen fÃ¶r korrektorn mindre Ã¤n fÃ¶r prediktorn. Detta ger att minsta kostnaden
fÃ¶r regleringen fÃ¥s om mÃ¤tningar Ã¤r tillgÃ¤ngliga vid varje tidpunkt, sÃ¥ att korrektorn kan berÃ¤knas vid varje steg och den anvÃ¤nda reglersignalen berÃ¤knas som
Ã¥terkoppling av det skattade tillstÃ¥ndet frÃ¥n korrektorn.

4.4 Optimal reglering av tvÃ¥ kopplade elnÃ¤tverk
ProblemstÃ¤llningen med kopplade elnÃ¤tverk Ã¤r att reglera referenspunkterna fÃ¶r
belastningseekten pÃ¥ bÃ¥da nÃ¤tverken sÃ¥ att frekvensen pÃ¥ nÃ¤tverken hÃ¥lls konstant. Det intressanta med kopplade nÃ¤tverk Ã¤r att om det ena nÃ¤tverket utsÃ¤tts
fÃ¶r en stor Ã¤ndring i belastningen sÃ¥ kommer mera eekt Ã¶da frÃ¥n det ena nÃ¤tverket till det andra och referenspunkterna behÃ¶ver Ã¤ndras fÃ¶r bÃ¥da nÃ¤tverken.
FÃ¶r regleringen anvÃ¤nds vanligtvis en PI- eller PID-reglerare, men hÃ¤r anvÃ¤nds
linjÃ¤rkvadratisk reglering, dÃ¤r tillstÃ¥ndsvariablerna fÃ¶r systemet estimeras med
Kalmanprediktorn eller Kalmankorrektorn. FÃ¶r teorin om elnÃ¤tverk och hur ekvationerna nedan beskriver systemet hÃ¤nvisas lÃ¤saren till [Kun94]. Ekvationerna
och anvÃ¤nda parametrar har tagits ur artiklarna [AA11], [PMH13], [Ros+13]
och [Sha+16] som studerats fÃ¶r detta Ã¤ndamÃ¥l.

58

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

TvÃ¥ kopplade elnÃ¤tverk beskrivs av fÃ¶ljande dierentialekvationer:

dâˆ†fi
dt
dâˆ†Pmechi
dt
dâˆ†PGi
dt
i,j
dâˆ†Ptie
dt
dÃ¤r indexen

= âˆ’ T1P i âˆ†fi âˆ’

i,j
KP i
âˆ†Ptie
TP i

= âˆ’ T1T i âˆ†Pmechi +
= âˆ’ T1Gi âˆ†PGi +

och

j

KP i
âˆ†Pmechi
TP i

âˆ’

KP i
âˆ†PLi ,
TP i

j Ì¸= i,

1
âˆ†PGi ,
TT i

1
âˆ†Prefi
TGi

= 2Ï€Ti,j (âˆ†fi âˆ’ âˆ†fj ),
i

+

âˆ’

(4.25)
(4.26)

1
âˆ†fi
TGi Ri

(4.27)

j Ì¸= i,

(4.28)

betecknar vilket elnÃ¤tverk det Ã¤r. Beteckningen

âˆ†

anvÃ¤nds

fÃ¶r att betona att tillstÃ¥ndsvariablerna Ã¤r avvikelser frÃ¥n det stationÃ¤ra vÃ¤rdet.
Parametrarna och variablerna fÃ¶r nÃ¤tverket fÃ¶rklaras i tabell 4.1.

Parameter

Beskrivning

TP i

Elsystemets tidskonstant

KP i

Elsystemets fÃ¶rstÃ¤rkning

TT i

Ã…ngturbinens tidskonstant

TGi

Centrifugalregulatorns tidskonstant

Ri

Ã…ngturbinens reglerparameter

Ti,j

Kopplingslinans tidskonstant

fi

Frekvens

Pmech

Mekanisk eekt

PLi

Frekvensoberoende belastning

PGi

Ã…ngturbinens eekt

Prefi

Referenspunkt fÃ¶r belastningseekt

i,j

Ptie

EektÃ¶de frÃ¥n omrÃ¥de

i

till

j

Tabell 4.1: Beskrivning av parametrarna och variablerna fÃ¶r modellen.

Om nÃ¤tverken antas ha en konstant belastning sÃ¥ Ã¤r

dâˆ†PLi
dt

= 0,

smÃ¥ fÃ¶rÃ¤nd-

ringar i belastningen simuleras med processbrus. TillstÃ¥ndsvektorerna fÃ¶r tvÃ¥
nÃ¤tverk Ã¤r dÃ¥

x

(1)

x(2)

(ï¸‚

1,2

)ï¸‚T

= âˆ†f1 âˆ†Pmech1 âˆ†PG1 âˆ†Ptie
,
(ï¸‚
)ï¸‚T
2,1
.
= âˆ†f2 âˆ†Pmech2 âˆ†PG2 âˆ†Ptie

59

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Eftersom eektÃ¶det fÃ¶r det andra systemet Ã¤r

Tuomas Virtanen

2,1
1,2
âˆ†Ptie
= âˆ’âˆ†Ptie

kan vi kombi-

nera dessa tillstÃ¥ndsvektorer till en tillstÃ¥ndsvektor:

(ï¸‚
)ï¸‚T
1,2
x = âˆ†f1 âˆ†Pmech1 âˆ†PG1 âˆ†f2 âˆ†Pmech2 âˆ†PG2 âˆ†Ptie
.
âˆ†f1 â†’ 0

FrÃ¥n (4.28) fÃ¥s att dÃ¥

och

1,2
âˆ†Ptie
â†’0

sÃ¥

âˆ†f2 â†’ 0.

(4.29)

Variablerna som

kan regleras Ã¤r referenspunkterna fÃ¶r belastningseekten pÃ¥ bÃ¥da nÃ¤tverken och

(ï¸‚
)ï¸‚T
u = âˆ†Pref1 âˆ†Pref2 . BelastningsfÃ¶rÃ¤ndringar simuleras som
(ï¸‚
)ï¸‚T
v = âˆ†PL1 âˆ†PL2 . Systemet kan nu skrivas pÃ¥ tillstÃ¥ndsformen

dÃ¥ Ã¤r insignalen
processbrus

Ì‡
x(t)
= Ax(t) + Bu(t) + Hv(t),
dÃ¤r systemmatriserna fÃ¥s ur (4.25)(4.28) som

â›

âˆ’ TP1 1

âœ
âœ
0
âœ
âœ
âœâˆ’ (TG11R1 )
âœ
A=âœ
0
âœ
âœ
âœ
0
âœ
âœ
0
â
2Ï€T1,2
B=
(ï¸„
H=

KP 1
TP 1
âˆ’ TT1 1

0

0

0

0

1

0

0

0

TT 1

0

1
âˆ’ TG1

0

0

0

0

0

âˆ’ TP1 2

0

0

0

0

KP 2
TP 2
âˆ’ TT1 2

0

0

âˆ’ (TG21R2 )

0

0

0

âˆ’2Ï€T1,2

0

(ï¸„
0 0 1/TG1 0 0
0 0

0

1
TT 2
1
âˆ’ TG2

0

0

0

0 0 âˆ’KP 2 /TP 2 0 0 0

0

âŸ
0 âŸ
âŸ
âŸ
0 âŸ
âŸ
KP 2 âŸ
,
TP 2 âŸ
âŸ
0 âŸ
âŸ
0 âŸ
â 
0

,

)ï¸„T
0 0 0

0

â

)ï¸„T

0 0 1/TG2 0

âˆ’KP 1 /TP 1 0 0

P1
âˆ’K
TP 1

.

DÃ¥ processbruset modelleras med en Wienerprocess beskrivs systemet av en stokastisk dierentialekvation

dx(t) = Ax(t)dt + Bu(t)dt + HdVt .
Systemet samplas med perioden

T

och dÃ¥ fÃ¥s det diskretiserade systemet

xk+1 = As xk + Bs uk + vk ,
dÃ¤r matriserna ges enligt sats 3.23 av

As = e

AT

(ï¸ƒâˆ«ï¸‚
,

Bs =

As

)ï¸ƒ

e ds B
0

60

T

(4.30)

KAPITEL 4.

OPTIMAL STOKASTISK REGLERING

Parameter NÃ¤tverk 1

Tuomas Virtanen

NÃ¤tverk 2

TP i

20 s

20 s

KP i

120 Hz/MW

120 Hz/MW

TT i

0.3 s

0.3 s

TGi

0.08 s

0.08 s

Ri

2.4 Hz/MW

2.4 Hz/MW

Tabell 4.2: AnvÃ¤nda vÃ¤rden pÃ¥ parametrar fÃ¶r simuleringen

och kovariansmatrisen fÃ¶r det diskreta processbruset ges av (3.41)

T

âˆ«ï¸‚
Pv =

eAÏ„ HPV (eAÏ„ H)T dÏ„,

0
dÃ¤r

PV

Ã¤r inkrementella kovariansmatrisen fÃ¶r Wienerprocessen

FÃ¶r simuleringen antar vi att

T1,2 = 0.545

âˆ’5

PV = 10 I ,

A s , Bs

och

Pv

1,2
âˆ†Ptie

MÃ¤tbruset

âˆ†f1

frÃ¥n det fÃ¶rsta nÃ¤tverket och ef-

mellan nÃ¤tverken, sÃ¥ att utsignalen Ã¤r

yk = Cxk + wk ,

10 I ,

kopplingslinans tidskonstant Ã¤r

berÃ¤knas numeriskt med Matlab.

De mÃ¤tbara variablerna Ã¤r frekvensen

âˆ’5

i (4.30).

och Ã¶vriga anvÃ¤nda vÃ¤rden pÃ¥ parametrarna fÃ¶r simuleringen hittas

i tabell 4.2. Matriserna

fektÃ¶det

Vt

wk

C=

(ï¸„
)ï¸„
1 0 0 0 0 0 0
0 0 0 0 0 0 1

antas vara vitt brus, med vÃ¤ntevÃ¤rdet

.

0 och kovariansmatrisen

oberoende av processbruset. Kostnadsmatriserna vÃ¤ljs enligt

Q = 5 C TC ,

R=I

och

detta sÃ¥ att kostnaden Ã¤r endast pÃ¥ de uppmÃ¤tta tillstÃ¥nden och

Ã¤r stÃ¶rre Ã¤n kostnaden pÃ¥ regleringen av systemet.
Vi antar att systemet har kÃ¶rts en tid och Ã¤r i ett stationÃ¤rt tillstÃ¥nd. Detta
motiverar att starttillstÃ¥ndets vÃ¤ntevÃ¤rde Ã¤r

m0 = 0

och stationÃ¤ra tillstÃ¥ndets

kovariansmatris kan berÃ¤knas med hjÃ¤lp av sats 3.17 genom att lÃ¶sa den algebraiska Riccatiekvationen (3.34). LÃ¶sningen hittas med Matlab-funktionen `idare'
och denna kovariansmatris anvÃ¤nds som

P0

fÃ¶r simuleringen.

Vi kan nu tillÃ¤mpa Sats 4.8 fÃ¶r att reglera systemet optimalt. Matlab-koden
fÃ¶r denna simulering hittas i bilaga C.2. Varje simulering gÃ¶rs med en belastningsÃ¶kning pÃ¥ nÃ¤tverk 2 vid

t=1

s. PÃ¥ gurerna ses mÃ¤tningen av frekvensen pÃ¥

nÃ¤tverk 1 uppe till vÃ¤nster, mÃ¤tningen av belastningen pÃ¥ kopplingslinan uppe
till hÃ¶ger och de nedre graferna Ã¤r insignalen till respektive nÃ¤tverk.

61

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

Figur 4.1: MÃ¤tning varje sekund, samplingstid T=1s.

KAPITEL 4.

62

Figur 4.2: MÃ¤tning var fjÃ¤rde sekund, samplingstid T=1s.

KAPITEL 4.
OPTIMAL STOKASTISK REGLERING

63

Tuomas Virtanen

OPTIMAL STOKASTISK REGLERING

Tuomas Virtanen

Figur 4.3: MÃ¤tning varje sekund, samplingstid T=0.1s.

KAPITEL 4.

64

KAPITEL 5.

SAMMANFATTANDE DISKUSSION

Tuomas Virtanen

Kapitel 5
Sammanfattande diskussion
Arbetet betraktade det linjÃ¤rkvadratiska gaussiska reglerproblemet och estimeringsproblemet fÃ¶r stokastiska system. Kalmanltret som ger den bÃ¤sta skattningen av tillstÃ¥ndsvektorn med avseende pÃ¥ det kvadratiska medelfelet delades upp
i Kalmanprediktorn och Kalmankorrektorn. En uppdelning av Kalmanltret Ã¤r
inte vanligt i litteraturen och dÃ¤rfÃ¶r bestod mycket av arbetet att rigorÃ¶st bevisa
att prediktorn och korrektorn ger optimala skattningar av tillstÃ¥ndsvektorn givet den tillgÃ¤ngliga informationen om systemet. Speciellt var det nÃ¶dvÃ¤ndigt att
hitta ett sÃ¤tt att fÃ¥ fram det betingade vÃ¤ntevÃ¤rdet och kovariansen fÃ¶r gaussiska vektorer utan att anvÃ¤nda tÃ¤thetsfunktionen eller fÃ¶rdelningsfunktionen fÃ¶r
normalfÃ¶rdelningen vilket man ofta anvÃ¤nder som denitioner fÃ¶r normalfÃ¶rdelningen. Detta eftersom tillstÃ¥ndsvektorns kovariansmatris kan i vissa fall vara
singulÃ¤r, vilket den inte fÃ¥r vara i tÃ¤thetsfunktionen.
Behovet av uppdelningen av Kalmanltret kommer fram dÃ¥ mÃ¤tsignalen inte
Ã¤r tillgÃ¤nglig vid varje tidpunkt men en optimal skattning av tillstÃ¥ndet behÃ¶vs
fÃ¶r regleringen. FÃ¶r att tillÃ¤mpa den hÃ¤rledda teorin och pÃ¥ nÃ¥got sÃ¤tt fÃ¥ fram
data att optimal reglering fungerar med prediktorn och korrektorn beslÃ¶ts att
anvÃ¤nda MatLab fÃ¶r att simulera reglering av tvÃ¥ kopplade elnÃ¤tverk. Dierentialekvationerna och parametervÃ¤rdena som beskriver kopplade elnÃ¤tverk framkom ur artiklarna som anvÃ¤ndes. Ã„ven om MatLab har fÃ¤rdiga funktioner fÃ¶r
att sampla systemet och berÃ¤kna KalmanfÃ¶rstÃ¤rkningen fÃ¶r att reglera systemet
sÃ¥ anvÃ¤ndes dessa inte eftersom de nÃ¶dvÃ¤ndigtvis inte Ã¤r exakt samma som presenterats i denna avhandling. IstÃ¤llet nns dessa funktioner i programmet som
hittas i bilagan.

65

KAPITEL 5.

SAMMANFATTANDE DISKUSSION

Tuomas Virtanen

MÃ¥let med att reglera de kopplade elnÃ¤tverken Ã¤r att hÃ¥lla frekvensen fÃ¶r elnÃ¤tet stabilt. Simuleringarna gjordes sÃ¥ att det ena elnÃ¤tverket utsÃ¤tts fÃ¶r en stor
belastningsÃ¶kning och olika samplingstider och mÃ¤tintervall undersÃ¶ktes. Som
det framkommer ur gurerna 4.1, 4.2 och 4.3 sÃ¥ fungerar Kalmanprediktorn utmÃ¤rkt dÃ¥ samplingstiden och dÃ¤rmed reglerintervallet Ã¤r mycket kortare jÃ¤mfÃ¶rt
med hur ofta en mÃ¤tning Ã¤r tillgÃ¤nglig. Det lÃ¶nar sig alltsÃ¥ att anvÃ¤nda Kalmanprediktorn fÃ¶r att optimalt reglera systemet och att ha en kort samplingstid Ã¤ven
om nya mÃ¤tningnar Ã¤r sÃ¤llan tillgÃ¤ngliga.
Det Ã¤r vÃ¤rt att poÃ¤ngtera att under projektets gÃ¥ng framkom det ocksÃ¥ att
om systemet Ã¤r sÃ¥dant att man Ã¶nskar hÃ¥lla det i ett stationÃ¤rt tillstÃ¥nd och
vissa villkor fÃ¶r systemmatriserna samt kovariansmatriserna fÃ¶r starttillstÃ¥ndet
uppfylls sÃ¥ Ã¤r det stationÃ¤ra Kalmanltret ett utmÃ¤rkt verktyg fÃ¶r att berÃ¤kna
kovariansmatrisen fÃ¶r systemet.
Det kan konstateras att samplingen av ett tidskontinuerligt stokastiskt system
gicks inte igenom grundligt och endast ekvationen fÃ¶r tillstÃ¥ndsvektorn samplades. Skulle mÃ¤tsignalen och mÃ¤tbruset ocksÃ¥ fÃ¶rst modelleras som en stokastisk
dierentialekvation ger samplingen upphov till att processbruset och mÃ¤tbruset
inte nÃ¶dvÃ¤ndigtvis Ã¤r oberoende av varandra efter samplingen. I detta fall behÃ¶vs
ocksÃ¥ teorin och formlerna fÃ¶r estimeringen och optimala regleringen uppdateras
dÃ¤r istÃ¤llet fÃ¶r att ha tvÃ¥ olika oberoende processer fÃ¶r stÃ¶rningen i form av processbruset och mÃ¤tbruset anvÃ¤nds endast en stÃ¶rningsprocess. Detta gÃ¶rs delvis
i [HRS07] men dÃ¤r Ã¤r Kalmanltret inte uppdelat. Det kunde ocksÃ¥ vara intressant att undersÃ¶ka hur teorin kunde utvecklas dÃ¥ mÃ¤tsignalen ger information
om olika tillstÃ¥ndsvariabler vid olika tidpunkter.

66

BILAGA A.

MATRISTEORI

Tuomas Virtanen

Bilaga A
Matristeori
A.1 Matrisalgebra
LÃ¥t

A

vara en

mÃ—n

â›
âœ
A=âœ
â
huvuddiagonalen

ai j , i = 1, . . . , m; j = 1, . . . , n.
â

matris med reella element

A

fÃ¶r matrisen

a1 1

Â·Â·Â·

a1 n

.
.
.

..

.
.
.

.

am 1 Â· Â· Â·

âŸ
âŸ,
â 

am n

ges av elementen

a1 1 , a2 2 , . . . , aq q ,

dÃ¤r

q =

min(m, n).
Transponatet
elementen

av matrisen

A

betecknas med

AT

och Ã¤r en

nÃ—m

matris med

aj i .
â›
âœ
AT = âœ
â

FÃ¶r de reella matriserna

A

och

B

a1 1 Â· Â· Â·
.
.
.

..

am 1
.
.
.

.

a1 n Â· Â· Â·

â
âŸ
âŸ
â 

am n

gÃ¤ller att

(AB)T = B T AT .
En

kvadratisk

matris Ã¤r en matris fÃ¶r vilken

m = n, det vill sÃ¤ga den har lika

mÃ¥nga rader som kolonner. En kvadratisk matris Ã¤r

diagonalmatris

symmetrisk

om

AT = A. En

Ã¤r en kvadratisk matris fÃ¶r vilken gÃ¤ller att alla element utanfÃ¶r

huvuddiagonalen Ã¤r nollor.

Identitetsmatrisen I

element pÃ¥ huvuddiagonalen Ã¤r ettor. En matris

67

Ã¤r en diagonalmatris, dÃ¤r alla

A

Ã¤r

ortogonal

om

AT A = I .

BILAGA A.

Rangen

MATRISTEORI

av en

mÃ—n

matris

Tuomas Virtanen

A

anger hur mÃ¥nga linjÃ¤rt oberoende rader eller

kolonner matrisen har. Det gÃ¤ller alltid att

rang A â‰¤ min(m, n)
och om

rang A = min(m, n)

Ã¤r matrisen
En

surjektiv

nÃ—n

matris

âˆ’1

att

A A=I

en

hÃ¶gerinvers

och

och om

A

âˆ’1

= I,

till matrisen

inverterbara matriser

A

rang A = n

inverterbar

Ã¤r

AA

sÃ¥ sÃ¤gs matrisen ha

och

Lemma A.1.

sÃ¥ Ã¤r matrisen

A.

Inversen

B

gÃ¤ller att

A

Aâˆ’1

och

âˆ’1

Om

rang A = m

sÃ¥

injektiv.

nÃ—n

om det nns en

det vill sÃ¤ga

(AT )âˆ’1 = (Aâˆ’1 )T

full rang.

matris

Ã¤r bÃ¥de en

Aâˆ’1 ,

sÃ¥dan

vÃ¤nsterinvers

och

Ã¤r entydig dÃ¥ den existerar. FÃ¶r

(AB)âˆ’1 = B âˆ’1 Aâˆ’1 .

LÃ¥t S âˆˆ RmÃ—n och T âˆˆ RnÃ—m vara godtyckliga matriser. DÃ¥ Ã¤r

ST + I inverterbar om och endast om T S + I Ã¤r inverterbar och det gÃ¤ller att
(ST + I)âˆ’1 = I âˆ’ S(T S + I)âˆ’1 T .
Bevis.

Anta att

ST + I

Ã¤r inverterbar och sÃ¤tt

U = I âˆ’ T (ST + I)âˆ’1 S .

DÃ¥

gÃ¤ller att

(ï¸
)ï¸
(T S + I)U = (T S + I) I âˆ’ T (ST + I)âˆ’1 S
= T S âˆ’ T ST (ST + I)âˆ’1 S + I âˆ’ T (ST + I)âˆ’1 S
= T S + I âˆ’ T (ST + I)(ST + I)âˆ’1 S
=I
och

(ï¸
)ï¸
U (T S + I) = I âˆ’ T (ST + I)âˆ’1 S (T S + I)
= T S + I âˆ’ T (ST + I)âˆ’1 ST S âˆ’ T (ST + I)âˆ’1 S
= T S + I âˆ’ T (ST + I)âˆ’1 (ST + I)S
= I.
SÃ¥ledes Ã¤r
Ã¤r

ST + I

U

inversen av

T S + I.

Analogt fÃ¥s att dÃ¥

inverterbar och inversen ges av

68

TS + I

Ã¤r inverterbar sÃ¥

I âˆ’ S(T S + I)âˆ’1 T .

BILAGA A.

SpÃ¥ret

MATRISTEORI

av en

mÃ—n

Tuomas Virtanen

A,

matris

vilket betecknas

tr(A),

Ã¤r summan av huvud-

diagonalelementen

tr(A) = a1 1 + . . . + aq q ,
dÃ¤r

q = min(m, n).

tr(B).

Det Ã¤r klart att

tr(AT ) = tr(A)

och

tr(A + B) = tr(A) +

Dessutom gÃ¤ller att

tr(AB) = tr(BA),
ifall multiplikationerna
LÃ¥t

A

AB

BA

och

vara symmetrisk och

(A.1)

Ã¤r denierade.

x = (x1 , x2 , . . . , xn )T

dÃ¥ Ã¤r funktionen

Q(x) := xT Ax
en

kvadratisk form.

A â‰¥ 0,

Matrisen

A

sÃ¤gs vara

positivt semidenit,

vilket betecknas

om den Ã¤r symmetrisk och det gÃ¤ller att kvadratiska formen Ã¤r icke-

negativ fÃ¶r varje vektor

x,

det vill sÃ¤ga

xT Ax â‰¥ 0,

âˆ€x.

Om det Ã¤ven gÃ¤ller att

xT Ax > 0,
sÃ¥ sÃ¤gs

A

positivt denit,

vara

Sats A.2.

âˆ€x Ì¸= 0

vilket betecknas

A > 0.

En positivt semidenit matris A Ã¤r positivt denit om och endast om

A Ã¤r inverterbar.
Se [HJ12, Korollarium 7.1.7, s. 431] fÃ¶r bevis.

Lemma A.3.

LÃ¥t P vara en positivt semidenit n Ã— n matris och lÃ¥t T vara en

n Ã— m matris. DÃ¥ Ã¤r T T P T positivt semidenit.
Bevis.

LÃ¥t

x âˆˆ Rm

och

y = T x.

DÃ¥ gÃ¤ller att

xT T T P T x = y T P y â‰¥ 0.

Lemma A.4.

LÃ¥t A vara en positivt (semi)denit matris. DÃ¥ nns en en-

tydig positivt (semi)denit kvadratrot av A, vilket betecknas A1/2 , sÃ¥dan att

A1/2 A1/2 = A.
Lemmat Ã¤r ett specialfall av [HJ12, Sats 7.2.6, s. 439].

69

BILAGA A.

MATRISTEORI

Tuomas Virtanen

A.2 Matrisexponentialfunktionen
Matrisexponentialfunktionen

Ã¤r en funktion fÃ¶r kvadratiska matriser och de-

nieras med hjÃ¤lp av Taylorutvecklingen fÃ¶r den skalÃ¤ra exponentialfunktionen
enligt

At

e

âˆ k k
âˆ‘ï¸‚
t A

:=

k!

k=0
dÃ¤r

= I + tA +

t2
A2
2

+ ...,

(A.2)

A Ã¤r en godtycklig kvadratisk matris. Denitionen Ã¤r meningsfull om matris-

serien (A.2) konvergerar. Detta visas med hjÃ¤lp av

matrisnormen, som denieras

enligt

|Ax|
.
xÌ¸=0 |x|

âˆ¥Aâˆ¥ = sup

Lemma A.5.

FÃ¶ljande egenskaper gÃ¤ller fÃ¶r matrisnormen:

1. |Ax| â‰¤ âˆ¥Aâˆ¥ Â· |x|,
2. |ajk | â‰¤ âˆ¥Aâˆ¥ fÃ¶r alla matriselement ajk ,
(ï¸„ n
)ï¸„1/2
âˆ‘ï¸‚
3. âˆ¥Aâˆ¥ â‰¤
|ajk |2
,
j,k=1

4. âˆ¥A + Bâˆ¥ â‰¤ âˆ¥Aâˆ¥ + âˆ¥Bâˆ¥,
5. âˆ¥ABâˆ¥ â‰¤ âˆ¥Aâˆ¥ Â· âˆ¥Bâˆ¥,
6. âˆ¥tAâˆ¥ = |t| âˆ¥Aâˆ¥

(t âˆˆ C).

FÃ¶r bevis se [AB89, Lemma 2.2, s. 90].
FrÃ¥n egenskaperna

5.

och

6.

fÃ¥s nu att

âƒ¦ k kâƒ¦ âƒ“ kâƒ“
âƒ¦t A âƒ¦ âƒ“t âƒ“
k
âƒ¦
âƒ¦ âƒ“ âƒ“
âƒ¦ k! âƒ¦ â‰¤ âƒ“ k! âƒ“ âˆ¥Aâˆ¥ ,
dÃ¤r hÃ¶gra ledet Ã¤r en term i en konvergent serie. Det fÃ¶ljer att

âˆ âƒ¦ k kâƒ¦
âˆ‘ï¸‚
âƒ¦t A âƒ¦
âƒ¦
âƒ¦
âƒ¦ k! âƒ¦ < âˆ,
k=0

vilket visar att matrisserien (A.2) Ã¤r konvergent.

70

BILAGA A.

Sats A.6.

MATRISTEORI

Tuomas Virtanen

FÃ¶ljande egenskaper gÃ¤ller fÃ¶r matrisexponentialfunktionen:

1. eA+B = eA Â· eB

om AB = BA,

2. (eA )âˆ’1 = eâˆ’A ,
3.

d At
e
dt

= AeAt = eAt A,

t âˆˆ R.

FÃ¶r bevis se [AB89, Sats 2.2, s. 93], likheten

AeAt = eAt A i egenskap 3. fÃ¶ljer

frÃ¥n denitionen (A.2).
Som fÃ¶ljd av egenskap

1.

och denitionen (A.2) fÃ¥s, dÃ¥

eA Â· eâˆ’A = eAâˆ’A = e0 = I,
det vill sÃ¤ga

eA

och

eâˆ’A

Ã¤r varandras inverser.

71

B = âˆ’A,

att

BILAGA B.

SANNOLIKHETSTEORI

Tuomas Virtanen

Bilaga B
Sannolikhetsteori
Som kÃ¤lla till det hÃ¤r kapitlet har i huvudsak [JP03], [Ros10] och [DV85] anvÃ¤nts.

B.1 GrundlÃ¤ggande sannolikhetslÃ¤ra
LÃ¥t

â„¦ vara en icke-tom mÃ¤ngd med elementen Ï‰ . MÃ¤ngden â„¦ kallas utfallsrummet

Denition B.1.
mÃ¤ngden

â„¦

En familj

A

â„¦

av delmÃ¤ngder av

sÃ¤gs vara en

Ïƒ -algebra

Ã¶ver

om det gÃ¤ller att

1.

â„¦ âˆˆ A,

2.

A âˆˆ A =â‡’ Ac âˆˆ A

3.

A1 , A2 , . . . âˆˆ A =â‡’

âˆ
â‹ƒï¸

Ai âˆˆ A.

i=1
Elementen

Ai i Ïƒ -algebran A

Denition B.2.
Ïƒ -algebran A
1.

2.

En avbildning

kallas

hÃ¤ndelser.

P : A â†’ [0, 1]

sÃ¤gs vara ett

sannolikhetsmÃ¥tt

pÃ¥

om det gÃ¤ller att

P(â„¦) = 1
(ï¸ƒ âˆ )ï¸ƒ
âˆ
â‹ƒï¸
âˆ‘ï¸
P
Ai =
P(Ai ),
i=1

dÃ¥

A1 , A2 , . . . âˆˆ A

och

Ai âˆ© Aj = âˆ…, i Ì¸= j.

i=1

SannolikhetsmÃ¥ttet ger sannolikheten fÃ¶r varje hÃ¤ndelse

Ai âˆˆ A.

Denition B.3. Ett sannolikhetsrum ges av trippeln (â„¦, A, P), dÃ¤r â„¦ Ã¤r en icketom mÃ¤ngd,
pÃ¥

A

Ã¤r

Ïƒ -algebran

av delmÃ¤ngder av

A.
72

â„¦

och

P

Ã¤r sannolikhetsmÃ¥ttet

BILAGA B.

SANNOLIKHETSTEORI

Denition B.4.

LÃ¥t

(â„¦, A, P)

Tuomas Virtanen

vara ett sannolikhetsrum. En avbildning

X:â„¦â†’R
stokastisk variabel

sÃ¤gs vara en

om fÃ¶r varje

xâˆˆR

gÃ¤ller att

{Ï‰ âˆˆ â„¦ : X(Ï‰) â‰¤ x} âˆˆ A
och dÃ¥ sÃ¤gs

X

vara en

X:â„¦â†’R

LÃ¥t

mÃ¤tbar

funktion med avseende pÃ¥

Ïƒ -algebran A.

vara en stokastiskt variabel. DÃ¥ Ã¤r

Ïƒ(X) := {X âˆ’1 (A) | A âˆˆ A}
Ïƒ -algebra

en

algebran som

B.1.1

och kallas

X

Ïƒ -algebran som genereras av X .

Det Ã¤r den minsta

Ïƒ-

Ã¤r mÃ¤tbar med avseende pÃ¥.

VÃ¤ntevÃ¤rde och varians

Denition B.5.
variabel.

LÃ¥t

VÃ¤ntevÃ¤rdet

(â„¦, A, P)
av

X

X

vara ett sannolikhetsrum och

en stokastisk

ges av

âˆ«ï¸‚
Xd P .

E(X) :=
â„¦
FÃ¶r att vÃ¤ntevÃ¤rdet ska existera krÃ¤vs att

âˆ«ï¸‚
|X| d P < âˆ

E(|X|) =
â„¦
och dÃ¥ sÃ¤gs

X

Variansen

vara
fÃ¶r

X

integrerbar.
ges av

âˆ«ï¸‚
Var(X) :=

(X âˆ’ E(X))2 d P .

â„¦
Notera att

(ï¸
)ï¸
Var(X) = E (X âˆ’ E(X))2 = E(X 2 ) âˆ’ E(X)2
och om

X2

Ã¤r integrerbar, det vill sÃ¤ga

E(X 2 ) < âˆ,

sÃ¥ Ã¤r

Var(X)

(B.1)
vÃ¤ldenierat.

VÃ¤ntevÃ¤rdet Ã¤r en linjÃ¤r avbildning, det vill sÃ¤ga fÃ¶r stokastiska variablerna

X

och

Y

och ett tal

Î²âˆˆR

gÃ¤ller att

E(Î²X) = Î² E(X)
Om

X

och

Y

och

E(X + Y ) = E(X) + E(Y ).

Ã¤r integrerbara stokastiska variabler, sÃ¥dana att

att

E(X) â‰¤ E(Y ).
73

X â‰¤ Y,

sÃ¥ gÃ¤ller

BILAGA B.

B.1.2

SANNOLIKHETSTEORI

Tuomas Virtanen

FÃ¶rdelningsfunktion

Denition B.6.

(â„¦, A, P) vara ett sannolikhetsrum och lÃ¥t X : â„¦ â†’ R vara

LÃ¥t

en stokastisk variabel.

FÃ¶rdelningsfunktionen fÃ¶r X

Ã¤r funktionen

FX : R â†’ [0, 1]

och bestÃ¤ms av

FX (x) := P(X â‰¤ x)
Om

fÃ¶r varje

x âˆˆ R.

X1 , . . . , Xn : â„¦ â†’ R Ã¤r stokastiska variabler ges deras simultana fÃ¶rdelnings-

funktion

av

FX1 ,...,Xn : Rn â†’ [0, 1]

genom

FX1 ,...,Xn (x1 , . . . , xn ) := P(X1 â‰¤ x1 , . . . , Xn â‰¤ xn )

Denition B.7.

LÃ¥t

xi âˆˆ R, i = 1, . . . , n.

X : â„¦ â†’ R vara en stokastisk variabel och lÃ¥t FX

delningsfunktionen fÃ¶r

fX : R â†’ R,

fÃ¶r varje

vara fÃ¶r-

X . Om det existerar en icke-negativ, integrerbar funktion

sÃ¥dan att

âˆ«ï¸‚

x

FX (x) =

fX (u)du,
âˆ’âˆ

sÃ¥ sÃ¤gs

fX

vara

Lemma B.8.

tÃ¤theten

fÃ¶r

X.

LÃ¥t X : â„¦ â†’ R vara en integrerbar stokastisk variabel med tÃ¤t-

hetsfunktionen fX . LÃ¥t g : R â†’ R och Y = g(X). DÃ¥ gÃ¤ller att
âˆ«ï¸‚
g(u)fX (u)du.
E(Y ) =
R

Speciellt fÃ¥s att

âˆ«ï¸‚

ufX (u)du och

E(X) =

âˆ«ï¸‚
Var(X) =

R

(u âˆ’ E(X))2 fX (u)du.

R

NormalfÃ¶rdelningen
Den stokastiska variabeln
parametrarna

Om det dessutom

X

Ã¤r

normalfÃ¶rdelad

gaussisk

eller

med

2

Âµ âˆˆ R och Ïƒ > 0, vilket betecknas med X âˆ¼ N (Âµ, Ïƒ ), om den har

tÃ¤theten

har

X : â„¦ â†’ R

1 xâˆ’Âµ 2
1
fX (x) = âˆš eâˆ’ 2 ( Ïƒ ) x âˆˆ R.
Ïƒ 2Ï€
tillÃ¥ts att Ïƒ = 0 sÃ¤gs X vara singulÃ¤rt normalfÃ¶rdelad

och dÃ¥

ingen tÃ¤thet.

Det gÃ¤ller att vÃ¤ntevÃ¤rdet och variansen fÃ¶r en normalfÃ¶rdelad stokastisk
variabel ges av

E(X) = Âµ,

Var(X) = Ïƒ 2 ,

vilket kan verieras med hjÃ¤lp av lemma B.8. Dessa parametrar bestÃ¤mmer entydigt fÃ¶rdelningen fÃ¶r en normalfÃ¶rdelad stokastisk variabel.

74

BILAGA B.

B.1.3
LÃ¥t
Den

SANNOLIKHETSTEORI

Tuomas Virtanen

Oberoende stokastiska variabler

(â„¦, A, P) vara ett sannolikhetsrum och A, B âˆˆ A Ã¤r hÃ¤ndelser med P(B) > 0.
betingade sannolikheten

fÃ¶r hÃ¤ndelsen

P(A | B) :=
Man kan visa att

P(A | B)

Denition B.9.

HÃ¤ndelserna

A

givet hÃ¤ndelsen

B

ges av

P(A âˆ© B)
.
P(B)

(B.2)

Ã¤r ett sannolikhetsmÃ¥tt.

A

och

B

sÃ¤gs vara

oberoende

om

P(A âˆ© B) = P(A) P(B).
Om

A

och

B

Ã¤r oberoende hÃ¤ndelser fÃ¥s att

P(A | B) =

Denition B.10.
1. LÃ¥t

LÃ¥t

P(A âˆ© B)
P(A) P(B)
=
= P(A).
P(B)
P(B)

(â„¦, A, P)

vara ett sannolikhetsrum.

(Ai )iâˆˆI âŠ† A vara Ïƒ -algebror. DÃ¥ sÃ¤gs Ai

vara

oberoende Ïƒ -algebror

J âŠ‚ I och varje hÃ¤ndelse Ai âˆˆ Ai
(ï¸„
)ï¸„
âˆï¸‚
â‹‚ï¸‚
P
P(Ai ).
Ai =

fÃ¶r varje Ã¤ndlig indexmÃ¤ngd

2. Stokastiska variabler

Ïƒ(Xi )

Sats B.11.

Ã¤r oberoende

(Xi )iâˆˆI

om

gÃ¤ller att

iâˆˆJ

iâˆˆJ

sÃ¤gs vara

oberoende stokastiska variabler

om

Ïƒ -algebror.

LÃ¥t f, g vara integrerbara funktioner och X, Y stokastiska variabler.

DÃ¥ gÃ¤ller att f (X) och g(Y ) Ã¤r oberoende och

(ï¸
)ï¸
E f (X)g(Y ) = E(f (X)) E(g(Y )).

Denition B.12.

Kovariansen

mellan tvÃ¥ stokastiska variabler

(B.3)

X

och

Y

med

Ã¤ndliga varianser denieras enligt

(ï¸
)ï¸
Cov(X, Y ) := E (X âˆ’ E(X))(Y âˆ’ E(Y )) = E(XY ) âˆ’ E(X) E(Y ).
Om

Cov(X, Y ) = 0

DÃ¥

X

och

Y

sÃ¥ sÃ¤gs

X

och

Y

vara

(B.4)

okorrelerade.

Ã¤r oberoende sÃ¥ gÃ¤ller enligt (B.3) att

E(XY ) = E(X) E(Y ) och

sÃ¥ledes Ã¤r oberoende stokastiska variabler ocksÃ¥ okorrelerade, men omvÃ¤ndningen
gÃ¤ller vanligtvis inte.

75

BILAGA B.

B.1.4

SANNOLIKHETSTEORI

Tuomas Virtanen

Betingat vÃ¤ntevÃ¤rde

Denition B.13.

LÃ¥t

(â„¦, A, P)

stokastiska variabler pÃ¥

â„¦.

Det

vara ett sannolikhetsrum och lÃ¥t

X

betingade vÃ¤ntevÃ¤rdet av X givet Y

och

Y

Ã¤r en

vara

Ïƒ(Y )-

mÃ¤tbar stokastisk variabel, betecknad

E(X | Y ),
sÃ¥dan att

âˆ«ï¸‚

âˆ«ï¸‚
E(X | Y )d P

Xd P =
A

fÃ¶r varje

A âˆˆ Ïƒ(Y ).

A

Denition B.14.

LÃ¥t

stokastisk variabel pÃ¥

(â„¦, A, P)

vara ett sannolikhetsrum och

X

en integrerbar

â„¦. LÃ¥t V âŠ† A vara en Ïƒ -algebra. Det betingade vÃ¤ntevÃ¤rdet
E(X | V)

Ã¤r en

V -mÃ¤tbar

stokastisk variabel pÃ¥

â„¦,

fÃ¶r vilken det gÃ¤ller att

âˆ«ï¸‚

âˆ«ï¸‚

E(X | V)d P

Xd P =

fÃ¶r varje

A âˆˆ V.

A

A

FrÃ¥n denitionerna ser man att

E(X | Y ) = E(X | Ïƒ(Y )).
Nedan fÃ¶ljer viktiga resultat fÃ¶r det betingade vÃ¤ntevÃ¤rdet. FÃ¶r bevis av resultaten, se [JP03, Kapitel 23, s. 197-207].

Sats B.15.

LÃ¥t (â„¦, A, P) vara ett sannolikhetsrum. LÃ¥t X och Y vara integrer-

bara stokastiska variabler pÃ¥ â„¦ och V âŠ† A Ã¤r en Ïƒ -algebra. FÃ¶ljande pÃ¥stÃ¥enden
gÃ¤ller
1. Avbildningen X â†’ E(X | Y ) Ã¤r linjÃ¤r.
2. Det nns en funktion f , sÃ¥dan att E(X | Y ) = f (Y ).

(ï¸
)ï¸
3. E E(X | Y ) = E(X).
4. E(X | Y ) = E(X) om X och Y Ã¤r oberoende.
5. E(XY | V) = X E(Y | V) om XY Ã¤r integrerbar och X Ã¤r V -mÃ¤tbar.
6. FÃ¶r konstanter a, b âˆˆ R gÃ¤ller att E(aX+bY | V) = a E(X | V)+b E(Y | V).
76

BILAGA B.

SANNOLIKHETSTEORI

Tuomas Virtanen

7. Olikheten X â‰¤ Y implicerar att E(X | V) â‰¤ E(Y | V).
Betingade variansen

fÃ¶r

X

givet

Y

denieras enligt

(ï¸
)ï¸
Var(X | Y ) := E (X âˆ’ E(X | Y ))2 | Y
betingade kovariansen

och

kastiska variabeln

Z

mellan tvÃ¥ stokastiska variabler

X

och

Y

givet sto-

denieras enligt

(ï¸
)ï¸
Cov(X, Y | Z) := E (X âˆ’ E(X | Z))(Y âˆ’ E(Y | Z)) | Z .

B.2 Flerdimensionella stokastiska variabler
LÃ¥t

X1 , . . . , Xn

vara stokastiska variabler pÃ¥ sannolikhetsrummet

T
kallas kolonnvektorn x := (X1 , . . . , Xn )
en

en

(â„¦, A, P).

n-dimensionell stokastisk vektor

DÃ¥

eller

n-dimensionell stokastisk variabel.
x = (X1 , . . . , Xn )T ges av
âˆ«ï¸‚
(ï¸‚âˆ«ï¸
)ï¸‚T (ï¸‚
)ï¸‚T
âˆ«ï¸
E(x) :=
xd P = â„¦ X1 d P . . . â„¦ Xn d P = E(X1 ) . . . E(Xn ) .
VÃ¤ntevÃ¤rdet av en stokastisk vektor

â„¦

(B.5)

stokastisk matris genom
â›
â
X1,1 . . . X1,n
âœ .
âŸ
. âŸ
..
.
.
M =âœ
.
.
.
â
â 
Xm,1 . . . Xm,n

PÃ¥ samma sÃ¤tt denieras en

och vÃ¤ntevÃ¤rdet av en stokastisk matris ges av

â›

E(X1,1 ) . . . E(X1,n )
.
.
.

âœ
E(M ) := âœ
â

..

.

.
.
.

â
âŸ
âŸ.
â 

(B.6)

E(Xm,1 ) . . . E(Xm,n )
Lineariteten hos vÃ¤ntevÃ¤rdet gÃ¤ller ocksÃ¥ fÃ¶r stokastiska vektorer och matriser.
Det gÃ¤ller alltsÃ¥ fÃ¶r den

n-dimensionella

stokastiska vektorn

x

att

E(Ax + b) = A E(x) + b,
dÃ¤r

A

Ã¤r en konstant

Det gÃ¤ller ocksÃ¥ fÃ¶r

mÃ—n

matris och

n-dimensionella

b

en konstant

stokastiska vektorer

m-dimensionell
x1 , x2 , . . . , xk

E(x1 + x2 + . . . + xk ) = E(x1 ) + E(x2 ) + . . . + E(xk )
77

vektor.

att

BILAGA B.

SANNOLIKHETSTEORI

Kovariansmatrisen Px = Cov(x)

Tuomas Virtanen

fÃ¶r en stokastisk vektor

x = (X1 , . . . , Xn )T

denieras som

(ï¸
)ï¸
Px := E (x âˆ’ E(x))(x âˆ’ E(x))T

(B.7)

och enligt denitionerna (B.1), (B.4) och (B.6) fÃ¥s att

â›

â
. . . (X1 âˆ’ E(X1 ))(Xn âˆ’ E(Xn ))
âœ
âŸ
.
.
..
âŸ
.
.
Px = E âœ
.
.
.
â
â 
2
(Xn âˆ’ E(Xn ))(X1 âˆ’ E(X1 )) . . .
(Xn âˆ’ E(Xn ))
â›
â
Var(X1 )
. . . Cov(X1 , Xn )
âœ
âŸ
.
.
..
âŸ.
.
.
=âœ
.
.
.
â
â 
Cov(Xn , X1 ) . . .
Var(Xn )
(X1 âˆ’ E(X1 ))2

PÃ¥ samma sÃ¤tt denieras kovariansmatrisen
stokastiska vektorer

n

xâˆˆR

m

yâˆˆR

och

Px,y = Cov(x, y)

mellan tvÃ¥

som

(ï¸
)ï¸
Px,y := E (x âˆ’ E(x))(y âˆ’ E(y))T .
DÃ¥ alla par av variabler
Ã¤r oberoende fÃ¥s att

(Xi , Yj )

(B.8)

Ã¤r oberoende, det vill sÃ¤ga vektorerna

Px,y = 0 och

dÃ¥ sÃ¤gs vektorerna

x och y

x

och

y

vara okorrelerade,

omvÃ¤ndningen gÃ¤ller vanligtvis inte. FÃ¶ljande rÃ¤kneregler, som fÃ¶ljer direkt frÃ¥n
denitionerna och lineariteten hos vÃ¤ntevÃ¤rdet, gÃ¤ller fÃ¶r kovariansmatrisen.

Lemma B.16. LÃ¥t x âˆˆ Rn och y âˆˆ Rm vara stokastiska vektorer och lÃ¥t A och B
vara konstanta matriser, sÃ¥dana att multiplikationerna nedan Ã¤r vÃ¤ldenierade.
Det gÃ¤ller att
T .
1. Py,x = Px,y

2. Cov(Ax Â± By) = A Cov(x)AT Â± A Cov(x, y)B T

Â± B Cov(y, x)AT + B Cov(y)B T .

Denition B.17.

LÃ¥t

stokastiska vektorer pÃ¥

(â„¦, A, P)
â„¦.

Det

vara ett sannolikhetsrum och lÃ¥t

betingade vÃ¤ntevÃ¤rdet av x givet y

mÃ¤tbar stokastisk vektor, betecknad

E(x | y),
sÃ¥dan att

âˆ«ï¸‚

âˆ«ï¸‚
E(x | y)d P

xd P =
A

x

A
78

fÃ¶r varje

A âˆˆ Ïƒ(y).

och

y

Ã¤r en

vara

Ïƒ(y)-

BILAGA B.

SANNOLIKHETSTEORI

Tuomas Virtanen

betingade kovariansmatrisen Px|y = Cov(x | y)

Den
givet

y

fÃ¶r en stokastisk vektor

denieras som

(ï¸
)ï¸
Cov(x | y) := E (x âˆ’ E(x | y))(x âˆ’ E(x | y))T | y .
Den

x

(B.9)

betingade kovariansmatrisen mellan x och y givet z ges av
(ï¸‚(ï¸
)ï¸(ï¸
)ï¸T âƒ“ )ï¸‚
Cov(x, y | z) := E x âˆ’ E(x | z) y âˆ’ E(y | z) âƒ“ z .

(B.10)

FÃ¶ljande resultat fÃ¶ljer frÃ¥n resultaten fÃ¶r endimensionella fallet.

Sats B.18. LÃ¥t (â„¦, A, P) vara ett sannolikhetsrum. LÃ¥t x och y vara integrerbara
stokastiska variabler pÃ¥ â„¦ och Ïƒ(y) âŠ† A Ã¤r Ïƒ -algebran som genereras av y .
FÃ¶ljande pÃ¥stÃ¥enden gÃ¤ller
1. Avbildningen x â†’ E(x | Ïƒ(y)) Ã¤r linjÃ¤r.
2. Det nns en funktion f , sÃ¥dan att E(x | Ïƒ(y)) = f (y).

(ï¸
)ï¸
3. E E(x | Ïƒ(y)) = E(x).
4. Om x Ã¤r oberoende av y fÃ¥s att E(x | Ïƒ(y)) = E(x).
5. E(xz | Ïƒ(y)) = x E(z | Ïƒ(y)) om z och xz Ã¤r integrerbara och x Ã¤r

Ïƒ(y)-mÃ¤tbar.
6. Om x Ã¤r Ïƒ(y)-mÃ¤tbar fÃ¥s att E(x | Ïƒ(y)) = x och Cov(x | Ïƒ(y)) = 0.
7. Cov(Ax Â± Bz | Ïƒ(y)) = A Cov(x | Ïƒ(y))AT Â± A Cov(x, z | Ïƒ(y))B T

Â± B Cov(z, x | Ïƒ(y))AT + B Cov(z | Ïƒ(y))B T .
Om de stokastiska vektorerna
olikheten

xâ‰¤y

att

Xi â‰¤ Yi

x

och

fÃ¶r varje

y

Ã¤r av samma dimension

i = 1, . . . , n

E(x) â‰¤ E(y).

79

n

sÃ¥ betyder

och det gÃ¤ller att

(B.11)

BILAGA B.

B.2.1

SANNOLIKHETSTEORI

p-dimensionell

Denition B.19.
malfÃ¶rdelade

Tuomas Virtanen

normalfÃ¶rdelning

X1 , . . . , X p

sÃ¤gs vara

simultant nor-

om det gÃ¤ller fÃ¶r varje fÃ¶ljd av reella tal

a1 , . . . , a p

att den stokas-

Stokastiska variablerna

tiska variabeln

Z :=

p
âˆ‘ï¸‚

ai X i

i=1
Ã¤r normalfÃ¶rdelad. En stokastisk vektor
eller

p-dimensionellt normalfÃ¶rdelad

x = (X1 , . . . , Xp )T

sÃ¤gs vara

om de stokastiska variablerna

gaussisk

X1 , . . . , X p

Ã¤r

simultant normalfÃ¶rdelade.
Denna denition tillÃ¥ter att komponenterna kan vara singulÃ¤rt normalfÃ¶rdelade, i vilket fall vektorn

x

saknar tÃ¤thet i

Rp .

Ifall

x

har en tÃ¤thet sÃ¥ kan den

gaussiska vektorn denieras med hjÃ¤lp av den simultana tÃ¤thetsfunktionen.
Det Ã¤r uppenbart frÃ¥n denitionen att komponenterna

X1 , . . . , X p

i en

gaussisk vektor Ã¤r gaussiska. OmvÃ¤ndningen gÃ¤ller nÃ¶dvÃ¤ndigtvis inte, det vill
sÃ¤ga det faktum att tvÃ¥ stokastiska variabler
inte att vektorn

x = (X1 , X2 )T

X1

och

X2

Ã¤r gaussiska implicerar

Ã¤r gaussisk, det Ã¤r alltsÃ¥ viktigt att de Ã¤r ocksÃ¥

simultant normalfÃ¶rdelade.
Som fÃ¶r den normalfÃ¶rdelade stokastiska variabeln sÃ¥ bestÃ¤ms fÃ¶rdelningen fÃ¶r
den gaussiska vektorn

Cov(x) = Px ,

x entydigt av vÃ¤ntevÃ¤rdet E(x) = Âµ och kovariansmatrisen

vilket kan betecknas

x âˆ¼ Np (Âµ, Px ).

Nedan fÃ¶ljer nÃ¥gra viktiga resultat fÃ¶r gaussiska vektorer, dÃ¤r det tredje pÃ¥stÃ¥endet Ã¤r ett Ã¶verraskande och anvÃ¤ndbart resultat. FÃ¶r bevis, se [JP03, Sats
16.2, s. 129], [JP03, Sats 16.3, s. 130] och [JP03, Sats 16.4, s. 131].

Sats B.20.

LÃ¥t x vara en n-dimensionell gaussisk vektor med vÃ¤ntevÃ¤rdet Âµ och

lÃ¥t y vara en m-dimensionell gaussisk vektor oberoende av x. FÃ¶ljande pÃ¥stÃ¥enden gÃ¤ller
1. Det nns oberoende stokastiska variabler Z1 , . . . , Zn , dÃ¤r Zi âˆ¼ N (0, Î»i ),

Î»i â‰¥ 0, fÃ¶r varje 1 â‰¤ i â‰¤ n och en ortogonal matris A, sÃ¥dan att x =
Âµ + Az , dÃ¤r z = (Z1 , . . . , Zn )T .
(ï¸„ )ï¸„
x
2. Vektorn h =
Ã¤r gaussisk i Rn+m .
y
3. TvÃ¥ komponenter Xj och Xk av x = (X1 , . . . , Xn )T Ã¤r oberoende om och
endast om de Ã¤r okorrelerade.
80

BILAGA C.

PROGRAMKOD

Tuomas Virtanen

Bilaga C
Programkod
C.1 Exempel 3.24
1

clear;

clf;

2

t = 12;

%simuleringstid i sekunder

3

dt = 0.1;

%samplingsintervall

4

dy = 0.1;

%mÃ¤tintervall fÃ¶r bild 1

5

n = t/dt;

%antalet iterationer av simuleringen

6
7

%systemmatriser

8

A1 = [0, 0, 1,

9

A = expm(A1*dt);

0;0, 0,

0, 1;0, 0,

10

H = [0, 0, 1, 0;0, 0, 0, 1]';

11

C = [1, 0, 0, 0;0, 1, 0, 0];

0,

0;0, 0,

0,

0];

12
13

%processbrusets vÃ¤ntevÃ¤rde och kovarians

14

q1 = 1;

15

Pv = integral(@(s)expm(A1*s)*H*r*H'*expm(A1'*s),0,dt,'ArrayValued',true);

16

mv = zeros(size(A,1),1);

q2 = 1;

r = [q1^2, 0;0, q2^2];

F = eye(size(Pv,1));

17
18

%mÃ¤tbrusets vÃ¤ntevÃ¤rde och kovarians

19

r1 = 0.5;

20

mw = zeros(size(C,1),1);

r2 = 0.5;

Pw = [r1^2, 0;0, r2^2];
G = eye(size(Pw,1));

21
22

%starttillstÃ¥ndets vÃ¤ntevÃ¤rde och kovarians

23

m0 = zeros(size(A,1),1);

P0 = eye(size(A,1));

24

x0 = mvnrnd(m0,P0,1)';

p1 = plot(x0(1),x0(2),'b*');

25

81

hold on

BILAGA C.

PROGRAMKOD

Tuomas Virtanen

26

%allokering av variabler

27

x = cell(1,n);

y = cell(1,n);

28

x_pred = cell(1,n);

x_korr = cell(1,n);

29

P_pred = cell(1,n);

P_korr = cell(1,n);

x_est = cell(1,n);

30
31

if dy == dt %gÃ¶r mÃ¤tning vid t = 0 om mÃ¤tintervall = samplingsintervall

32
33

y0 = C*x0 + G*mvnrnd(mw,Pw,1)';
[x_est_0, P_est_0] = kf_korrektion(m0,P0,y0,C,G,Pw);

34

[x_pred{1},P_pred{1}] = kf_prediktion(x_est_0,P_est_0,A,F,Pv);

35

p3 = plot(y0(1),y0(2),'r.');

36

else

37

[x_pred{1},P_pred{1}] = kf_prediktion(m0,P0,A,F,Pv);

38

x_est_0 = m0;

39

end

40
41

p4 = plot(x_est_0(1),x_est_0(2),'k*');

42
43

for k = 1:n

44

%iterera tillstÃ¥ndet och berÃ¤kna prediktionen

45

if k == 1

46
47

x{1} = A*x0 + F*mvnrnd(mv,Pv,1)';
else

48
49

=

=

=

=

=

[x_pred{k},P_pred{k}] = kf_prediktion(x_pred{k 1},P_pred{k 1},A,F,Pv);

50
51

else

[x_pred{k},P_pred{k}] = kf_prediktion(x_korr{k 1},P_korr{k 1},A,F,Pv);

52
53
54

=

x{k} = A*x{k 1} + F*mvnrnd(mv,Pv,1)';
if isempty(x_korr{k 1})

end
end

55
56

%gÃ¶r mÃ¤tning och berÃ¤kna korrektion vid varje mÃ¤tintervall dy

57

if mod(k*dt,dy) == 0

58

y{k} = C*x{k} + G*mvnrnd(mw,Pw,1)';
[x_korr{k},P_korr{k}] = kf_korrektion(x_pred{k},P_pred{k},y{k},C,G,Pw);

59
60

end

61
62

if isempty(x_korr{k})
x_est{k} = x_pred{k};

63
64

else
x_est{k} = x_korr{k};

65
66

end

82

BILAGA C.

67

PROGRAMKOD

Tuomas Virtanen

if k == 1

==');

68

p2 = plot([x0(1),x{1}(1)],[x0(2),x{1}(2)],'b

69

p4 = plot([x_est_0(1),x_est{k}(1)],[x_est_0(2),x_est{k}(2)],'k ');

70

else

=

=

=

==');

71

p2 = plot([x{k 1}(1),x{k}(1)],[x{k 1}(2),x{k}(2)],'b

72

p4 = plot([x_est{k 1}(1),x_est{k}(1)],[x_est{k 1}(2),x_est{k}(2)],'k ');

=

73

end

74

if mod(k*dt,dy) == 0

75

=

p3 = plot(y{k}(1),y{k}(2),'r.');

76
77

=

end
end

78
79

legend([p1; p2; p3; p4],{'Starttillst{\aa}nd','Verklig position','M{\"a}tning','
Estimat'},'Interpreter','latex');

80

hold off

81
82

function [x,P] = kf_prediktion(x,P,A,F,Q)

83

x = A * x;

84

P = A * P * A' + F * Q * F';

85

end

86
87

function [x,P] = kf_korrektion(x,P,y,C,G,R)

=

88

K = P * C' * (C * P * C' + G * R * G')^( 1);

89

x = x + K * (y

90

P = P

91

=K

=C

* x);

* C * P;

end
FÃ¶ljande kod kÃ¶rs efter ovanstÃ¥ende kod fÃ¶r att berÃ¤kna prediktion och korrektion
med lÃ¤ngre mÃ¤tintervall men med samma realisering som i ovanstÃ¥ende kod.

1

dy = 0.4;

2

x_pred = cell(1,n);

x_korr = cell(1,n);

3

P_pred = cell(1,n);

P_korr = cell(1,n);

%mÃ¤tintervall fÃ¶r bild 2
x_est = cell(1,n);

4
5

[x_pred{1},P_pred{1}] = kf_prediktion(m0,P0,A,F,Pv);

6

x_est_0 = m0;

7
8
9

p1 = plot(x0(1),x0(2),'b*');
hold on
_
_
_
_
p4 = plot(x est 0(1),x est 0(2),'k*');

10
11
12

for k = 1:n
%berÃ¤na prediktionen

83

BILAGA C.

13

PROGRAMKOD

if k >1

Tuomas Virtanen

=

if isempty(x_korr{k 1})

14

=

=

=

=

[x_pred{k},P_pred{k}] = kf_prediktion(x_pred{k 1},P_pred{k 1},A,F,Pv);

15
16

else

[x_pred{k},P_pred{k}] = kf_prediktion(x_korr{k 1},P_korr{k 1},A,F,Pv);

17
18

end

19

end

20

%berÃ¤kna korrektion vid varje mÃ¤tintervall dy

21
22

if mod(k*dt,dy) == 0
[x_korr{k},P_korr{k}] = kf_korrektion(x_pred{k},P_pred{k},y{k},C,G,Pw);

23

end

24

if isempty(x_korr{k})
x_est{k} = x_pred{k};

25
26

else
x_est{k} = x_korr{k};

27
28

end

29
30

if k == 1

==');

31

p2 = plot([x0(1),x{1}(1)],[x0(2),x{1}(2)],'b

32

p4 = plot([x_est_0(1),x_est{k}(1)],[x_est_0(2),x_est{k}(2)],'k ');

33

else

=

=

=

==');

34

p2 = plot([x{k 1}(1),x{k}(1)],[x{k 1}(2),x{k}(2)],'b

35

p4 = plot([x_est{k 1}(1),x_est{k}(1)],[x_est{k 1}(2),x_est{k}(2)],'k ');

36

end

37

if mod(k*dt,dy) == 0

38

=

=

p3 = plot(y{k}(1),y{k}(2),'r.');

39
40

=

end
end

41
42

legend([p1; p2; p3; p4],{'Starttillst{\aa}nd','Verklig position','M{\"a}tning','
Estimat'},'Interpreter','latex');

hold off

43
44

function [x,P] = kf_prediktion(x,P,A,F,Q)

45

x = A * x;

46

P = A * P * A' + F * Q * F';

47

end

48

function [x,P] = kf_korrektion(x,P,y,C,G,R)

=

49

K = P * C' * (C * P * C' + G * R * G')^( 1);

50

x = x + K * (y

51

P = P

52

=K

=C

* x);

* C * P;

end

84

BILAGA C.

PROGRAMKOD

Tuomas Virtanen

C.2 Kopplade elnÃ¤tverk
1

clear;

clf;

2

t = 100;

%simuleringstid i sekunder

3

dy = 1;

%mÃ¤tintervall

4

dt = 1;

%samplingsintervall

5

du = dt;

%reglerintervall

6

n = t/dt;

7
8

tp1 = 20;

tp2 = 20;

9

kp1 = 120;

kp2 = 120;

10

tt1 = 0.3;

tt2 = 0.3;

11

tg1 = 0.08;

tg2 = 0.08;

12

r1 = 2.4;

r2 = 2.4;

13

t12 = 0.545;

14
15

%systemmatriser

16

A1 = [ 1/tp1, kp1/tp1,

17

=

0,

18

=1/(tg1*r1),

19

=1/tt1,

0,

0,

0,

0,

=kp1/tp1;

1/tt1,

0,

0,

0,

0;

0,

0,

0,

0;

kp2/tp2,

0,

kp2/tp2;

1/tt2,

0;

0,

=1/tg1,

0,

0,

0,

=1/tp2,

20

0,

0,

0,

0,

21

0,

0,

22

2*pi*t12,

0,

=1/(tg2*r2),
0,
=2*pi*t12,
0,

=1/tt2,

0,

=1/tg2,

0,

0,

23
24
25
26

B1 = [0, 0, 1/tg1, 0, 0,
0, 0,

0, 0;

0, 0, 0, 1/tg2, 0]';

=

27

H1 = [ kp1/tp1, 0, 0,

28

0, 0, 0,

0, 0, 0, 0;

=kp2/tp2,

0, 0, 0]';

29
30
31

C = [1, 0, 0, 0, 0, 0, 0;
0, 0, 0, 0, 0, 0, 1];

32
33

%Wienerprocessens inkrementella kovarians

34

Ph = 10^ 5 * eye(size(H1,2));

=

35
36

%sampla systemet

37

A = expm(A1*dt);

38

B = integral(@(s)expm(A1*s),0,dt,'ArrayValued',true)*B1;

39

H = integral(@(s)expm(A1*s),0,dt,'ArrayValued',true)*H1;

85

0;
0];

BILAGA C.

PROGRAMKOD

Tuomas Virtanen

40
41

%processbrusets vÃ¤ntevÃ¤rde och kovarians

42

Pv = integral(@(s)expm(A1*s)*H1*Ph*H1'*(expm(A1*s)'),0,dt,'ArrayValued',true);

43

mv = zeros(size(Pv,1),1);

44

F = eye(size(Pv,1));

45
46

%mÃ¤tbrusets vÃ¤ntevÃ¤rde och kovarians

47

Pw = 10^ 5 * eye(size(C,1));

48

mw = zeros(size(C,1),1);

49

G = eye(size(Pw,1));

=

50
51

%starttillstÃ¥ndets vÃ¤ntevÃ¤rde och kovarians

52

m0 = zeros(size(A,1),1);

53

P0 = idare(A',C',F*Pv*(F'),Pw);

54

x0 = m0;

55
56

%kostnadsmatriser

57

Q = 5*(C')*C;

58

R = eye(size(B,2));

59

SN = zeros(size(Q));

60
61
62

%LÃ¶s Riccatiekvationen och berÃ¤kna Ã¥terkopplingsmatriserna fÃ¶r regleringen i
fÃ¶rvÃ¤g
[L0, L, S0, S] = aterkoppling(A,B,SN,Q,R,n);

63
64

%allokering av variabler

65

x = cell(1,n);

y = cell(1,n);

66

x_pred = cell(1,n);

x_korr = cell(1,n);

67

P_pred = cell(1,n);

P_korr = cell(1,n);

68

v = cell(1,n);

w = cell(1,n);

u = cell(1,n);

69
70

%slumpa process- och mÃ¤tbruset

71

v0 = mvnrnd(mv,Pv,1)';

72

w0 = mvnrnd(mw,Pw,1)';

73

for k = 1:n

74

v{k} = mvnrnd(mv,Pv,1)';

75

w{k} = mvnrnd(mw,Pw,1)';

76

end

77
78

%bestÃ¤m styrsignalen vid tid t = 0 och berÃ¤kna prediktionen

79

if dy == dt

80

%om mÃ¤tning gÃ¶rs vid tid 0 berÃ¤kna korrektionen

y0 = C*x0 + w0;

86

BILAGA C.

PROGRAMKOD

Tuomas Virtanen

81

[x_korr_0,P_korr_0] = kf_korrektion(m0,P0,y0,C,G,Pw);

82

u0 =

83
84

=

L0*x_korr_0;
_
[x pred{1},P_pred{1}] = kf_prediktion(x_korr_0,P_korr_0,A,B,u0,F,Pv);

else

85

x_pred_0 = m0;

86

u0 =

87

[x_pred{1},P_pred{1}] = kf_prediktion(x_pred_0,P0,A,B,u0,F,Pv);

88

=L0*x_pred_0;

end

89
90

for k = 1:n

91

%iterera tillstÃ¥ndet och berÃ¤kna prediktionen

92

if k == 1

93

% H âˆ— [0; 1] simulerar en 0.1% Ã¶kning pÃ¥ nÃ¤tverk 2

94

x{1} = A*x0 +B*u0 + H*[0; 1] + F*v0;

95
96

else

=

=

=

x{k} = A*x{k 1} + B*u{k 1} + F*v{k 1};

97

=

if isempty(x_korr{k 1})

98

=

=

=

=

[x_pred{k},P_pred{k}] = kf_prediktion(x_pred{k 1},P_pred{k 1},A,B,u{

99

=

k 1},F,Pv);
100

else

[x_pred{k},P_pred{k}] = kf_prediktion(x_korr{k 1},P_korr{k 1},A,B,u{

101

=

k 1},F,Pv);
102

end

103

end

104

%gÃ¶r mÃ¤tning och berÃ¤kna korrektion vid varje mÃ¤tintervall dy

105

if mod(k*dt,dy) == 0

106

y{k} = C*x{k} + G*w{k};
[x_korr{k},P_korr{k}] = kf_korrektion(x_pred{k},P_pred{k},y{k},C,G,Pw);

107
108

end

109

%berÃ¤kna styrsignalen

110

if mod(k*dt,du) == 0
if isempty(x_korr{k})

111
112
113

u{k} =
else

114
115
116
117
118
119

u{k} =

=L{k}*x_pred{k};
=L{k}*x_korr{k};

end
else
if k == 1
u{1} = u0;
else

87

BILAGA C.

Tuomas Virtanen

=

120

u{k} = u{k 1};

121

end

122
123

PROGRAMKOD

end
end

124
125

%plotta mÃ¤tningarna och styrsignalerna

126

for k = 1:n

127

if k == 1

128

if exist('y0','var')

129

s1 = subplot(2,2,1); plot(0,y0(1),'r.'); hold on;

130

s2 = subplot(2,2,2); plot(0,y0(2),'r.'); hold on;

131

else

132

s1 = subplot(2,2,1); hold on;

133

s2 = subplot(2,2,2); hold on;

134

end

135

s3 = subplot(2,2,3); plot([(k 1)*dt,k*dt],[u0(1),u0(1)],'b '); hold on

136

s4 = subplot(2,2,4);

137

=
=
plot([(k=1)*dt,k*dt],[u0(2),u0(2)],'b=');

hold on

else

138

if ~isempty(y{k})

=
plot((k=1)*dt,y{k}(2),'r.');

139

s1 = subplot(2,2,1); plot((k 1)*dt,y{k}(1),'r.');

140

s2 = subplot(2,2,2);

141

end

142

s3 = subplot(2,2,3); plot([(k 1)*dt,k*dt],[u{k 1}(1),u{k 1}(1)],'b ');

143

s4 = subplot(2,2,4);

144

end

145

if k == n

=
=
=
=
plot([(k=1)*dt,k*dt],[u{k=1}(2),u{k=1}(2)],'b=');

146

subplot(2,2,1); plot([0,t],[0,0],'k:');

hold off

147

subplot(2,2,2); plot([0,t],[0,0],'k:');

hold off

148

subplot(2,2,3); plot([0,t],[0,0],'k:');

hold off

149

subplot(2,2,4); plot([0,t],[0,0],'k:');

hold off

150

end

151

end

152

xlabel(s1, 'tid (s)'); ylabel(s1, '\Delta w_1');

153

xlabel(s2, 'tid (s)'); ylabel(s2, '\Delta P_{tie}');

154

xlabel(s3, 'tid (s)'); ylabel(s3, '\Delta P_{ref1}');

155

xlabel(s4, 'tid (s)'); ylabel(s4, '\Delta P_{ref2}');

156

h = zoom;

h.Motion = 'vertical';

h.Enable = 'on';

157
158

function [x,P] = kf_prediktion(x,P,A,B,u,F,Pv)

159

%Prediktionen av tillstÃ¥ndet

160

x = A * x + B * u;

88

BILAGA C.

PROGRAMKOD

161

%Prediktionens kovarians

162

P = A * P * A' + F * Pv * F';

163

Tuomas Virtanen

end

164
165

function [x,P] = kf_korrektion(x,P,y,C,G,Pw)

166

%KalmanfÃ¶rstÃ¤rkningen

167

K = P * C' * (C * P * C' + G * Pw * G')^( 1);

168

%Korrektionen av tillstÃ¥ndet

169

x = x + K * (y

170

%Korrektionens kovarians

171

P = P

172

=K

=C

=

* x);

* C * P;

end

173
174

function [L0, L, S0, S] = aterkoppling(A,B,SN,Q,R,n)

175
176

L = cell(n,1);

177

S = cell(n,1);

178

=

179

L{n} = (B'*SN*B + R)^( 1)*B'*SN*A;

180

S{n} = A'*SN*A + Q

181

= A'*SN*B*(B'*SN*B

=

182

for i = 1:n 1

=
=
=
S{n=i} = A'*S{n=i+1}*A + Q...
= A'*S{n=i+1}*B*(B'*S{n=i+1}*B

183

=

L{n i} = (B'*S{n i+1}*B + R)^( 1)*B'*S{n i+1}*A;

184
185
186

=

=

+ R)^( 1)*B'*S{n i+1}*A;

end

187

=

188

L0 = (B'*S{1}*B + R)^( 1)*B'*S{1}*A;

189

S0 = A'*S{1}*A + Q

190

=

+ R)^( 1)*B'*SN*A;

= A'*S{1}*B*(B'*S{1}*B

end

89

=

+ R)^( 1)*B'*S{1}*A;

LITTERATUR

Tuomas Virtanen

Litteratur
[AA11]

Ehab S. Ali och Sahar M. Abd-Elazim.  Bacteria foraging optimization algorithm based load frequency controller for interconnected
power system. I:

Systems

Internationa Journal of Electrical Power & Energy

33 (3 2011), s. 633638. doi:

10.1016/j.ijepes.2010.12.

022.
[AB89]

Karl G. Andersson och Lars-Christer BÃ¶iers.

lekvationer.
[DGG86]

OrdinÃ¤ra dierentia-

Lund: Studentlitteratur AB, 1989, s. vi+358.

Carlos E. De Souza, Michel R. Gevers och Graham C. Goodwin.  Riccati equations in optimal ltering of nonstabilizable systems having
singular state transition matrices. I:

tic Control

IEEE Transactions on Automa-

AC-31.9 (1986), s. 831838. doi:

10.1109/TAC.1986.

1104415.
[DV85]

M. H. A. Davis och R. B. Vinter.

Stochastic modelling and control.

Monographs on Statistics and Applied Probability. Chapman & Hall,
London, 1985, s. xii+393. doi:
[Gir03]

Narayan C. Giri.

10.1007/978-94-009-4828-0.

Multivariate statistical analysis.

2. utg. New York:

Marcel Dekker, 2003, s. xiv+558.
[HJ12]

Roger A. Horn och Charles R. Johnson.

Matrix Analysis. 2. utg. New

York, NY, USA: Cambridge University Press, 2012, s. xviii+643.
[HRS07]

Christiaan Heij, AndrÃ© Ran och Freek van Schagen.

Introduction to

mathematical systems theory: Linear systems, identication and control. BirkhÃ¤user Verlag, Basel, 2007, s. x+166. doi: 10.1007/978-3-

7643-7549-2.

90

LITTERATUR

[JP03]

Tuomas Virtanen

Jean Jacod och Philip Protter.

Probability essentials.

sitext. Springer-Verlag, Berlin, 2003, s. x+254. doi:

2. utg. Univer-

10.1007/978-

3-642-55682-1.
[Kal60]

Rudolph E. Kalman.  A New Approach to Linear Filtering and Prediction Problems. I:

Engineering
[Kay93]

Transactions of the ASMEJournal of Basic

82.1 (1960), s. 3545. doi:

10.1115/1.3662552.

Steven M. Kay.

Fundamentals of Statistical Signal Processing: Esti-

mation Theory.

Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,

1993, s. xii+595.
[Kun94]

Prabha S. Kundur.

Power System Stability and Control.

McGraw-

Hill, Inc., 1994.
[LR95]

Peter Lancaster och Leiba Rodman.

Algebraic Riccati Equations. Ox-

ford science publications. Clarendon Press, jan. 1995, s. xvii+480.
[Lue79]

David G. Luenberger.

Introduction to dynamic systems: Theory, Mo-

dels, and Applications.
[LXP08]

New York: Wiley, 1979, s. xvi+446.

Frank L. Lewis, Lihua Xie och Dan Popa.

Optimal and robust estima-

tion: with an introduction to stochastic control theory.

2. utg. Boca

Raton: Taylor & Francis, 2008, s. xxii+523.
[PMH13]

Sidhartha Panda, Banaja Mohanty och Prakash K. Hota.  Hybrid
BFOAPSO algorithm for automatic generation control of linear and
nonlinear interconnected power systems. I:
13 (12 2013), s. 47181730. doi:

[Ros+13]

Applied Soft Computing

10.1016/j.asoc.2013.07.021.

Samira Roshany-Yamchi m. .  Kalman Filter-Based Distributed Predictive Control of Large-Scale Multi-Rate Systems: Application to Power Networks. I:

IEEE Transactions on Control Systems Technology

21.1 (2013), s. 2739. doi:
[Ros10]

Sheldon M. Ross.

10.1109/TCST.2011.2172444.

A First Course in Probability. 8. utg. Pearson Pren-

tice Hall, 2010, s. xiv+530.
[Sha+16]

Nilaykumar N. Shah m. .  Automatic load frequency control of two
area system using L-Q-R method. I:

Engineering and Scientic Research

21276/Ijcesr.
91

International Journal of Current
3 (6 2016), s. 5466. doi:

10.

LITTERATUR

[SÃ¤r13]

Simo SÃ¤rkkÃ¤.

Tuomas Virtanen

Bayesian Filtering and Smoothing.

Vol. 3. Institute

of Mathematical Statistics Textbooks. Cambridge University Press,
2013, s. xxii+232. doi:
[TSH01]

10.1017/CBO9781139344203.

Harry L. Trentelman, Anton A. Stoorvogel och Malo Hautus.

theory for linear systems.

Control

Communications and Control Engineering

Series. Springer-Verlag London, Ltd., London, 2001, s. xvi+389. doi:

10.1007/978-1-4471-0339-4.
[Ã…st70]

Karl J. Ã…strÃ¶m.

Introduction to stochastic control theory.

Mathema-

tics in Science and Engineering, Vol. 70. Academic Press, New YorkLondon, 1970, s. xiv+299.
[Ã˜ks98]

Bernt Ã˜ksendal.

Stochastic Dierential Equations. An Introduction

with Applications.

5. utg. Universitext. Springer-Verlag, 1998. doi:

10.1007/978-3-662-03620-4.

92

