Mika Nokelainen

Aktiv investering med hjälp
av förstärkt inlärning
-

Ett test av algoritmisk handlande

Pro gradu-avhandling informationssystem
Handledare: Markku Heikkilä och Jozef Mezei
Fakulteten för samhällsvetenskaper och ekonomi
Åbo Akademi
Åbo 2020

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

INNEHÅLLSFÖRTECKNING
1. INLEDNING
1.1. PROBLEM
1.2. SYFTE OCH AVGRÄNSNINGAR
1.3. DISPOSITION
2. METOD
2.1 METOD I HÄNSYN TILL TEORIN
2.3 METODVAL
2.3 DATA
3. MASKININLÄRNINGSMETODER
3.1. ÖVERVAKAD INLÄRNING
3.2. OÖVERVAKAD INLÄRNING
3.3. FÖRSTÄRKT INLÄRNING
3.3.1 Grunderna till förstärkt inlärning
3.3.2 Markov-beslutsprocess
3.3.3 Mål och belöningar
3.3.4 Episoder
3.3.5 Utforskning och exploatering
3.3.6 Erfarenhetsuppspelning
3.4. Q-INLÄRNING
3.4.2 Q-inlärning exempel
3.5. DJUPINLÄRNING
3.5.1 Neurala nätverk
3.5.2 Faltningsnätverk
3.5.3 Recurrent neural network
3.5.4 Djup förstärkt inlärning
4. VÄRDERINGEN AV FINANSIELLA MEDEL MED HJÄLP AV
MASKININLÄRNINGSMETODER

1
2
2
3
4
4
5
6
8
10
12
13
14
17
19
20
21
23
24
26
30
32
34
37
37
40

4.1. GRUNDER I AKTIEVÄRDERING
4.1.1 Teknisk analys
4.1.2 Fundamental analys
4.1.3 Hypotesen om effektiva marknader
4.1.4. Random walk-hypotesen
4.2. HANDLING MED FINANSIELLA INSTRUMENT MED HJÄLP AV FÖRSTÄRKT INLÄRNING

40
40
43
45
46
47

5. PORTFOLIO OPTIMERING OCH AKTIEHANDEL MED HJÄLP AV FÖRSTÄRKT
INLÄRNING

53

5.1 AKTIE HANDLINGS BESLUT MED DJUP Q-INLÄRNING
5.3 TRÄNING OCH TEST
5.4 PORTFÖLJSIMULATION
6. SAMMANFATTNING OCH DISKUSSION
6.1 FORTSATTA STUDIER

54
56
62
66
68

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
KÄLLFÖRTECKNING

71

BILAGOR:

75

Förkortningar
MLP: Multilayer perceptron
CNN: Convolutional neural network
LSTM: Long short-term memory
DQN: Deep Q-network
AI: Artificial Intelligence
RNN: Recurrent neural network
RSI: Relative strength index
PEG: Price/earnings to growth ratio
AVGO: Aktiesymbol för Broadcom Inc
S&P 500: Standard and Poors 500 index
MDP: Markov decision process

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Abstrakt
Ämne: Informationssystem
Författare: Mika Nokelainen
Titel: Aktiv investering med hjälp av förstärkt inlärning
Handledare: Markku Heikkilä och Jozef Mezei
Abstrakt:
Att investera passivt har visat sig vara ett effektivt sätt att investera i aktier. Passiv
investering innebär att följa en buy-and-hold-investeringsstrategi där man minimerar
aktiv handel av aktier. Motsatsen till detta vore att investera aktivt, där man tillvaratar
kortsiktiga prisförändringar för att få en högre avkastning jämfört med marknaden.
Hypotesen om effektiva marknader anser dock att det inte är möjligt att få högre
avkastning på ens investeringar utan att öka risken med investeringar.
Kan maskininlärning, närmare bestämt förstärkt inlärning, vinna över en passiv
investerare genom att aktivt handla aktier? Syftet med avhandlingen är svara på denna
fråga. En djupt förstärkt inlärd algoritm programmerades i Python för att handla aktier.
Aktierna som var i fokus i denna avhandling hör till S&P 500-indexet. Avhandlingen
fokuserade sig på två olika variabler i de modeller som testas:
● Mängden historiska data som algoritmen får för att fatta handelsbesluten
● Mängden noder i neurala nätverket i den djupt förstärkta inlärda algoritmen.
Modellerna testades på dedikerade testdata enskilt och som helhet i en portfölj.
Resultaten av testandet var relativt stokastiska, men avhandlingen lyckades visa att en
förstärkt inlärd algoritm kan under specifika omständigheter lära sig generella drag av
historiska aktiedata som hjälper den att aktivt handla aktier.
Nyckelord: Maskininlärning, förstärkt inlärning, aktiehandel, aktiv investering
Datum: 11.4.2020
Sidor: 90

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

1. INLEDNING
I kärnan av vår ekonomi finns den finansiella marknaden, och aktiemarknaden är en del av
detta. Aktiemarknadens grunduppgift är att hjälpa företag samla kapital till deras
verksamhet. På den allmänna aktiemarknaden kan vem som helst köpa och sälja aktier
vilket gör att dem till extremt likvid form av varor vilket i sig leder till att flera av de
noterade företagens aktier handlas dagligen med hög volym.
New York Stock Exchange är världens största aktiemarknad, och den har ett sammanlagt
marknadskapital på cirka 28 biljoner dollar (NYSE, 6/2018). En hel del aktier köps och
säljs på aktiemarknader runtom i världen varje dag de är öppna. Att investera i
aktiemarknaden anses överlag var ett bra sätt att placera sina besparingar, det kräver dock
att investeraren är medveten om riskerna som följer med att investera i aktier. Aktiernas
värde är direkt kopplat till hur mycket någon är redo att köpa aktien för. Vid lågkonjunktur
brukar aktiernas värde sjunka, men de har hittills alltid återhämtat sig. Undantag från detta
är företag som inte klarar sig genom lågkonjunkturen, det är alltså möjligt att mista sitt
investerade kapital helt och hållet.
Aktier anses vara relativt volatila jämfört med andra värdepapper. Till exempel obligationer
brukar vara ett alternativ för dem som är riskaverta. Obligationer har mycket lägre
volatilitet än aktiemarknaden. Historiskt sett har dock aktiemarknaden haft en betydligt
högre avkastning på investerat kapital än obligationer; risk och avkastning går ofta hand i
hand.
Datorer har använts redan länge för att handla aktier. NASDAQ, som öppnade 1971, är i
dagens läge världens näst största aktiemarknad. NASDAQ var också världens första
elektroniska aktiemarknad. Eftersom aktier i dagens läge kan handlas på datorer har det
möjliggjort användningen av algoritmisk handel av aktier. Detta har lett till att
maskininlärning har börjats använda i handlingen av aktier. (Tripp & DeSieno 1992)
Maskininlärning har gjort flera framsteg under det senaste decenniet. En förstärkt inlärd
algoritm har redan vunnit de bästa spelarna i Go (Alphago) och Starcraft 2 (Alphastar).
Krizhevsky et al. (2012) använde faltningsnätverk för känna igen 1000 olika objekt i
miljontals olika bilder. Mnih et al. (2015) fortsatte på detta arbete genom att koppla en
1

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
förstärkt inlärd algoritm med ett faltningsnätverk för att lära en maskin att spela Atari 2600spel med revolutionerande resultat.
Vad händer ifall man skulle kombinera aktiemarknaden med den senaste forskningen inom
maskininlärning? Målet med denna studie är att undersöka om det går att använda förstärkt
inlärning för att förutspå en akties framtida prisutveckling med hjälp av historisk aktiedata.

1.1. Problem
Enligt hypotesen om effektiva marknader och random walk-hypotesen är det inte möjligt
att förutspå hur aktiepriserna kommer att röra sig, och det är inte möjligt att få större
avkastning jämfört med marknaden utan att öka risken på investeringarna. Det antas att all
information om marknaden och de företag som den hör till är tillgängligt för alla.
Aktiekurserna anpassar sig genast när ny information kommer ut, t.ex. nyheter om företaget
eller en kvartalsrapport. Ifall det är möjligt att förutspå en akties prisutveckling, betyder det
att hypotesen om effektiva marknader samt random walk-hypotesen är i alla fall till en del
ogiltiga.
Algoritmen fattar handelsbesluten baserat på n antal historiska aktiedata, vid en viss
tidpunkt. Under träningsfasen har algoritmen sett tusentals olika scenarion med samma
antal n historiska aktiedata. Utifrån vad den lärt sig under träningen försöker algoritmen
hitta generella drag ur dessa scenarion som hjälper algoritmen fatta rätt handelsbeslut vid
scenario den inte sett förr. Eftersom algoritmen ska endast fatta beslut på basen en
historiska aktiedata, liknar detta mycket teknisk analys. Om algoritmen lyckas hitta
generella drag från historiska aktiedata som hjälper den fatta bra handelsbeslut, betyder det
att teknisk analys fungerar i alla fall dels.

1.2. Syfte och avgränsningar
Syftet med denna studie är att undersöka om det går att förutspå aktiemarknaden med hjälp
av olika förstärkta inlärda algoritmer. Den första forskningsfrågan är: Kan en förstärkt
inlärd algoritm lära sig att handla aktier på ett sätt som ökar avkastningen av aktien
jämfört med aktiens egna prisutveckling på aktiemarknaden? Med andra ord: kan man få
2

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
högre avkastning jämfört med att passivt investera i en aktie genom att aktivt handla aktien?
Med utgångspunkt i aktiens tidigare prisutveckling blir frågan då: kommer aktien att gå
upp eller ner? Detta är ett klassifikationsproblem. Utifrån av den givna data ska algoritmen
fatta ett handels beslut. Denna studie kommer att innehålla en simulation där dessa
algoritmer används för att aktivt handla aktier. Aktierna som avhandlingen fokuserar på
finns i S&P 100 och S&P 500 som innehåller de 100 respektive 500 största företagen på
den amerikanska aktiemarknaden enligt deras marknadsvärde.
Avhandlingen kommer också att försöka simulera en portfolio vars aktier handlas baserat
på beslut som görs av algoritmen. Den andra forskningsfrågan är: Kan en förstärkt inlärd
algoritm öka avkastningen av en portfolio jämfört med en passiv investering? Den första
forskningsfrågan har enskilda aktier i fokus medan den andra forskningsfrågan fokuserar
på en portfölj med flera aktier.
Avhandlingen kommer att fokusera sig på algoritmer som använder djupa q-nätverk (DQN)
som är Q-inlärning kombinerat med neurala nätverk. Simulationen kommer att
programmeras i Python.

1.3. Disposition
Kapitel 2 kommer att behandla de forskningsmetoder som används och vilka data som
kommer används. Kapitel 3 behandlar olika maskininlärningsmetoder. Övervakad och
oövervakad inlärning behandlas kort medan mer vikt läggs på förstärkt inlärning och
djupinlärning. Med tanke på om att avhandlingen handlar om aktiehandel ger kapitel 4 en
överblick över hur man värdesätter aktier samt lite teori om aktiehandel. Kapitel 4.2
redovisar tidigare forskning om över förstärkt inlärda algoritmer som handlar olika
finansiella instrument. I kapitel 5 behandlas modellerna och simulationen som gjorts för
denna avhandling i detalj. I kapitel 6 sammanfattas avhandlingen och fortsatt forskning
inom ämnet diskuteras med personliga åsikter sammanfattar avhandling och personliga
åsikter om fortsatt forskning inom ämnet.

3

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

2. METOD
En djupt förstärkt inlärd algoritm som förutspår en akties prisutveckling programmeras i
denna avhandlingen med hjälp av Python. Data som används är historiska aktiedata av S&P
100 och S&P 500-aktier. Algoritmens förmåga att förutspå en akties prisutveckling testas
med hjälp av dedikerade testdata.

2.1 Metod i hänsyn till teorin
Järvinen (2012) lyfter upp Jenkins (1985) modell om forskningsmetoder. Denna modell har
använts som riktlinje för avhandlingen, och för att svara på avhandlingens
forskningsfrågor.

Modellen

har

också

använts

för

att

iterera

och

bearbeta

maskininlärningsmodellerna i kapitel 5.
1. Idea
2. Library research
3. Research topic
4. Research strategy
5. Experimental design
6. Data capture
7. Data analysis
8. Publish results
Jenkins modell ur Järvinen (2012)
Denna avhandling grundar sig på idén att maskininlärning kan användas för att förbättra
avkastningen av aktieinvesteringar. Flera avhandlingar har använt sig av övervakad och
oövervakad inlärning för att förutspå aktier. Till dessa hör till exempel Powell et al. (2008),
Egeli (2003) och Shumaker & Chen (2009). Det finns också en del forskning som använder
sig av förstärkt inlärning för att förutspå den framtida prisutvecklingen av olika finansiella
medel, som till exempel valutapar och olika index, men få forskningar har aktier i fokus.
Denna avhandling kommer att fokusera sig på att öka avkastningen av ens investeringar
med hjälp av förstärk inlärning jämfört med en passiv investerare. Att investera passivt har
visat sig att vara ett effektivt sätt att investera i aktier. Passiv investering innebär att följa
4

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
en buy-and-hold-investeringsstrategi där man minimerar aktiv handel av aktier. Mer om
passivt investerande kan läsas från Sushko & Turner (2018).
Järvinen (2012) hävdar att när man empiriskt studerar framtiden och de förflutna kan vi
använda theory testing-modeller. Denna avhandling faller inom området av theory testingmodeller på Järvinens taxonomi om forskning metoder. För att svara på avhandlingens
forskningsfrågor används kontrollerade experiment. Kontrollerade experiment är en metod
som Järvinen (2012) nämner att man kan använda för att till exempel jämföra två
sorteringsalgoritmer. I detta fall jämförs dock flera maskininlärningsmodeller med
varandra.
Järvinen (2012) nämner att kontrollerade experiment använder sig av beroende och
oberoende

variabler.

Oberoende

variabler

kommer

i

detta

falla

vara

programmeringsspråket, kör miljön, indata och flera andra konstanter för neurala nätverket
som finns uppräknade i bilaga 3. De variabler som gör att modellerna i slutskede skiljer
sig från varan är neurala nätverkets typ / storlek och mängden indata.
I slutet analyseras hur effektiva maskininlärning modellerna är jämfört med en passiv
investerare.
Test metodologin som används baserar sig på Shmueli & Koppious (2011) forskning där
de beskriver empiriska modeller som kan användas för prognoser. Denna modell är menad
för algoritmer som är designade för att förutspå framtida händelser, i denna avhandling
skulle detta vara en akties framtida prisutveckling. Shmueli & Koppious (2011) anser att
det går att testa hur pålitlig modellen är på framtida observationer eller på andra
observationer som inte var i testdata. Denna avhandling kommer att testa modellera på båda
alternativen.

2.3 Metodval
Valet av förstärkt inlärning som metod för avhandlingen baserar sig på de senaste
framgångarna inom ämnet. Bland annat Mnih et al. (2015) med djupa Q-nätverk och
5

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
framgångarna av Alphastar och AlphaGo som är gjorda av Deepmind (2017 & 2019) har
lyft förstärkt inlärning fram med sina respektive förbättringar.
Största fördelen med förstärkt inlärning jämfört med övervakad inlärning är att de kräver
inte märkta data utan algoritmen lär sig genom att röra sig fram i en miljö. Till skillnad från
övervakad inlärning lämpar sig förstärkt inlärning speciellt då när man vill att algoritmen
ska lära sig en kedja av händelser.
Python valdes som verktyg för att skapa denna algoritm på grund av färdiga kodpaket som
är ägnade för maskininlärning och datahantering som t.ex. Keras, Tensorflow, Pandas och
Numpy. Dessa paket möjliggör en effektiv utveckling av algoritmen. R skulle också ha
varit ett bra alternativ men jag valde Python på grund av att jag har mera erfarenhet av det.
För att få en optimal algoritm testas flera olika variabler som har med inlärningen att göra.
Vissa av variablerna har ett konstant värde, dessa värden är tagna ur tidigare forskning,
t.ex. diskonteringen och optimiseraren är från Huang (2019). Flera av de tidigare
förbättringarna som tidigare forskare inom ämnet har hittar har använts i denna avhandling.
Till exempel Huang (2019) har inspirerat det sätt som algoritm delar ut belöningen samt
storleken av uppspelningsminnet. Deng et al. (2016) bevisade att råa data kan användas för
att träna algoritmen, inga tekniska indikatorer behövs. Dessa ovannämnda forskningar och
till exempel Azhikodan et al. (2019) har inspirerat mig att pröva på hur LSTM-nätverk
fungerar i den simulation jag byggt. Neurala nätverkets storlek i fråga vid mängden lager
och noder per lager är en variabel som kommer att påverka resultatet. Deng et al. (2016)
har forskat i detta och deras resultat har inspirerat valet av mängden lager och noder.

2.3 Data
Historiska aktiedata av S&P 100 aktier har använts för att träna algoritmen. En lista på de
aktier som hör till S&P 100 indexet när avhandlingen finns som bilaga. Algoritmen har
blivit tränad på historiska data från januari 2017 till maj 2019. Dess framgång testas på
S&P 500 aktier som inte tillhör till S&P 100. Testdata innehåller observationer från januari
2017 till mars 2020. Detta ger en inblick i hur algoritmen fungerar på data som den inte

6

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
sett förut. Orsaken till att jag valde S&P 100 och S&P 500 aktier är att dessa data är lätt
tillgänglig. Dessa data är hämtad från alphavantage.co.
Aktiedata är på daglig nivå. Detta ger oss fem olika variabler som kommer att fungera som
indata till algoritmen:
-

Open: Priset på aktien när marknaden öppnade

-

High: Högsta priset som aktien handlades med på en viss dag

-

Low: Lägsta priset som aktien handlades med på en viss dag

-

Close: Priset på aktien när marknaden stängde

-

Volume: Mängden aktier som handlades på den specifika dagen.

Hypotesen är att med hjälp av dessa data skulle algoritmen kunna lära sig förutspå den
framtida prisutvecklingen av aktien. Ett antagande är att dessa data räcker för att göra en
simpel tekniska analys på aktien, kapitel 4.1.1 beskriver detta noggrannare.

7

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

3. MASKININLÄRNINGSMETODER
Varför ska maskinerna lära sig något, räcker det inte med att de löser ett specifikt problem
som de är programmerade att lösa? Ofta räcker det, men i flera fall är det inte rimligt. I
stället för att formulera lösningar till ett problem bildas algoritmer vars uppgift är att lära
sig att lösa problemet.
Hur kan maskiner lära sig automatiskt genom ens erfarenheter? Detta är frågan som Jordan
och Mitchell (2015) anser att forskning inom maskininlärning försöker svara på. Detta är i
dagens läge ett av de fält inom datavetenskap som växer snabbast. Fastän själva teorin är
rätt så gammal har maskininlärning blivit aktuellt igen på 2000-talet. Orsaken är att nya
inlärningsalgoritmen har tillgång till stora mängder data tack vare internet och förmånlig
hårdvara får att göra de krävda kalkylationerna.
Under de senaste decennierna har maskininlärning gjort stora framsteg enligt Jordan och
Mitchell (2015). Det började med små experiment i laboratorier som idag har evolverat och
hittat flera kommersiella problem som har kunnat lösas med hjälp av maskininlärning.
Inom artificiell intelligens (AI) har maskininlärning framstått som den metod som valts för
att utveckla praktisk programvara för datorsyn, taligenkänning, naturlig språkbehandling
och robotstyrning. Många utvecklare av AI-system inser nu att för flera problem kan det
vara mycket enklare att träna ett system genom att visa algoritmen ett exempel på önskat
input𑁋output-beteende, än att programmera det manuellt genom att förutse önskat svar
för alla möjliga output.
Ett inlärningsproblem kan enligt Jordan och Mitchel (2015) vara till exempel att känna igen
kreditkortsbedrägeri. Algoritmens uppgift vore då att få en korttransaktion som input, som
sedan ska klassificeras som antingen bedrägeri eller icke-bedrägeri. För att träna upp
algoritmen behövs det mycket data om tidigare transaktioner som är klassificerade som
bedrägeri eller icke-bedrägeri. Detta kallas för märkta data. Maskininlärnings algoritmen
övar sig på en del av dessa data som kallas för träningsdata. Efter övningen testas hur bra
algoritmen fungerar på data som den inte sett förr. Detta kallas kallas testdata.

I

kreditkortsbedrägeriexemplet kan algoritmen läras att undvika falska negativa resultat,
exempelvis transaktioner där bedrägeri klassificeras fel som icke-bedrägeri.
8

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Med hjälp av historiska data kan man redan i dagens läge använda sig av maskininlärning
i flera olika branscher. Jordan och Mitchel (2015) nämner att man kan till exempel förutspå
vilka mediciner som passar bäst för vilka patienter, trafikdata kan användas för att förutspå
rusningar och historiska data över brott kan användas för att skicka poliser till rätta områden
vid ett visst klockslag.
Gori (2018) presenterar ett exempel i sin bok Machine learning, ett problem som kan
användas för att exemplifiera behovet av maskininlärning. I detta exempel försöker en
maskin läras att känna igen bokstäver som är skrivna för hand.

Figur 1. Gori(2018)
Denna bild ska representera en inskannad tvåa i en 8x8-matris. Hur ska vi lära maskinen
att denna matris ska klassificeras som en tvåa? Förs konverteras matrisen till en sträng med
64 bittar:
0001100000100100000000100000001000000010100001000111110000000011
Strängen läggs till en tabell och den klassificeras som en tvåa. Problemet skulle då bara blir
att leta upp samma sifferkombination i en tabell för att hitta vad bokstaven ska representera.
Gori (2018) påpekar att detta är dock tyvärr inte rimligt eftersom vi borde ha en rad tabellen
för alla möjliga kombinationer som en 8x8-tabell kan ha. För att räkna ut hur många
möjligheter som kan finnas kan utgå från hur många möjliga värden en cell kan ha i fakultet
med hur många celler som finns. Det blir 2^64, 18446744073709551616 olika möjliga
kombinationer. De vore orimligt att behöva spara en tabell med alla dessa värden bara fört
att kunna klassificera handskrivna bokstäver. Ifall resolutionen på bilden är ännu större,
t.ex. 10x10, blir siffran för möjliga kombinationer redan 2^100.

9

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Ett annat exempel som Gori (2018) tar upp är när ett program ska kunna urskilja ord från
varandra i ett ljudklipp. Detta kan till exempel basera sig på en tröskel där en tillräcklig
sänkning av tonhöjden skulle betyda slutet av ett ord och början av ett annat. Exempelvis
yttrandet“computers are attacking the secret of intelligence” kunde enligt Gori uppfattas
av ett program som: com / pu / tersarea / tta / ckingthesecre / tofin / telligence.
Enligt Gori (2018) är ljudsignalen nästan noll före ljuden p, t och k. Ett annat problem är
de sammansatta orden, ljudsignalen försvagas ofta i mitten av orden vilket skulle leda till
att tröskel underskrids och nästa ljud uppfattas som ett nytt ord. En maskininlärd algoritm
kan enligt Gori (2018) med tillräcklig mycket träningsdata lära sig att inte särskilja på ord
vid p, t och k.
Detta kapitel kommer att redogöra över olika sätt vi kan lära maskiner att lösa olika
problem. Generellt brukas maskininlärningsalgoritmer delas i tre olika kategorier enligt
Gori(2018) är dessa:
1. Övervakat lärande. Används då det finns klart definierad indata och utdata.
Programmet ska kunna lära sig generaliserade regler utifrån den givna data för att
lösa det aktuella problemet.
2. Oövervakat lärande. Här finns det bara indata och programmet ska med hjälp av
dessa lyckas lära sig de underliggande strukturerna.
3. Förstärkt inlärning. En agent lär sig genom interaktioner. Genom upprepade försök
inom en miljö lära agenten att agera på ett visst sätt i en viss situation.
Dessa olika sätt kan lösa olika problem som till exempel klassifikation, regression och
klustring. I de kommande kapitlen kommer dessa förklaras noggrannare. Mera vikt
kommer att läggas på förstärkt inlärning som behandlas i kapitel 3.3.

3.1. Övervakad inlärning
De vanligaste maskininlärningsmetoderna är enligt Mitchell och Jordan (2015) övervakade
inlärningsmetoder.

Dessa kan innehålla till exempel klassifikations problem där

algoritmen ska fungera som ett spamfilter i ens mailinkorg, känna igen ansikten i bilder
eller att hjälpa diagnosticera patienter. Grundprincipen med övervakad inlärning är att
10

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
träningsdata innehåller (x, y) där x är data som prediktionen ska göras på och y är det
förväntade svaret. Målet är att producera en prediktion y* med hjälp av x*.
Input x kan till exempel vara en vektor som innehåller flera variabler eller mer komplexa
dokument som bilder eller grafer. Likvis kan algoritmens output också ta flera former. Den
vanligaste formen är en binär klassifikation där svaret blir antingen 0 eller 1, dessa siffror
representerar två olika svar, som till exempel spam och icke spam. Ofta räcker dock inte
binär klassifikation, därför har mycket forskning gjorts på klassifikations problem med
flera möjliga kategorier som svar, samt problem där ett svar kan vara en kombination av
flera kategorier.
Övervakade inlärningssystem formar överlag ens prediktioner med hjälp av inlärd
kartläggning av f(x). Det finns flera olika sätt att kartlägga funktionen f , Jordan och
Mitchell (2015) nämner några exempel, dessa är beslutsträd, logistisk regression,
stödvektor maskiner och neurala nätverk.
Russel och Norvig (2003) tar upp ett exempel där en övervakad algoritm ska lära sig om
den ska bromsa eller inte med hjälp av bilder som är tagna från en bil. Detta är alltså ett
binärt klassifikations problem. För varje bild som ges till algoritmen ska den lära sig att
känna igen om det finns ett person i närheten som är i risk att bli överkörd av bilen om den
inte bromsar. Haiste et al. (2011) beskriver att en övervakad inlärnings agent har en “lärare”
som berättar algoritmen om den prediktion den gjort är rätt eller fel. Russell och Norvigs
(2003) exempel med bilen kan också blir ett regressions problem ifall vi vill ha som output
hur långt borta personen är från bilen.
LeCun et al. (2015) hävdar att den vanligaste formen av maskininlärning, djupt eller inte,
är övervakad inlärning. LeCun et al. (2015) tar upp ett exempel där ett system byggs vars
uppgift är att klassificera bilder som kan innehålla ett hus, en bil, en person eller ett husdjur.
Först samlas en stor mängd av bilder av hus, bilar, människor och husdjur. Varje bild måste
vara märkt med ens respektive kategori. Under träningen visas maskinen en bild, maskinen
producerar en output i form av en vektor av poäng, en för varje kategori. Målet är att den
önskade kategorin får högsta poängen. Kategorin med högsta poäng anses vara maskinens
prediktion.

11

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
En del av övervakad inlärning som har sett stor framgång under de senaste åren är neurala
nätverk. Neurala nätverk använder sig av flera olika lager av noder och vikter som
innehåller tröskelvärden. Jordan och Mitchell (2015) hävdar att varenda nod använder sig
av simpla funktioner för ens input och output. Djupinlärning, som neurala nätverk hör till,
använder sig av en gradient nedstigning (eng. gradient decent) för att ändra värden på
vikterna mellan noderna. Dessa vikter bestämmer hur stark länken är mellan noderna.
Jordan och Mitchell (2015) påpekar att djupinlärning kan utnyttja moderna parallella
databehandlingsarkitekturer, såsom grafikbehandlingsenheter (GPU) som ursprungligen
utvecklats för videospel. Med hjälp av dessa är det möjligt att bygga djupa inlärningssystem
som innehåller miljarder variabler som kan tränas på stora samlingarna av bilder, videor,
och talprov som är tillgängliga på internet. Djupinlärningssystem har haft en stor inverkan
på forskningen under de senaste åren inom datorsyn (eng. computer vision) och
taligenkänning (eng. speech recognition). Djupinlärningssystem har producerat stora
mätbara framsteg jämfört med tidigare inlärning algoritmer som används för att lösa samma
problem. Djupinlärning behandlas noggrannare i kapitel 3.5.

3.2. Oövervakad inlärning
Haiste et al. (2011) beskriver oövervakad inlärning som “inlärning utan en lärare”. I
övervakad inlärning finns det en “lärare” som konstant berättar algoritmen om den gjort ett
rätt eller fel beslut. I oövervakad inlärning finns inte denna lärare utan algoritmen ska själv
lära sig att finna ett samband i de givna data. Haiste et al. (2011) skriver att det är inte
möjligt att direkt mäta framgången av oövervakad inlärning, de anser att man måste
använda sig av heurisktiska metoder för att bedöma kvaliteten av resultaten. En oövervakad
inlärningsagent kommer aldrig att lära sig vad den ska göra för att den vet inte vad som är
en önskad eller oönskad output enligt Russel och Norvig (2003). Den följer bara de givna
reglerna.
Jordan och Mitchell (2015) skriver att oövervakad inlärning involverar i stort sett en analys
av omärkt eller okategoriserade data med hjälp av antaganden gällande de strukturella
egenskaper för data (t.ex. algebraisk, kombinatorisk eller probabilistisk). Till exempel kan
12

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
man anta att data ligger på ett lågdimensionellt föreningsrör (eng. manifold), oövervakade
inlärningens uppgift är således att identifiera detta föreningsrör uttryckligen ur de givna
data. Det finns också flera metoder för reduktion av dimensioner som till exempel
principalkomponentanalys,

mångfaldsinlärning,

faktoranalys

och

slumpmässiga

projektioner gör som olika specifika antaganden om det underliggande föreningsröret.
Jordan och Mitchell (2015) fortsätter med att ta upp klusteranalys som ett annat exempel
av oövervakad inlärning. Klusteranalys används för att dela upp data i olika kluster i
frånvaro av explicita etiketter som indikerar en uppdelning. Ett brett spektrum av
klusteranalys algoritmer har utvecklats, dessa är baserade på specifika antaganden om arten
av ett kluster.

3.3. Förstärkt inlärning
Jordan och Mitchell (2015) och Du et al. (2016) hävdar att till skillnad från övervakad
inlärning, där en algoritm lär sig om ens output är rätt eller fel, spjälkas problemet upp i
förstärkt inlärning i flera handlingar som leder till en output. I förstärkt inlärning lär sig
algoritmen de bästa handlingarna vid ett givet tillstånd med hjälp av belöningar. Till
skillnad från övervakad inlärning vet algoritmen inte genast om den fattat ett bra eller dåligt
beslut. I slutet av en episod, som är en samling av tillstånd och handlingar, kan den dock
värdera om den valda kedjan av handlingar gav en hög belöning eller inte.
Om en maskin ska läras att spela schack kan man enligt Russell och Norvig (2003) använda
sig av övervakad inlärning. Maskinen kan bli given en lista på alla olika stadier som spelet
kan befinna sig i och sedan ge den bästa lösningen till detta stadie. Att kartlägga alla möjliga
tillstånd som spelknapparna kan befinna sig i och bestämma en vettig handling är dock inte
en enkel uppgift. Finns det en bättre lösning om dessa data inte är tillgängligt? För att lösa
problemet kan en algoritm användas som spelar spelet och försöker att vinna genom
upprepade försök och misstag. Detta kallas med andra ord för förstärkt inlärning.

Callan (2003) berättar att tillstånd är den indata som agenten får. Tillståndet beskriver till
agenten hur miljön där den befinner sig ser ur. På basen av dessa tillstånd ska agenten fatta
beslut om vilken handling som är den bästa. Denna koppling mellan tillstånd ger algoritmen
13

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
möjligheten att lära sig vilken kedja av handlingar leder till den högsta möjliga belöning.
Förstärkt inlärning handlar alltså om att lära sig genom interaktioner. Mer specifikt är det
en agent i en miljö enligt Callan (2003). Agentens uppgift är att göra ett eller flera
handlingar för varje tillstånd i miljön. Efter att agenten gjort en handling ger miljön agenten
en belöning. Om handlingen som agenten gjorde är förväntad, får den en stor belöning för
att främja den att välja samma handling i framtiden. Om handlingen var icke förväntad är
belöningen negativ. Målet blir således att maximera belöningen.
Russell och Norvig (2003) skriver att när agenten märker att en viss kedja av handlingar
leder till en hög belöning förstärker den denna kedja i hopp om att den i framtiden också
får en hög belöning från denna kedja av handlingar. I början är dess handlingar
slumpmässiga. Agenten börjar således med att röra sig i miljön utan att veta utkomsten av
olika handlingar. Detta leder till att i början blir det agentens uppgift att utforska miljön för
att få en bild av handlingsalternativen och miljön. Varje handling i varje tillstånd lagras
och det förväntade beteendet övas in. Sakta men säkert kommer agenten att lära sig att
undvika samma misstag för att komma fram till en optimal lösning.
Sutton och Barto (2018) berättar att utöver agenten och miljön kan man identifiera fyra
grundprinciper i förstärkningssystemet: en policy, en belöning, en värdefunktion och
möjligen en modell av miljön. Dessa grundprinciper behandlas noggrannare i följande
kapitel.

3.3.1 Grunderna till förstärkt inlärning
En policy definierar enligt Sutton och Barto (2018) inlärningsagentens sätt att bete sig vid
en viss tidpunkt. Överlag är en policy en kartläggning från upplevda miljötillstånd till
åtgärder som ska vidtas när agenten befinner sig i dessa tillstånd. I vissa fall kan policyn
vara en enkel funktion eller uppslagstabell, medan i andra fall kan det innebära omfattande
beräkningar. Policyn är i kärnan av en förstärkt inlärd agent i den meningen att den kan
ensam bestämma beteendet av agenten. I allmänhet är policyn stokastiska i början men
efter att agenten är tränad kan den med större sannolikhet säga vilken handling leder till
den högsta belöningen.

14

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
En belöning definierar målet för ett förstärkt inlärningsproblem. Sutton och Barto (2018)
fortsätter med att berätta att vid varje steg skickar miljön agenten en siffra som kallas en
belöning. Agentens enda mål är att maximera den totala belöningen den får från en episod
av miljön. Belöningen definierar således vad som är de goda och dåliga handlingarna för
agenten. I ett biologiskt system kan vi tänka på belöningar som upplevelserna av nöje eller
smärta. De är de omedelbara och med hjälp av dem avgör agenten ifall handlingen ska
upprepas. Belöningen är den primära grunden för att ändra policyn. Ifall en handling valt
av policyn följs av låg belöning, kan policyn ändras till att välja någon annan handling i det
samma tillståndet i framtiden vilket hoppeligen ger en högre belöning. Detta upprepas tills
den bästa lösningen i tillståndet i frågan hittas.

Sutton och Barto (2018) anser att medan belöningen indikerar vad som är bra och dåliga
handlingar omedelbart, är det värdefunktionen som specificerar vilken kedja av handlingar
som är bra på en lång sikt. Belöningar bestämmer det omedelbara värdet av tillståndet,
värdefunktionen tar i beaktande den slutliga belöningen av kedjan av handlingar. Agenten
väljer alltså inte handlingar endast utifrån belöningen av den nästa handlingen utan på basis
av den totala belöningen av hela kedjan av handlingar. Till exempel kan ett tillstånd alltid
ge en låg belöning men fortfarande ha ett högt värde eftersom det följs regelbundet av andra
tillstånd med höga belöningar. För att göra en mänsklig analogi är belöningar något som
nöje (om det är högt) och smärta (om lågt), medan värdefunktionen ger ett värde som
motsvarar en mer förfinad och framsynt bedömning av hur nöjda eller missnöjda vi är i
framtiden på grund av handlingen.
Uppskattningen av värdet av ett tillstånd i förstärknings inlärnings algoritmen, är utan
tvekan enligt Sutton och Barto (2018) det viktigaste som har lärts om förstärkt inlärning
under senaste decennierna. Utan belöningar kunde det inte finnas något värde för ett
tillstånd. Det enda syftet med att uppskatta värden är att uppnå en större belöning.
Handlingsval görs baserat på värderingsbedömningar. Vi söker handlingar som följs av
tillstånd med högsta värde, inte högsta belöning, eftersom dessa åtgärder får den största
belöningen för oss på lång sikt. Belöningar ges direkt av miljön, men värden måste
uppskattas med hjälp av att pröva sig fram. Faktum är att den viktigaste komponenten i
nästan alla förstärknings inlärnings algoritmer är att uppskatta värden av den framtida
tillstånden.
15

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Förstärkt inlärning använder tillstånd och belöningar som indata till policyn och
värdefunktionen. Tillståndet är en signal som beskriver hur miljön ser ut till agenten. Mer
grundligt bestäms detta av Markovs beslutsprocess (eng. Markov decision process) som
kommer behandlas senare i avhandlingen.
För att få en bättre bild över hur förstärkt inlärning fungerar använder Sutton och Barto
(2018) sig av luffarschack som ett exempel.

Figur 2. Sutton och Barto (2018)
Figur 2 beskriver en simpel sekvens av rörelser i ett spel av luffarschack. Position a är
starttillståndet. Motståndaren gör sin tur och tillståndet flyttar till b. Nu överväger
algoritmen dess möjliga handlingar, de streckade linjerna är olika handlingar som
algoritmen överväger men handlingen som leder till tillstånd c har historiskt sett haft den
högsta belöningen i slutet. Detta leder till att algoritmen väljer att röra sig till tillstånd c.
Efter att motståndaren har gjort sin tur befinner vi oss i tillstånd d. I detta tillstånd har
handlingen som leder till e* haft den största belöningen på långt sikt men algoritmen
bestämmer sig för att göra en stokastisk handling med att välja e istället. Dessa handlingar
kallas för utforskande handlingar. Med hjälp av utforskande handlingar letar algoritmen
efter bättre lösningar. Det är möjligt att e leder till slut till en högre belöningen än om
algoritmen valt e*. Om denna utforskande handling ledde till en hög belöning kommer den
att väljas oftare i samma tillstånd, men om belöningen är låg kommer den att inte att väljas
senare i samma tillstånd.

16

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Även om detta exempel illustrerar ett möjligt användningsområde av i förstärkt inlärning,
kan det ge intrycket att förstärkt inlärning är mer begränsat än vad det egentligen är. Sutto
och Barto (2018) hävdar dock att även om luffarschack är ett spel mellan två personer, kan
förstärkt inlärning också användas för problem där det inte finns någon extern motståndare.
Förstärkt inlärning är inte heller begränsat till problem där beteendet delas upp i separata
avsnitt, som de separata spelen luffarschack. Det är lika tillämpligt när beteendet fortsätter
på obestämd tid och när belöningar av olika storlekar kan erhållas när som helst. Allmänna
principen inom förstärkt inlärning kan också användas i tidskontinuerliga problem.
Modell-fira system behöver inte någon modell av miljön överhuvudtaget. Modell-fria
system tänker inte enligt Sutton och Barto (2018) hur enskilda handlingar påverkar miljön.
Luffarschack problemet är dels ett modellfritt problem, spelaren har ingen modell över
motståndaren. Modellerna måste vara relativt pålitliga för att vara till någon nytta, modellfria metoder har en fördel över metoder med en modell om det blir för komplicerat att
beskriva miljön till agenten. Då blir det lättast om agenten själv lär sig hur miljön ser ut
genom de olika handlingar och tillstånd.

3.3.2 Markov-beslutsprocess
Sutton och Barto (2018) beskriver att en Markov-beslutsprocess (eng. Markov decision
process) är processen som försöker lösa ett problem i förstärkt inlärda algoritmer. Denna
process innehåller bearbetandet av den indata den får för att välja en handling de i olika
situationerna. Markov-beslutsprocess är en klassisk formalisering av sekventiellt
beslutsfattande, där handlingar inte bara påverkar den omedelbara belöningen utan också
påföljande tillstånd eftersom den tar i beaktande framtida belöningar. Detta betyder att
denna beslutsprocess kan överväga framtida belöningar istället för att söka efter kortsiktiga
belöningar.
Markovs beslutsprocess används enligt Sutton och Barto (2018) för att lösa ett problem
med av att lära algoritmen en kedja av handlingar för att uppnå ett mål. Den del som lär sig
kallas för agenten och delen utanför agenten kallas för miljön. Dessa två delar agerar med
varann hela tiden, agenten gör en handling och miljön ger tillbaka ett nytt tillstånd. Miljön
17

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
ger också tillbaka en belöning för handlingen. Som det tidigare nämndes spelar inte
belöningen av en enskild handling en stor inverkan utan målet är att maximera belöningen
på lång sikt genom de olika handlingarna.

Figur 3. Interaktionen mellan agenten och miljön i en Markov-beslutsprocess, ur
Sutton och Barto (2018)
Vid varje steg får agenten en belöning, R i figur 3, som bestäms av miljön. Till exempel en
oväntad handling kan ge -25 som belöning och en förväntad handling kan ge 10. I en ändlig
Markov-beslutsprocess finns det en ändlig mängd olika tillstånd som kan finnas, i qinlärning är alla dessa möjliga tillstånd och olika handlingar för dessa tillstånd sparade i en
tabell som uppdateras periodiskt.
Sutton och Barto (2018) beskriver Markov-beslutsprocess som ett abstrakt och flexibelt
ramverk som kan tillämpas på många olika problem på många olika sätt. Tidsstegen
behöver till exempel inte hänvisa till fasta intervaller, de kan hänvisa på varandra
påföljande steg i beslutsfattande processen. Handlingen som algoritmen fattar kan vara på
en låg nivå, till exempel att byta spänningar som appliceras på en robots motorer, eller
beslut på en hög nivå, till exempel om man ska äta lunch eller skippa lunchen. På liknande
sätt den kan indata som algoritmen får beskriva tillståndet ta en mängd olika former. Den
kan bestämma på en låg nivå, som direkt avläsningen på ett värde av en viss sensor. Den
kan också vara på en hög nivå, som till exempel symboliska beskrivningar av objekt i ett
rum.
Till exempel kan vissa åtgärder bestämma vad en agent väljer att tänka på eller var den
fokuserar sin uppmärksamhet på. I allmänhet kan handlingar vara vilket beslut som helst
man vill lära agenten att göra och tillstånden ska innehålla allt man kan tänka på att kan
vara till nytta för agenten för att göra den förväntade handlingen.
18

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Gränsen mellan agenten och miljön är vanligtvis inte densamma som den fysiska gränsen
för en robot eller djurens kropp. Till exempel bör motorerna till en robot och dess mjukvara
betraktas som delar av miljön snarare än delar av agenten. På samma sätt, om vi tillämpar
MDP-ramverket på en person eller ett djur, bör muskler, skelett och organ betraktas som
en del av miljön.
Den allmänna regeln som vi följer är att allt som inte kan ändras godtyckligt av agenten
anses vara utanför det och därmed en del av miljö den är i. Sutton och Barto (2018) anser
att vi inte ska anta att allt i miljön är okänt för agenten. Till exempel vet agenten ofta hur
dess belöningar beräknas som en funktion av dess handlingar. I vissa fall känner agenten
till allt om hur dess miljö fungerar men den står fortfarande inför ett inlärningsproblem.
Flera känner till exempel till hur en Rubiks kub fungerar och vad målet är, men lyckas ändå
inte lösa det.

3.3.3 Mål och belöningar
Sutton och Barto (2018) beskriver att i förstärkt inlärning formuleras agentens syfte eller
mål i form av en signal som kallas belöningen. Miljön ger agenten en belöning efter varje
handling. Agentens mål att maximera den totala belöningen den får. Detta innebär inte att
maximera den omedelbara belöningen, utan den kumulativa belöningen på lång sikt.
Användningen av en belöning för att uppnå ett mål är ett av de tydligaste särdragen
i förstärkt inlärning. Sutton och Barto (2018)
Även om formulering av mål som belöningar kan till en början verka begränsande, har det
i praktiken visat sig vara flexibelt och tillämpligt i flera situationer menar Sutton och Barto
(2018). Exempelvis, för att få en robot att lära sig själv gå har forskare tilldelat en belöning
för varje steg som är leder till att roboten rör sig framåt. Genom att få en robot att lära sig
fly från en labyrint är belöningen ofta -1 för varje steg som den tar; detta uppmuntrar
agenten att fly så snabbt som möjligt. För att lära en robot att hitta och samla tomma
läskburkar för återvinning, kan man ge den en belöning av noll största delen av tiden, men
sedan en belöning av +1 för varje burk som samlas in. Det är också vanligt att ge roboten
19

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
negativa belöningar när den gör oväntade handlingar. För att lära en agent att spela ett spel,
som till exempel schack, är de naturliga belöningarna +1 för att vinna och -1 för att förlora.
Som det nämndes tidigare är agentens mål är alltid att maximera sin belöning sin framtida
kumulativa belöning.
Belöningssignalen är ett sätt att kommunicera till roboten vad du vill att den ska
uppnå, inte hur du vill att den ska uppnås. Sutton och Barto (2018)
Sutton och Barto (2018) berättar att det alltså är viktigt för att de belöningar vi skapar
indikerar vad vi vill uppnå. Belöningen ska inte ge agenten förkunskaper om hur man vill
att den ska uppnå målet. Till exempel bör en schackspelande agent belönas bara för att
vinna, inte för att uppnå underliggande mål som till exempel att äta motståndarens bitar
eller att få kontroll över spelplattans mittpunkt. Yuxi (2018) instämmer med Sutton och
Barto (2018) och nämner att för att lära en förstärkt inlärd algoritm att spela Go får den
också belöningen först till slut. Om att uppnå delmoment belönades, kan agenten hitta ett
sätt att uppnå dem utan att uppnå det verkliga målet, vilket var att vinna spelet. Belönings
signalen är ett sätt att kommunicera till roboten vad man vill att den ska uppnå, inte hur
man vill att den ska uppnås.

3.3.4 Episoder
Agentens mål är att maximera den kumulativa belöningen den får på lång sikt. Om
sekvensen av belöningar som mottas efter varje handling betecknas:𝑅𝑡+1 , 𝑅𝑡+2 , 𝑅𝑡+3 ,
vilken aspekt av denna sekvens ska maximeras? Sutton och Barto (2018) berättar att i
allmänheten strävas efter att maximera den förväntade avkastningen, där avkastningen,
betecknad Gt, definieras som någon specifik funktion i belöningssekvensen. I det enklaste
fallet är avkastningen summan av belöningarna:

T beskriver det sista steget eller med andra ord den sista en handling som agenten utför.
Enligt Sutton och Barto (2018) fungerar denna formel i applikationer där det finns en
20

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
naturlig uppfattning om det sista steget eller handlingen, det vill säga när agentmiljöinteraktion bryter naturligt till slutet av en episod. Ett exempel på detta kunde vara ett
spel av luffarschack eller ett försök att hitta ut ur en labyrint. Varje episod slutar i ett
tillstånd som kallas terminal tillståndet. Detta följs av en återställning till ett standard start
tillstånd eller ett stokastisk tillstånd som följer de givna reglerna i miljön. Även om episoder
slutar på olika sätt, som att vinna och förlora ett spel, börjar nästa episod i ett tillstånd som
är oberoende av hur det föregående slutade.
I andra fall bryts inte agent-miljöinteraktion naturligt i olika identifierbara episoder, utan
den fortsätter oändligt. Dessa kallas för kontinuerliga problem, ett exempel på detta kunde
vara en algoritm som övervakar en process kontinuerligt. Belönings funktionen kan inte
användas på samma sätt här som i exemplet ovan eftersom T kommer vara oändligt. För att
räkna ut belöningen för en algoritm som fungerar på detta sätt behövs en mer komplicerad
formel enligt Sutton och Barto (2018).
En annat koncept som behövs är diskontering (eng. discounting). Diskonteringen
presenteras av symbolen ᵧ, som är ett värde mellan 0 och 1. Med diskonteringen ändras
formeln för att beräkna avkastningen:

Sutton och Barto (2018) hävdar att mängden av diskonteringen bestämmer hur stort värde
som läggs på de framtida belöningarna. Ifall värdet närmar sig 0 kommer agenten bara att
fokusera sig på kortsiktiga belöningar, belöningen i framtiden spelar inte en stor roll. Om
värdet närmar sig 1 kommer agenten att värdera framtida belöningar i mycket högre grad.
Agenten värdesätter stora belöningar i framtiden i stället för stora belöningar genast. I flesta
fall är detta det förväntade resultatet.

3.3.5 Utforskning och exploatering
Om agenten skulle känna till miljön på förhand menar Otterlo & Wiering (2012) att det går
att räkna ut den optimala policyn till problemet. Men problem som löses med förstärkt
inlärning finns inte denna kunskap över miljön. Då blir det nödvändigt för att interagera
21

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
med miljön för att lära agenten en policy genom försök och misstag metoden. Agenten
måste utforska miljön genom att utföra handlingar och förstå handlingarnas konsekvenser.
Otterlo & Wiering (2012) menar att agenten måste utforska miljön för att se om det finns
möjliga förbättringar av dess policy, alltså en bättre lösning till problemet. Då måste den
pröva sig fram och forska miljön slumpmässigt. Detta kan sluta i sämre prestanda eftersom
åtgärderna kan också vara sämre än den nuvarande policyn. Men utan att pröva olika
handlingar hittar den aldrig möjliga förbättringar.
Russel och Norvig (2003) tar upp att i vissa fall kan algoritmen hitta en lösning till
problemet som den fastnar för, men denna lösning är inte den optimala. Om denna situation
uppstår kallas agenten för en girig agent. En girig agent hittar sällan den optimala lösningen
till problemet utan fastnar vi det första som fungerar.
Eftersom algoritmen inte känner igen den miljö den befinner sig i fastnar den lätt för en
icke optimal lösning. Russel och Norvig (2003) anser att för att undvika att en agent inte
ska bli girig ska den programmeras att konstant överväga mellan utforskning och
exploatering. Exploatering är då när algoritmen följer vad dess policy säger kommer att
leda till den högsta belöningen. Detta leder till att se på problemet kortsiktigt och att stöda
sig på de tidigare händelserna. Utforskning är då algoritmen nu och då tar en stokastisk
handling istället för den handling som ens policy säger. På detta vis är det meningen att
algoritmen kan sakta men säkert hitta nya lösningar till problemet som hoppeligen är bättre
än den gamla. Utforskning görs alltså för att möjligen öka den framtida belöningen.
Russel och Norvig (2003) skriver att endast exploatering leder till giriga agenter som inte
hittar den optimala lösningen och endast utforskning leder till att agenten lär sig aldrig
något. Vad är då den bästa lösningen, en gyllene mellanväg? Russel och Norvig (2003)
hävdar att det har forskats mycket inom detta ämne. Kortfattat ska agenten vara girig men
ändå utforska oändligt. En vanlig förkortning som används för detta är GILE (eng. greedy
in the limit of infinite exploration). Om agenten följer GILE schema ska den försöka sig på
varenda möjliga alternativ för att hitta de bästa lösningarna och att vara säker att man inte
missat en bättre lösning.

22

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
En agent som använder sig av GILE schemat kommer enligt Russel och Norvig (2003) till
slut att lära sig hur miljön den befinner sig i ser ut. Till slut måste agenten sluta utforska
eftersom vi kan anta att den redan har hittar den optimala lösningen. Ett sätt att få agenten
att sluta utforska är att minska på chansen att agenten väljer en utforskande handling, som
Sutton och Barto (2018) nämner, tills den blir 0 eller nära 0.

3.3.6 Erfarenhetsuppspelning
Liu & Zou (2017) skriver att vid förstärkt inlärning observerar agenten en ström av
händelser och använder varje händelse för att uppdatera sin interna uppfattning över miljön
och vilken handling leder till vad. Till exempel kan en upplevelse vara en tupel av (tillstånd,
handling, belöning, nytt tillstånd) och agenten använder varje upplevelse för att uppdatera
dess värdefunktion.
I flera förstärkt inlärda algoritmer kastas händelsen bort omedelbart efter att det har använts
för en uppdatering. De senaste genombrotten i förstärkt inlärning har fört med sig en viktig
teknik som kallas erfarenhetsuppspelning (eng. experience replay memory). Upplevelser
är lagrade i en minnesbuffert av en bestämd storlek; när bufferten är full, kastas de äldsta
minnen bort. Vid varje steg samplas ett slumpmässigt antal upplevelser från bufferten för
att uppdatera agentens parametrar. Liu & Zou (2017) bevisar i deras undersökning att
storleken av denna minnesbufferten kan spela en stor roll, både för liten och för stor buffert
påverkar hur snabbt agenten lär sig. De fann att det finns fall där den bästa agenten lär sig
snabbast med att inte kasta bort alls gamla händelser men i flera fall snabbas upp
inlärningen om inte denna minnesbuffert inte är allt för stor.
Det har bevisats att erfarenhetsuppspelning spelar en viktig roll i att förbättra resultaten i
förstärkt inlärning. Till exempel Silver et.al (2016) och Mnih et al. (2015) har använt sig
av erfarenhetsuppspelning med stor framgång.

23

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

3.4. Q-inlärning
Markovs beslutsprocess som behandlades i kapitel 3.3 ger en modell att beräkna
sannolikheten att röra sig från ett stadium till ett annat för att maximera värdefunktionen.
Detta skulle kallas för en modellbaserad algoritm. Callan (2003) menar att denna modell
av miljön inte alltid finns tillgänglig, då kallas algoritmen för modell-fri. Den modell-fria
agentens uppgift är att själv lära sig den optimala policyn för att maximera värdefunktionen
genom att utforska miljön. Agenten observerar och minns resultatet efter varje interaktion
med miljön. Detta minne är viktigt för att agentens läroprocess.
I verkligheten menar Callan (2003) att detta skulle betyda att i början rör sig algoritmen
slumpmässigt från ett stadium till ett annat. Efter flera episoder skulle algoritmen
slumpmässigt snubbla över en lösning som ger en stor vinst. Beroende på balansen mellan
utforskande och exploatering, som behandlades i 3.3.5, kommer algoritmen sedan att
balansera mellan att röra mot de stadier som har en garanterad stor vinst och nya,
outforskade stadier i hopp om att hitta en lösning med ännu större vinst.

Figur 4. Ett simpelt problem för en förstärkt inlärd algoritm ur Callan (2003)
Figur 4 representerar ett enkelt problem som en kan lösas med hjälp av en förstärkt
inlärning. Ruta 8 är start stadiet och ruta 11 är vårt mål, ruta 7 och 9 är väggar och ruta 10
är en sjö som är svår att passera. Målet är att röra sig från start till mål med så få steg som
24

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
möjligt. Vi ger algoritmen -1 poäng för varje steg, ett undantag är att röra sig genom sjön
som ger -2 poäng. Målet get algoritmen 10 poäng. En algoritm kan först hitta en lösning
längs den streckade linjen, denna lösning skulle ge -6 poäng om vi summerar varje rörelse
och sedan 10 poäng från målet vilket betyder att slutresultatet skulle vara 4 poäng. Med
hjälp av utforskning kan algoritmen försöka hitta andra lösningar. Hoppeligen skulle den
förr eller senare snubbla på en annan lösning, en rutt längs den andra pilen i figur 4 som
ger 5 poäng till sist och slut.
Målet med förstärkt inlärning är att främja de handlingar vid varje stadie som i slutet av
episoden maximerar mängden poäng som agenten får. Beroende på resultatet i slutet av
episoden vill vi öka nyttovärden för de handlingar som leder till höga poäng i slutet och
minska på nyttovärden på de handlingar som leder till låga poäng i slutet av episoden.
Callan (2003) berättar att ifall inlärningstakten 𝛼 är konstant kan det hända att algoritmen
misslyckas att konvergera till ett optimalt resultat. För att undvika detta kan vi bestämma
att 𝛼 minskar vid varje försök av handling i ett visst stadie. Detta kan enkelt beräknas ifall
man håller reda på hur många gånger som agenten gjort en viss handling i ett visst stadie.
Callan (2003) berättar att q-inlärnings algoritmen har en tabell som förvarar q-värden för
varje möjliga handling i varje möjliga stadie. När agenten i algoritmen bestämmer sig för
att röra sig från ett stadie till ett annat kollar den upp q-värden i tabellen för stadiet den
ligger i. De kan till exempel se ut som {0.1, 0.4, 0.7, 0.3}, i detta fall skulle agenten välja
handling nr 3 på grund av att 0.7 hade det högsta q-värdet.

Formel 1. Formeln för att uppdatera Q-värden i Q-inlärning ur Callan (2003)
Callan (2003) berättar att i början är alla q-värden i tabellen 0, men de uppdateras med hjälp
av formeln ovan. Algoritmen som är presenterad nedan börjar med att ändra varje värde i
tabellen till noll. Sedan börjar algoritmen att utforska sin miljö där den väljer alltid den
handling i som har det högsta q-värdet. En sak som fattas är ett kriterium för när inlärningen
ska sluta. Callan (2003) menar att ofta brukar man kolla ifall ett kriterium är uppfylt för att

25

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
sluta med inlärningen, till exempel medeltalet av poängen av de successiva episoderna är
tillräckligt högt.

Formel 2. En algoritm för Q-inlärning ur Callan (2003)
Denna algoritm som presenteras ovan är inte programmerad att utforska slumpmässigt nu
och då, vilket är viktigt för att algoritmen inte ska stanna vid den första lösningen den hittar,
alltså ett lokalt minimum. Callan (2003) rekommenderar att i början kan agenten utforska
ofta men sakta men säkert ska den börja mer och mer föredra händelserna med högre Qvärde.

3.4.2 Q-inlärning exempel
I detta kapitel visar jag ett exempel från nätsidan www.learndatasci.com skriven av Kansal
& Martin (2019) som beskriver hur man kan programmera en förstärkt inlärd algoritm som
använder sig av q-inlärning i python. Problemet som de försöker lösa är att lära en taxi
plocka upp en passagerare och släppa av den vart den vill. Jag kommer att referera till
tidigare kapitlen för att få en mer detaljerad bild över de olika delmomenten.
Som figur 3 (sida 20) visar består sig inlärningsprocessen av en miljö och en agent. Figur
7 är en mer detaljerad bild av samma process. Miljön ger ett tillstånd till agenten samt en
belöning som baserar sig på slutresultatet av den förra handlingen. Agenten får ett tillstånd
26

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
som indata. Efter detta är dess uppgift att fatta ett beslut som hoppeligen ger största möjliga
belöning. Inlärningsprocessen kan således delas in i sex olika steg enligt Kansal & Martin
(2019):
1. Observera miljön
2. Bestäm hur det lönar sig agera enligt en bestämd strategi
3. Agenten gör ett beslut
4. Få en belöning (eller straff)
5. Lär dig från händelsen och optimera strategin
6. Upprepa tills en optimal lösning hittas

Figur 4 Inlärningsprocessen för en förstärkt inlärning. Kansal & Martin (2019)
Problemet som vi försöker lösa är att lära en taxi att hitta samt plocka upp en passagerare
och föra den till sin destination med så få steg som möjligt. Det kommer också att finnas
några väggar som taxin måste lära sig att undvika.

Figur 5. Visualisering av miljön, ur Kansal & Martin (2019)
27

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Figur 5 är en visualisering av den miljö som taxin kommer att kunna röra sig i. Miljön
består av ett 5x5 rutfält och fyra stycken punkter där passageraren kan bli plockad upp eller
lämnad (R, Y, B, G). I figur 5 är det meningen att taxin ska plocka upp passageraren från
ruta (0,4) och hen ska bli lämnad i ruta (0,0). Kansal & Martin (2019) berättar att detta
betyder att det finns 25 olika ställen var taxin kan befinna sig. Fem olika ställen var taxin
passageraren kan finnas, i (R, Y, B, G) eller inne i taxin samt fyra möjliga destinationer för
passageraren om vi antar att den finns en möjlighet att hen vill bli släppt av i samma ruta
som hen blev plockad upp.
Efter summering av alla de möjliga tillstånden, 5 x 5 x 5 x 4 = 500, betyder det att det finns
500 olika möjliga tillstånd denna miljö. Miljö som koncept är noggrannare beskrivet i
kapitel 3.1.
Efter fastställningen av miljön samt problemet måste det bestämmas hur taxin kan röra sig
i miljön. För att röra sig måste taxin göra en handling, handlingar är noggrannare beskrivna
i kapitel 3.1 och 3.2. I detta problem har taxin 6 olika handlingar som den kan fatta:
1. Röra sig mot syd
2. Röra sig mot nord
3. Röra sig mot öst
4. Röra sig mot väst
5. Plocka upp passageraren
6. Släppa av passageraren
Vid varje tillstånd i miljön måste taxin alltså välja mellan dessa handlingar. Olika
handlingar leder till olika mängder av poäng för taxin. Eftersom målet är att lära taxin att
lösa problemet, kommer de förväntade handlingarna belönas och de oväntade straffas. Ett
exempel på en oväntad handling kunde vara till exempel om taxin försöker åka över väggen
eller om den försöker släppa av passageraren vid fel ställe. Målet är att taxin ska hitta en
lösning med så få steg som möjligt ger vi också ett litet straff för varje handling. Agenten
får en stor belöning då den har lyckats plocka upp passageraren och fört hen till rätt plats.

28

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
I tillståndet som taxin befinner sig i figur 5 måste den välja mellan de sex olika
handlingarna som är givna till den. I början är sannolikheten lika stor för varje handling
men efter att agenten har blivit tränad kommer dessa att ändra för att reflektera de bästa
alternativen i de givna tillstånden. Kansal & Martin (2019) påpekar att i detta stadie har
taxin en vägg till väst, ifall taxin skulle försöka gå in i väggen kommer den bli bestraffad
och den kommer inte att röra sig. På grund av att vi lär taxin att lösa problemet i så få steg
som möjligt kommer taxin att börja undvika väggarna eftersom de leder till en bestraffning
men inget framsteg.
Fö att lösa problemet i frågan används q-inlärning. Q-inlärning använder sig av en Q-tabell
var den förvarar sannolikheten att välja olika alternativ vid varje tillstånd. Tabell 1 och 2
illustrerar hur sannolikheten för varje möjliga handling förvaras i tabellen före agenten har
blivit tränad och efter. Till exempel i stadie nr 328 som taxin befinner sig i tabell 2 kommer
agenten att välja handling nr 1 (rör sig mot nord) eftersom det har visat sig i träningen att
denna handling ger den högsta möjliga totala belöningen i slutet av episoden ifall
passageraren är i ruta (0,4) och taxin befinner sig i ruta (1, 3). För att uppdatera de olika
värden i Q-tabellen används formel i formel 1 i kapitel 4.

Tabell 1 och 2 . Visualisering av belönings tabellen i Q-inlärning före träningen
och efter. Ur Kansal & Martin (2019).
29

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Tre vanliga variabler som används för att optimera inlärningen är:
α: Alpha, som också syns i formel 1. Detta är inlärnings hastigheten (eng. learning
rate) som används också i övervakad inlärning.
ℽ:

Gamma.

Gamma

är

diskonteringsfaktorn

för

algoritmen.

En

hög

diskonteringsfaktor (nära 1) används för att taxin skulle välja lösningar som get på
lång sikt den högsta möjliga belöningen. Ifall gamma är nära 0 tar taxin bara i
beaktande de lösningar som ger högsta belöning på kort sikt. Diskonteringen
behandlas noggrannare i kapitel 3.4.
𝛜: Epsilon. Epsilon är förknippad med utforskning och exploatering, högre epsilon
leder till en högre grad an utforskning. Detta behandlas i kapitel 3.6.
I början av träningen kommer agenten att göra flera misstag och bli bestraffad i form av
minuspoäng men sakta men säkert kommer den att röra sig mot en optimal lösning.
Taxi problemet kan lösas med q-inlärning, men nackdelen med q-inlärning är att den kräver
en tabell som den kan referera till. Ifall mängden tillstånd och möjliga handlingar blir större
blir denna tabell också exponentiellt större. Till exempel om taxin skulle befinna sig i ett
10x10 rutfält skulle det finnas redan 10 x 10 x 5 x 4 = 2000 olika möjliga tillstånd som
taxin kan befinna sig i.
För att undvika en massiv q-tabell kan ett neuralt nätverk användas istället för Q-tabellen
för att förvara resultaten agentens förflutna handlingar i de olika tillstånden. Detta kallas
för djup q-nätverk och behandlas i kapitel 3.5.4.1.

3.5. Djupinlärning
Med hjälp av djupinlärning är det enligt LeCun et al. (2015) möjligt att göra
beräkningsmodeller som består av flera bearbetningslager för att lära sig mönster ur data
med flera abstraktionsnivåer. Dessa metoder har drastiskt förbättrat den senaste tekniken
inom taligenkänning, objektigenkänning i bilder och många andra domäner såsom
läkemedels uppteckning. Djupinlärning upptäcker invecklade struktur i stora datamängder
30

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
med hjälp av backpropagation algoritmer. Backpropagation används för att indikera hur en
maskin ska ändra sina interna parametrar (vikterna mellan noderna) som används för att
beräkna representationen i varje lager från representationen i föregående lager.
Konventionella maskininlärningstekniker var begränsade i deras förmåga att bearbeta data
från naturen i sin råa form. Om man ville göra ett mönsterigenkänningssystem eller
maskininlärningssystem under de senaste decennier krävde det enligt LeCun et al. (2015)
ett noggrant konstruerat funktions extraktor (eng. feature extractor). Denna omvandlar den
råa naturdata (som en bilds pixelvärden) till en lämplig representation eller funktionsvektor.
Med hjälp av detta kan inlärningssystemet, ofta en klassificerare, upptäcka eller klassificera
mönster av dess input.
Djupinlärningsmetoder

är

metoder

för

lärande

av

representation

med

flera

representationsnivåer. Djupinlärning består således av flera enkla icke-linjära moduler som
var och en omvandlar representationen på en nivå (börjar med indata) till en representation
på en högre, en mer abstrakt nivå. Med hjälp av tillräckligt många av sådana moduler kan
en maskin lära sig mycket komplexa funktioner. För klassificeringsproblem förstärker
högre skikt av representation aspekter av indata som är viktiga för diskriminering
irrelevanta variationer. (LeCun et al. (2015))
En bild kan till exempel vara i form av en matris med pixelvärden. De inlärda funktionerna
i det första representationslagret representerar vanligtvis närvaron eller frånvaron av kanter
vid särskilda riktningar på platser i bilden. Det andra lagret upptäcker vanligtvis motiv
genom att upptäcka speciella kantarrangemang, oavsett små variationer i lägen av kanterna.
Det tredje lagret kan montera motiv i större kombinationer som motsvarar delar av kända
föremålen, de efterföljande lagerna skulle upptäcka objekt som är en kombination av dessa
delar. (LeCun et al. (2015))
Djupinlärning har visat sig vara bra på att upptäcka strukturer i högdimensionella data och
är därför tillämpar sig på många områden inom vetenskap, företag och myndigheter.
I ett klassificerings problem försöker maskinen till exempel para ihop rätt bild med rätt
kategori. Bilden visas flera gånger till maskinen och varje gång ger den ut en poängvektor
för varje klass. Maskinen ändrar sedan de interna justerbara parametrarna efter varje
31

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
visning för att minska mängden fel klassificeringar. Dessa justerbara parametrar kallas ofta
för vikter. Dessa är i verkligheten siffror som kan ses som 'knoppar' vilka definieras av
maskinens ingångs- och utgångsfunktion. I ett typiskt djupinlärningssystem kan det finnas
hundratals miljoner av dessa justerbara vikter och hundratals miljoner märkta exempel som
man kan träna maskinen. (LeCun et al. (2015))

3.5.1 Neurala nätverk
En neuron är en cell i hjärnan vars huvudsakliga uppgift är att samla och bearbeta elektriska
signaler. Hjärnans förmåga att bearbeta information anses vara kopplad till förmågan att
bilda flera nätverk av dess neuroner. På grund av denna uppfattning av hjärnans
funktionalitet, skriver Russel och Norvig (2003), riktades den tidigaste forskningen inom
artificiell intelligens in på att bilda artificiella neurala nätverk. Russel och Norvig skriver
att redan 1943 har McCulloch och Pitts ritat upp ett schema över hur dessa artificiella
neuroner ska fungera. Grundprinciperna i deras modell var att neuronen kommer att
aktiveras när dess input överstiger en given gräns.
Russel och Norvig (2003) berättar att ett neuralt nätverk består av flera noder som är
anslutna till varan genom riktade länkar. Länken mellan noderna binder dem till varan, och
länkens funktion är också att föra signalen vidare för att aktivera nästa nod. Varje länk har
en specifik parameter som kallas för vikt, och denna vikt bestämmer styrkan på länken.
Varje nod beräknar först summan av dess input med hänsyn till länkens vikt som signalen
kommer ifrån. Denna summa läggs in i en aktiveringsfunktion (eng. activation function)
som sedan avgör om noden ska aktiveras.
Aktiveringsfuntkionen har två uppgifter enligt Russel och Norvig (2003). Först och främst
är det meningen att noden är aktiv, vilket betyder nära +1, ifall en rätt input är given, och
nära 0 ifall en fel input är given. Resultatet förväntas vara icke linjärt. För att uppnå detta
använder vi till exempel oss av en tröskelfunktion (eng. threshold function) eller en
sigmoidfunktion.

32

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Figur 6. En tröskelfunktion (a) och en sigmoidfunktion (b) ur Russel och Norvig
(2003)
Båda funktionerna har ett tröskelvärde, antingen hårt (tröskelfunktionen) eller mjukt
(sigmoidfunktionen) berättar Russel och Norvig (2003). Vikten av länken till noden måste
dock tas i hänsyn när man räknar värdet för input och kollar det i mot den bestämda
tröskeln.
Russel och Norvig (2003) nämner två typer av neurala nätverk, nämligen framåtriktade
nätverk (eng. feed-forward networks) och återkommande nätverk (eng. recurrent
networks). Ett framåtriktat nätverk representeras endast av en funktion av dess nuvarande
input, detta betyder att den inte har något internt stadie annat än dess vikter. Ett
återkommande nätverk är en aning mer komplicerat, den matar ens output tillbaka till ens
ägna inputs. Detta kan leda till ett stabilt nätverk men i vissa fall kan slutresultatet var
nästan kaotiskt enlig Russel och Norvig (2003). Denna typs nätverk har dock också
fördelar, de kan nämligen han ett korttidsminne också, detta är i motsats till framåtriktade
nätverk som inte kan.

Figur 7. Enkelt neuralt nätverk ur Russel och Norvig (2003)

33

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Figur 5 representerar ett enkelt framåtriktat neuralt nätverk med två input värden (1,2), två
dolda enheter (3,4) samt vår output 5. W är vikten mellan dessa två enheter.

Beräkningarna ovan visar att nätverkets output räknas ut genom att räkna summan av
vikterna och enheternas input vid varje nod. Värdet för nod 5, vår output, är summan av
resultatet av nod 3 och 4. Nätverket lär sig enligt Russel och Norvig (2015) genom att ändra
vikterna mellan enheterna, eftersom då ändrar nodens output.
Det vanligaste sättet att räkna ut sluttningsvektor som används för att ändra värden på
vikterna är enligt LeCun et al. (2015) stokastisk gradientstigning (eng. stochastic gradient
decent). Detta består av att visa inmatningsvektorn några exempel, och på basen av dess
output justera vikterna i enligt resultatet. Efter träningen av algoritmen med hjälp av
träningsuppsättningen testas dess prestanda med hjälp av en testuppsättning som består av
exempel som algoritmen inte sett förr. Detta görs för att testa hur bra algoritmen är att
generalisera, vilket betyder att hur bra algoritmen kan lösa dess givna problem med data
som den inte sett under träningen.
En vanlig form av ett framåtriktat nätverk som Russel och Norvig (2015) nämnde kallas
för Multi layer perceptron. Förkortas ofta till MLP. MLP kallas ofta för den vanligaste
formen av neurala nätverket. I MLP-nätverk är varje nod kopplat till alla andra noder i
följande lager. Dessa lager kallas för fullt kopplade lager (eng. fully connected layers).
Dense layers är också en vanlig benämning för dessa lager.

3.5.2 Faltningsnätverk
Faltningsnätverk (eng. Convolutional neural network, CNN) är en annan vanlig form av
neurala nätverk. Enligt LeCun (2015) är dessa nätverk skapade för att bearbeta indata som
kommer i form av flera matriser, till exempel en färgbild som är sammansatt av tre
34

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
tvådimensionella-vektorer som innehåller värdet för pixeln i de tre färgkanalerna (röd,
grön, blå). Flera typer av data formas med hjälp av flera matriser: en dimension för signaler
och sekvenser, inklusive språk; två dimensioner för bilder eller ljudspektrogram; och tre
dimensioner för video. Det finns fyra viktiga idéer bakom fältningsnätverk som drar nytta
av egenskaperna hos naturliga signaler: lokala förbindelser, sammanslagning,
användningen av många lager och delade vikter.
Arkitekturen av ett vanligt faltningsnätverk liknar ofta arkitekturen som avbildas i figur 8.
De första stegen är innehåller två typer lager: faltningslager och samlingslager.
Faltningslagrens uppgift är att känna igen lokala motiv, medan samlingslagrens uppgift är
att slå samman likadana motiv. Data i matriser, som till exempel bilder, formar ofta lätt
igenkännbara lokala motiv. I faltningsnätverk spelar det ingen roll var i bilden dessa
igenkännbara motiv uppkommer, detta är viktigt på grund av att till exempel ögat av en
hund kommer knappast vara på samma ställe i varje bild. (LeCun et al. 2015)
Ofta finns det enligt LeCun et al. (2015) två till tre lager av faltningslager som följs av ett
samlings lager, dessa följa av mer faltningslager och till sist ett lager som har som output
en poängvektor. Som flera andra neurala nätverk använder faltningsnätverk sig också av
bakåtpropagering. Dessa faltningslager och samlingslagren i faltningsnätverket har blivit
inspirerade av hur synceller fungerar med simpla celler och komplexa celler.
LeCun et al (2015) och Duda och Hart (1973) hävdar att sedan 1960 talet har vi vetat att en
linjär klassifikations algoritm kan dela dess input i bara några enkla områden. Problem
uppstår då bilden och ljud kräver en ingångs- och utgångsfunktion som inte bryr sig om
enkel variation som ställning, lutning eller belysningen av ett objekt. Samtidigt måste
maskinen vara dock sensitiv för små skillnader som till exempel problemet att skilja på en
vit varg som en Samoyed hund som LeCun et al (2015) tar upp.
LeCun et al. (2015) tar upp ett exempel med två Samoyed hundar och i olika ställningar
och miljöer. De menar att hundarna kan se olika ut, medan en vit varg och en Samoyed
hund i samma miljö och position kan likna varan till en hög grad. En klassifikations
algoritm som kollar endast på pixelvärden kan kämpa med att urskilja på djuren i det senare
exemplet. På grund av detta behövs en funktions extraktor som tar i hänsyn speciella
aspekter på djuren som är distinkta för den givna rasen. Vanligtvis har dessa funktions
35

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
extraktorer gjorts för hand menar LeCun et al. (2015), men detta kan undvikas i dagens
läge om man använder algoritmer med allmänt syfte som själv lär sig dessa specifika
detaljerna. Detta är en av djupinlärnigens märkvärdigaste fördelar.

Figur 8 beskriver stegen i ett vanligt faltningsnätverk. Bilderna representerar varje
nivås output vid de gömda enheterna. Bilderna innehåller de inlärna
känneteckande särdrag för att känna igen olika djur. Nivån högst upp letar efter
olika kanter, nivån under den letar efter en samling av kanter osv. Ur LeCun et al.
(2015)

LeCun et al. (2015) fortsätter med att berätta att faltningsnätverk har använts med stor
framgång sedan början av 2000 talet i upptäckande och igenkännande av olika objekt och
områden i bilder. I början användes dessa främst till problem som redan hade en hel del
kategoriserade data, som till exempel att känna igen trafikmärken. Nyligen har den senaste
framgången varit i att känna igen ansikten.
Faltningsnätverk blev dock inte populära förrän ImageNet tävlingen år 2012. Nätverk
tränades med 1000 olika klasser och en miljon olika bilder, de bästa lösningarna halverade
mängden fel klassificeringar jämfört med tävlande teknologier enligt Krizhevsky et al.
(2012). Krizhevsky kom med flera förbättringar som till exempel en ny teknik som kallades
dropout där vissa noder stängs ner för att förbättra nätverkets chanser att generalisera data
istället för att minnas det utantill. En annan viktig förbättring var att generera mera
träningsdata med hjälp av att förvränga de existerande exempel. Denna tävling startade en
36

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
revolution i datorsyn. Faltningsnätverk är än i denna dag den dominerande algoritmen för
att känna igen och upptäcka objekt.

3.5.3 Recurrent neural network
Efter att backåtpropagering introducerades användes det i början till för att lära RNNnätverk (eng. recurrent neural network). LeCun et al. (2015) menar att för problem som har
en sekventiell input som till exempel talprov och text är det ofta bäst att använda ett RNN.
De processar ens input ett element i taget. Samtidigt kommer nätverkets dolda enheter åt
en vektor som innehåller information av alla tidigare element i sekvensen. Nätverket har
med hjälp av denna vektor ett minne av de tidigare input värden.
LeCun et al. (2015) fortsätter med att påstå att RNN har visat sig vara ett väldigt kraftfullt
system. Att träna dem har dock visat sig vara problematiskt eftersom den bakåtpropagerade
lutningen ökar eller minskar vid varje steg vilket leder till att lutningen kan enligt LeCun
et al. (2015) “explodera eller försvinna”. Med hjälp av framsteg i deras arkitektur och olika
sätt att träna nätverket har det visat sig att RNN kan vara väldigt bra i att förutspå nästa
bokstaven i ett ord eller nästa ordet i en mening.
Fastän RNN-nätverkets uppgift är att lära sig samband på lång sikt, finns det flera
undersökningar som bevisar att det är svårt att lagra information i nätverk på lång sikt. Ett
exempel är Bengio et al. (1994) som skriver att det är svårt att lära samband på lång sikt
med hjälp av gradientnedstigning. För att fixa detta problem tar LeCun et al (2015) upp en
undersökning gjord av Hochreiter och Schmidhuber (1997) var de introducerade LSTM
nätverk. LSTM står för långt-korttidsminne (eng. long short term memory). Dessa nätverk
skiljer sig från RNN med att de skulle ha speciella dolda enheter var uppgift var att minnas
input under en lång tid. LeCun et al. (2015) menar att LSTM nätverk har visat sig vara mer
effektiva än konventionella RNN.

3.5.4 Djup förstärkt inlärning
Q-inlärning har samma problem som andra simpla maskininlärningsalgorimer, om
datamängden blir för stor eller komplex blir beräkningarna extremt komplexa. Till exempel
nackdelen med Q-inlärning är att alla stadier och möjliga handlingar måste lagras i en
37

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
tabell. Komplexa problem med flera variabler kan leda till en tabell som inte ryms på en
dators RAM-minne.
För att lösa detta problem kan djup förstärkt inlärning användas. Yuxi (2018) berättar att
istället för en massiv tabell för q-värden, kan vi använda ett neuralt nätverk som uppskattar
nyttovärden och vinstfunktionen för de olika handlingarna. Istället för att ändra värden i en
tabell ändras vikterna i nätverket. En stor fördel med att använda ett neuralt nätverk istället
för en stor tabell är att nätverket kan uppskatta nyttovärdet för handlingar i ett visst tillstånd
fastän agenten inte har besökt det specifika tillståndet ännu. Detta är på grund av att
nätverket lagrar resultatet av de tillstånd den har sett och detta påverkar nyttovärden av
motsvarande tillstånd. I q-inlärning ändras värden i q-tabellen endast då agenten besökt
tillståndet och valt en specifik handling. Q-inlärning kräver enligt Callan (2003) att agenten
besöker varje tillstånd flera gånger för att ändra q-värden i tillståndet. I extremt komplexa
problem är det möjligt att algoritmen besöker vissa sällsynta tillstånd väldigt sällan vilket
gör att nyttovärden för det tillståndet inte konvergerar till en optimal handling.

3.5.4.1 Djupa Q-nätverk
Mnih et al. (2015) har utvecklat en förstärkt inlärd algoritm som de kallar för ett djupt qnätverk (DQN). Denna algoritm kombinerar ett faltninsnätverk med q-inlärning som är
specialiserad för att hantera mångdimensionella data, som till exempel bilder. Sutton och
Barto (2018) skriver att före Mnih et al. (2015) hade neurala nätverk och faltningsnätverk
producerat imponerande resultat men att kombinera dem med en förstärktinlärning var
sällsynt.
Målet för Mnih et al. (2015) var att skapa en förstärkt inlärd agent som kan lösa flera olika
problem utan att ändra parametrarna i algoritmen eller att ge den problem specifika input
data. De demonstrerade detta med att låta deras DQN-algoritm spela 49 olika Atari 2600spel med hjälp av en emulator. Mnih et al. (2015) valde att kombinera deras
faltningsnätverk med Q-inlärning enligt Sutton och Barto (2018) på grund av att Qinlärning är modell-fri och den har ingen bestämd policy.

38

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Figur 9. Schemat för DQN algoritmen som Mnih et al (2015) använde. Den består
av tre lager av faltninsnätverk som är kopplade med två helt kopplade lager som
till sist leder till output lagret som är kopplat med emulatorn för att röra sig i spelen.
Ur Mnih et al. (2015)
Som input använde Mnih et al. (2015) endast bilder direkt ur spelen. Bilderna från spelen
var 210 x 160 pixlar, men de förminskade dessa bilder ner till 84x84 pixlar för att minska
mängden dimensioner samt för att försnabba träningen, mindre bilder kräver mindre
beräkningar.
DQN-algoritmen spelade enligt Mnih et al. (2015) varje spel för en tidsperiod som skulle
motsvara 38 dagar. Algoritmen lärde sig att spela dessa spel bättre än alla tidigare förstärkt
inlärda algoritmer i all utom 6 spel. De jämförde också hur algoritmen klarade sig jämfört
med en människa. Ifall algoritmen fick 75% eller mera poäng i ett visst spel ansåg de att
den var lika bra eller bättre på spelet som en människa. Mnih et al (2015) konstaterade att
DQN-algoritmen klarade att sig lika bra eller bättre än en människa i 29 av de 49 spel som
testades.
Sutton & Barto (2018) menar att det är imponerande att någon inlärd algoritm klarade sig
så bra som denna, men vad som gör detta ännu mer imponerande är att det var samma
inlärningssystem som användes för att spela de olika spelen. Inga specifika modifikationer
gjordes och inga specifika instruktioner för respektive spel gavs till algoritmen.

39

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

4. VÄRDERINGEN AV FINANSIELLA MEDEL MED
HJÄLP AV MASKININLÄRNINGSMETODER
4.1. Grunder i aktievärdering
Detta kapitel handlar om grunderna till aktievärdering och hur man kan värdera företag.
Teknisk analys baserar sig på en analys av en akties historiska prisutveckling i hopp om att
det ska ge en inblick i dess framtida prisutveckling. Fundamental analys fokuserar mera på
företaget och dess nyckeltal. Kapitlet behandlar också hypotesen om effektiva marknader
och random walk-teorin. De är två av de mest kända teorierna som hävdar att det inte är
möjligt att förutspå en akties framtida prisutveckling och att man inte kan öka avkastningen
på investeringar utan att öka risken av investeringarna.

4.1.1 Teknisk analys
Enligt Nassitroussi et al. (2014) finns det två kända metoder för att förutspå hur
aktiemarknaden kommer att röra sig. Det första sättet kallas för teknisk analys och det andra
sättet är fundamental analys som behandlas i nästa kapitel. Teknisk analys baserar sig
prediktionerna på historiska aktiedata för att hitta trender som kan fungera som stöd och
motstånds punkter för kurssvängningarna. Till detta sätt hör också följandet och att
undersöka olika analytiska modeller som glidande medelvärde och relativa styrkeindexet.
Vid användning av teknisk analys är en graf över aktiekursens historiska utveckling en
aktiehandlarens främsta verktyg. Denna graf är ofta uppritad antingen som en linje eller
ljusstakediagram.

Figur 10 Ljusstakediagramkomponenter ur Nesbitt och Barass (2004)

40

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Figur 10 illustrerar en ljusstake i ett ljusstakediagram. Varje ljusstake representerar en
tidsperiod, som till exempel en minut eller en dag. Toppen av ljusstaken (high) är det högsta
värdet under tidsperioden och svansen av ljusstaken (low) är det lägsta värdet under
tidsperioden. När en ny period börjar ritas det första strecket. Detta vågräta streck (open)
är det första uppmätta värde på den nya tidsserien. Det sista värdet på tidsserien illustreras
med ett till vågrätt streck (close). Beroende på om värdet gått upp eller ner är det sista
vågräta strecket ovanför eller nedanför det första strecket. I figur 10 illustreras en uppgång
av priset illustrerat med en vit ljusstake och en nedgång med en svart.
Glidande medelvärde använder, enligt Wong, Mazur och Chew (2010), en historisk period
“n”, till exempel 100 dagar eller 20 minuter. Efter att man valt sin period på till exempel
100 dagar, räknar man medeltalet av aktiens stängningspriser för de senaste 100 dagarna.
Denna punkt ritas på en graf tillsammans med aktiens värde. Efter att denna process
upprepas för varje dag, leder det till att en linje börjar bildas på grafen som visar aktiens
glidande medelvärde på den givna perioden. Denna linje kan fungera som stöd och
motstånd för kurssvängningarna, vilket i sin tur kan leda till köp och säljsignaler. Till
exempel ifall aktiens värde har rört sig ovanför ett visst glidande medelvärde och historiskt
alltid när aktiekursen tangerar det glidande medelvärdet har kursen skjutit i höjd, då kan
detta ses som stöd för kurssvängningarna och som en köpsignal ifall kursen håller sig
ovanför medelvärdet. Ifall kursen sjunker under medelvärdet kan det ses som en säljsignal
för att aktiekursen har tappat sin motståndpunkt.
Wong et al. (2010) skriver att det relativa styrkeindexet (RSI, relative strength index) kan
användas för att undersöka om en viss aktie är “oversold” eller “overbought”. Dessa är
benämningar för en aktie ifall den är relativt svag eller stark. För att räkna ut RSI behövs
det på samma sätt som med glidande medelvärde väljas en viss tidsperiod. Indexet rör sig
mellan 0 och 100, och beroende på vad indexet visar kan man dra slutsatser om en aktie
kommer att gå upp eller ner. Ett exempel kunde vara då man valt en period på 14 dagar och
RSI börjar stiga över 70, vilket indikerar att under de senaste 14 dagarna har aktien varit
relativt stark och att en korrigering nedåt i priset kommer att inträffa snart. Det motsatta
inträffar då RSI varit svag och närmar sig 30 under samma period. Detta är en signal på att
en korrigering uppåt är att vänta.

41

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Figur 11 Exempel på mönster som används i teknisk analys ur Nesbitt och Barass
(2004)
Enligt Nesbitt och Barass (2004) används flera mönster som upprepas ofta också som köp
och säljsignaler för handlare. I figur 2 illustrerar a och b ett mönster som ofta leder till en
negativ värdeförändring, dessa mönster kallas för head and shoulders och double top. De
två följande mönstren, c och d, indikerar en positiv värdeförändring, dessa kallas för flags.
Aase (2011) har räknat upp tre principer för teknisk analys och dessa är: marknaden tar i
beaktande allting, priserna följer trender och att historien brukar upprepa sig själv. Den
första principen, att marknaden tar i beaktande allting, betyder att allt som händer, vare sig
det är fundamentalt, politiskt eller annat kommer det att reflekteras i priset. Den andra
principen, att priserna följer trender, är grunden till teknisk analys. Marknaden har alltid en
uppåtgående, nedåtgående eller en sidlänges trend. Priserna följer dessa trender tills något
utomstående händer som påverkar på utbudet och efterfrågan av aktien. Målet med att följa
trenden är att hoppa på den och sedan sälja när man ser tecken på att den börjar att avta.
Den sista principen är att historien brukar upprepa sig själv. De som använder sig av teknisk
analys tror att investerare kommer kollektivt upprepa samma beteende som investerare före
dem, och detta leder till mönster som går att förutspå. Nyckeln till att förstå framtiden är
således gömd i historien.

42

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

4.1.2 Fundamental analys
En fundamental analys undersöker ett företags finansiella data för att analysera om
företaget är överprissatt eller underprissatt. Detta sker genom att analysera företagets
nuvarande och historiska fundamentala data för att se hur de har utvecklats. Utifrån detta
görs en prognos om företagets framtid. Enligt Aase (2011) görs denna analys i tre olika
steg: ekonomisk analys, industrianalys och företagsanalys. Om det beräknade värdet på
företaget i analysen är högre än vad företaget är värderat till på marknaden, anses det som
en köpsignal. Det motsatta inträffar då det beräknade värdet är mindre än vad företaget är
värderat till på marknaden.
Företag som är börsnoterade kommer varje kvartal ut med flera rapporter, och bland dessa
är

de

viktigaste

enligt

Aase

(2011)

balansräkningen,

resultaträkningen,

kassaflödesanalysen och utbetalda dividender. Den fundamentala analysen av ett företags
finansiella tillstånd börjar med att undersöka dessa rapporter. Utifrån rapporterna räknas
olika nyckeltal ut som t.ex. skuldsättningsgraden, P/E (eng. price/earnings) och PEG (eng.
price/earnings to growth ratio). P/E är priset på en aktie delat med företagets vinst per aktie.
PEG motsvarar P/E, men det beaktar också den förväntade tillväxten av företaget. PEG
används för företag som växer i snabb takt så att de inte verkar övervärderade jämfört med
andra aktier som växer i en långsammare takt.
Enligt fundamental analys kan marknaden värdesätta en aktie fel på kort sikt, men detta
kommer att korrigeras på lång sikt. Vinst kan göras då man investerar i en aktie som är
värdesatt fel och sedan väntar man på att marknaden korrigerar sig själv och omprissätter
aktien.
Nilsson, Isaksson och Martikainen (2002) skriver att den fundamentala analysen kan delas
in i tre olika huvudmoment som illustreras nedan.

43

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Figur 12 Fundamentala analysens huvudmoment ur Nilsson et al (2002)
I strategisk analys analyseras enligt Nilsson et al. (2002) huvudsakligen ett företags
vinstdrivare eller framgångsfaktorer. Det är också viktigt att identifiera riskerna med
företagets verksamhet. Slutresultatet av en strategisk analys är en uppskattning om
företagets framtida marknadssituation. Detta görs oftast i form av en uppskattning av den
framtida omsättningen och dess framtida marknadsandelar.
Följande steg är redovisningsanalysen där analysens syfte enligt Nilsson et al. (2002) är att
bedöma i vilken grad som företagets redovisning avspeglar företagets underliggande
verksamhet. Målet med analysen är att bedöma kvaliteten på den redovisade resultat- och
balansräkningen med hjälp av att djupare analysera vissa poster i företaget där
värdesättningen ger utrymme för flexibilitet. Till exempel kan man studera noggrannare
företagets avskrivningar för att få en inblick i om resultatet är onormalt högt på grund av
låga avskrivningar som inte avspeglar den egentliga förbrukningen
Följande steg är redovisningsanalysen där analysens syfte enligt Nilsson et al. (2002) är att
bedöma i vilken grad som företagets redovisning avspeglar företagets underliggande
verksamhet. Målet med analysen är att bedöma kvaliteten på den redovisade resultat- och
balansräkningen med hjälp av att djupare analysera vissa poster i företaget där
värdesättningen ger utrymme för flexibilitet. Till exempel kan man noggrannare studera
företagets avskrivningar för att få en inblick i om resultatet är onormalt högt på grund av
låga avskrivningar som inte avspeglar den egentliga förbrukningen.
Efter dessa tre delmoment görs en prognos över företagets framtid i form av en proforma.
Proformamodellen innebär enligt Nilsson et al. (2002) att man konstruerar en
44

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
prognostiserad resultat- och balansräkning. Proforma används främst till att förutspå värdet
av framtida betalningsströmmar. Den sista delen är själva värderingen av företaget, och hit
hör enligt Nilsson et al. (2002) två olika ansatser, nämligen substansvärdering och
avkastningsvärdering. Substansvärderingen utgör en värdering av företagets befintliga
skulder samt tillgångar. Avkastningsvärderingen görs utifrån de prognostiserade
avkastningsmåtten, kassaflöden och residualvinster. Med hjälp av dessa kan företagets
framtida värde uppskattas genom en nuvärdesberäkning.

4.1.3 Hypotesen om effektiva marknader
Nassitroussi et al. (2014) skriver om hypotesen om effektiva marknader. Enligt denna
hypotes är alla marknader effektiva i att ta in all information och sedan justera aktiepriset
omedelbart upp eller ner för att reflektera det nya värdet av aktien. Enligt denna hypotes är
det alltså omöjligt att få högre avkastning på sitt kapital utan större risker jämfört med
marknaden när man har samma information som resten av marknaden vid
investeringsbeslutet.
Fama (1965) formulerade hypotesen om effektiva marknader 1965, men hen ändrade den
1970. Den nya hypotesen bestod av 3 olika nivåer, vilka är stark, semistark och svag (Fama,
1970). Enligt Nassitroussi et al. (2014) kan marknadens effektivitet direkt korreleras med
hur tillgänglig informationen är. Detta betyder att all information ska vara tillgänglig för
alla för att en marknad ska rankas som stark effektiv. I svagt effektiva marknader är det
möjligt att förutspå med god sannolikhet hur marknaden kommer att utvecklas. När ny
information släpps ut, absorberas den av marknaden som justerar aktiekursen enligt den
nya informationen, varefter den nya informationen blir redundant. Hur snabbt denna
information absorberas är dock nyckelfaktorn i om en marknad är starkt eller svagt effektiv.
Aase (2011) anser att ifall en marknad är svag finns det en möjlighet att få högre avkastning
med hjälp av fundamental analys eftersom man har en chans att investera innan marknaden
rör sig. Teknisk analys kommer inte att fungera eftersom en svag marknad betyder att
framtida priser inte går att förutspå på lång sikt med hjälp av analys av historiska priser. På
svaga marknader rör sig aktiekurserna inte enligt något mönster utan de följer
slumpvandring ifall det inte uppkommer någon ny fundamental information. Detta betyder
45

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
enligt Aase (2011) dock inte att aktiekurserna genast skulle förändras när ny fundamental
information kommer ut, vilket betyder att analys av nyhetsartiklar kunde ge högre
avkastning.
Aase (2011) berättar att vid semistarka marknader är det inte enligt hypotesen om effektiva
marknader möjligt att på lång sikt få högre avkastning eftersom aktiekursen reflekterar all
ny information snabbt men opartiskt. Detta leder till att det inte är pålitligt att använda
varken fundamental eller teknisk analys. Vid starka marknader säger hypotesen att
aktiekursen reflekterar fullt all information och ingen kan få högre avkastning än
marknaden utom vissa få som råkar ha bra tur.
Hypotesen om effektiva marknader har dock enligt Aase (2011) fått mycket kritik eftersom
den anser att alla investerare handlar rationellt. I verkligheten finns det flera kognitiva
biaser som till exempel bandwagoneffekt och konfirmeringsbias. Ett annat exempel kunde
vara Grauwe (2010) som skriver om John Kaynes djuriska instinkter vilka är definierade
som vågor av optimism och pessimism som också påverkar marknader orationellt.

4.1.4. Random walk-hypotesen
Schumaker och Chen (2006) skriver om random walk-hypotesen som liknar mycket den
semistarka delen av hypotesen om effektiva marknader. Enligt hypotesen är all information
tillgänglig för alla och på grund av detta anses det vara ineffektivt att förutspå framtida
kurssvängningar eftersom priserna fluktuerar slumpmässigt. Aase (2011) skriver att enligt
hypotesen är den bästa investeringsstrategin köp och håll, alltså att passivt investera, detta
betyder att man köper aktier och håller dem på lång sikt. Motsatsen till detta är att använda
sig av aktivt investerande där man utnyttjar kurssvängningar för att maximera avkastningen
med hjälp av att sälja när aktien är överprissatt och att köpa när den är underprissatt.
Aase (2011) skriver om ett experiment som gjordes för att stöda hypotesen. I detta
experiment ritades en graf över en akties prissvängningar, var kurssvängningarna
bestämdes slumpmässigt genom att singla slant. Efter att grafen ritats undersökte en
analytiker grafen var hen hittade köpsignaler. Detta användes som bevis på att
kurssvängningarna är lika slumpmässiga som att singla slant. Aase (2011) berättar att det
finns flera studier som har gjorts för att motbevisa random walk-hypotesen. Ett exempel på
46

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
detta är A non-random walk down wall street skriven av Lo och MacKinley (1999) som
innehåller signifikanta empiriska motbevisa för random walk-hypotesen.

4.2. Handling med finansiella instrument med hjälp av
förstärkt inlärning
Detta kapitel är en kort genomgång av tidigare forskning om handling med finansiella
medel med hjälp av förstärkt inlärning. Fokus ligger på annan forskning som har använt sig
av djupt förstärkt inlärning för att handla finansiella instrument. De finansiella
instrumenten i fråga är valutamarknaden, index och aktier.
Jangmin et al. (2005) menar att under de senaste decennierna har flera algoritmer använts
för att handla aktier. Flera av dem har använt komplexa matematiska formler för att
evaluera aktier inför handel. Dessa matematiska formler brukar basera sig på antingen
teknisk analys, som behandlas i kapitel 4.1.1, på en fundamental analys, som behandlas i
kapitel 4.1.2. Med hjälp av formlerna försöker algoritmerna hitta undervärderade aktier
med potential för hög avkastning. Det finns dock en hel del forskning som anser att det inte
är möjligt att hitta undervärderade aktier, som till exempel hypotesen om effektiva
marknader som behandlas i kapitel 4.1.3. Enligt hypotesen är det inte möjligt att få en högre
avkastning utan att öka risken med investeringen, och att all information om marknaden
och de företag som den hör till är tillgänglig för alla. Aktiekurserna anpassar sig genast när
ny information kommer ut. Detta har dock inte stoppat flera från att försöka.

Azhikodan et al. (2019) har undersökt om man kan använda en djupt förstärkt inlärd
algoritm för att handla aktier. Azhikodan et al. (2019) har fokuserat på att göra en
sentimentanalys på aktien genom att analysera aktiens trend med hjälp av ett RNN-nätverk.
Målet med undersökningen var att visa att en förstärkt inlärd algoritm kan lära sig de
generella dragen från träningsdata som krävs för att handla aktier.
Algoritmen som Azhikodan et al. (2019) skapade försöker inte handla aktier dagligen utan
målet är att den ska hålla aktien i några dagar. Figuren nedan illustrerar en överblick av
deras algoritm:

47

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Figur 13. En överblick över algoritmen ur Azhikodan et al. (2019)
Algoritmens indata är rubriker från nyhetsartiklar samt historiska aktiedata. Algoritmen
fattar beslut som grundar sig på dessa data om det lönar sig att köpa eller sälja aktien.
Azhikodan et al. (2019) tar upp två exempel fall där de simulerade användningen av deras
algoritm på två olika aktier, Google och General Electric. Algoritmen tränades på en
månads data varefter målet för algoritmen var att maximera portföljens värde med hjälp av
effektiv handel av aktierna. I båda fallen lyckades algoritmen öka värdet på portföljen
jämfört med att passivt investera i aktien, men det är oklart hur bra denna algoritm skulle
fungera på andra aktier som den inte har blivit tränad på. Azhikodan et al. (2019) menar
dock deras undersökning visar att man kan lära en förstärkt inlärd algoritm att handla aktier
kräver vidare arbete för att få bättre resultat.
Dai et al. (2019) har skrivit en vitbok där de använde förstärkt inlärning för att handla på
valutamarknaden. De valde valutamarknaden på grund av dess höga likviditet. En hög
likviditet lämpar sig bra till algoritmer som handlar ofta, likvida instrument har också ofta
en liten spridning (eng. spread) som gör att man tappar endast lite vinster på grund av
slirande ( eng. slippage). Spridningen är skillnaden mellan köp- och säljkursen av aktien
vid en viss tidpunkt. Aktier som handlas aktivt har ofta en liten spridning. Slirande har att
göra med mängden aktier som erbjuds att sälja eller köpa vid en viss tidpunk. Till exempel,
om man vill köpa 100 st aktier av Nokia Oyj, och det finns ett bud på endast 50 st aktier
till priser 3,56, kan man endast köpa 50 st aktier av den första försäljaren och 50 st
resterande aktier köps av nästa säljare, till exempel till priset 3,57. Då blir medelpriset av
uppköpet 3,565 fastän det bästa budet var vid köptillfället 3,56. Slirande och spridningen
48

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
påverkar investeraren således endast när man vill köpa eller sälja aktier. Detta betyder att
det är viktigare för aktiv investerare att ha en låg spridning och lite slirande än en passiv
investerare.
Dai et al. (2019) använder sig av data för tre olika valutapar på sekund nivå. De tränar först
algoritmen på 3 veckor värt av data och sedan testar de modellen på de två följande dagarna
efter träningsperioden. De berättar att de fokuserar sig endast på att optimera ett av dessa
valutapar, således är resultatet av de två andra paren en bra indikator hur bra algoritmen är
att generalisera.
Algoritmen har tre olika möjligheter, den satsar antingen på att valutans värde kommer att
gå ner, upp eller att den är oförändrad (1 /0/-1). Vid varje tillstånd använder de sig av en
timme värt av historiska data som indata. I början använder de sig av en relativt stor
uppspelnings buffert men mot slutet av träningen minskar de på storleken av den för att
prioritera nyligen inträffade händelser.
Slutsatsen av forskningen gjort av Dai et al. (2019) var att deras djupt förstärkt inlärda
algoritm inte lyckades hitta en optimal lösning. Algoritmen lärde sig att välja en neutral
position i de flesta fall, alltså att den anser att valutaparets värde skulle förbli oförändrat.
De anser att orsaken till detta är att algoritmen har svårt att hitta bra särdrag från deras
träningsdata som skulle hjälpa algoritmen att fatta bra handelsbeslut.
Deng et al. (2016) har också försökt tackla detta problem. I deras undersökning har de som
de tidigare forskningarna i detta kapitel använt sig av förstärkt inlärning kopplat med djup
inlärning. Problemet som de har fokuserat sig på är hur man ska summera situationen på
den finansiella marknaden på ett sätt att algoritmen kan tolka det och använda sig av det
för att fattahandelsbeslut. Rå finansiella data klassas enligt Deng et al. (2016) att innehålla
mycket störningar som kan negativt påverka hur algoritmen kan lära sig från dessa indata.
För att motkämpa detta brukar man använda olika finansiell indikatorer som till exempel
glidande medelvärde för att klargöra trenden av ett finansiellt instrument.
Deng et al. (2016) menar att till exempel glidande medelvärde är ett bra sätt att klargöra
trenden av ett instrument men det är ofta en dålig indikator eftersom den reagerar inte

49

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
tillräckligt snabbt vid trendbyte. Detta leder till att algoritmen fattar dåliga handelsbeslut.
På grund av detta har man valt att använda rå finansiella data trots dess nackdelar.
Deng et al. (2016) fokuserar på S&P 500-indexet i sin undersökning. Deras indata är
dagliga data av S&P 500-indexet. Till skillnad från andra liknande undersökningar,
bestämde de sig också för att använda data från andra stora index runtom i världen som
indata. Orsaken till detta är att de anser att S&P 500-indexet är starkt påverkat av
svängningar på den globala marknaden. Till exempel om de andra indexen i världen miskan
i värde, kommer s&p 500-indexet säkert också att följa med.

Tabell 3. Tabellen visar hur mängden lager och mängden noder påverkar vinsten.
TP är “Total profit” vilket betyder “den totala vinsten”. Ur Deng et al. (2016)
Tabell 3 visar att Deng et al. (2016) har undersökt hur storleken och formen på det neurala
nätverket som används i deras förstärkta inlärda algoritm påverkar resultatet. Slutsatsen är
att mängden lager och noder tycks korrelera positivt med den totala vinsten. Den största
vinsten fås med ett nätverk med 5 lager som har 256 noder var.
Det viktigaste slutsatserna ur Deng et al. (2016) är:
1. Rå aktiedata kan räcka, och är i vissa fall är det bättre än att använda färdiga
tekniska indikatorer som glidande medelvärde. Det är bättre att låta algoritmen själv
hitta särdragen att som den följer.
2. Storleken på nätverket kan korrelera positivt med vinsterna.

50

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Huang (2018) har undersökt ett motsvarande problem som behandlas i denna avhandling.
Huang använder sig likt Dai et al. (2019) av valutamarknaden som källa av träningsdata.
Valutaparenas data är hämtad från 2012 - 2017 i 15 minuters intervall. Fokus i
avhandlingen av Huang (2018) ligger på att försöka göra en handels algoritm som försöker
handla diverse valutapar.
Likt Dai et al. (2019) och Azhikodan et al. (2019) använder Huang (2019) också av LSTM
nätverk. Huang (2019) använder sig av tre lager av 256 noder i LSTM lager, fjärde lagret
är helt kopplat (eng. Fully connected) med bara 3 noder. De sista tre noderna motsvarar de
tre olika handlingsalternativen som algoritmen har.
Var Huang (2019) skiljer sig från de andra är med två klara förbättringar till strukturen av
handelsalgoritmen.
1. I vanliga fall brukar förstärkt inlärda algoritmer få deras belöning efter ett antal
händelser, eller i slutet av episoden. Detta motsvarar schack exemplet som
behandlas i kapitel 3.3. Huang (2019) har kommit fram till att det är bättre att ge
en belöning till algoritmen oftare när man använder sig av finansiell data som aktier
eller valutapar. Enligt Huang lär sig algoritmen snabbare och bättre om man ger en
belöning efter varje handelsbeslut. T.ex. +10 för en bra handling och -10 för en
dålig.
2. Uppspelningsminnet är relativt litet jämfört med de andra förstärkt inlärda
arkitekturer. Detta hjälper algoritmen att fokusera sin träning endast på de senaste
händelserna.
Du et al. (2016) och Berotoluzzo & Corazza (2012) har också forskat inom detta ämne.
Forskningen av Du et al. (2016) har fokuserat på att optimera ens portfölj med hjälp av en
förstärkt inlärd algoritm. Algoritmens mål är att balansera portfölj med i hänsyn till Capital
Asset Pricing Model-teorin (CAPM). CAPM tar i enligt Fama & French (2004) i beaktande
sambandet mellan risken och avkastningen av investeringen. Berotoluzzo & Corazza
(2012) har försökt med hjälp av en förstärkt inlärd algoritm att förutspå det framtida värdet
av Banca Intensa som är en aktie på en Italienska aktiemarknaden.

51

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Det som binder Du et al. (2016) och Berotoluzzo & Corazza (2012) ihop är att de har båda
använt sig av sharpekvoten (eng. Sharpe ratio) för att räkna ut belöningen för en handling.
Orsaken varför sharpekvoten används är för att ta i beaktande risken av investeringen när
man räknar värdet av avkastningen. En hög avkastning är inte alltid det bästa alternativet
ifall risken i investeringen är stor.

52

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

5. PORTFOLIO OPTIMERING OCH AKTIEHANDEL MED
HJÄLP AV FÖRSTÄRKT INLÄRNING
För att svara på avhandlingens forskningsfrågor har jag lärt en förstärk inlärd algoritm att
analysera aktier baserat på n antal historiska aktiedata. Detta kapitel kommer att förklara
hur denna algoritm är uppbyggd, hur den är tränad och vad resultaten är i hänsyn till
forskningsfrågorna. Algoritmen är skriven i Python. Länk till den fullständiga koden finns
som bilaga.
För att uppnå en optimal modell använde jag mig av Jenkins modell, mer noggrant stegen
5,6,7 vika var Experimental design, Data capture och Data analysis. Jag började med en
möjlig design av algoritmen, tränade den och testade den, och till slut analyserade jag
resultaten. På basen av denna analys gjorde jag förändringar till algoritmen och miljön. Jag
itererade ofta över dessa tre steg. Till exempel jobbade jag mycket med att optimera
frekvensen av belöningen och storleken av belöningen.
I början fick algoritmen en belöning först efter att den kört igenom alla aktier en gång, för
att förbättra resultaten ändrade jag belöningsfrekvensen så att agenten fick en belöning efter
att den kört igenom en aktie. Efter varje förbättring blev resultaten sakta men säkert bättre,
men jag märkte dock också att algoritmen blev mycket mera försiktig med sina handlingar.
Då jag gav en belöning först efter att algoritmen kört igenom en hel aktie handlade
algoritmen mera men variansen i resultaten var mycket högre. För vissa aktier kunde
algoritmen vinna referensportföljen med 100% men i andra fall förlorade den mot
referensportföljen med 80%. Efter detta blev jag inspirerad av Huang (2019) och jag valde
att ge en belöning efter varje steg. Variansen av resultaten minskade, och i medeltal hade
algoritmen en högre vinstprocent jämfört med referensportföljen än med de tidigare
belöningsmetoderna.
I kapitel 2 nämndes att test metodologin som användas baserar sig på Shmueli & Koppious
(2011) forskning om empiriska modeller för prognoser. Shmueli & Koppious (2011) anser
att man kan testa modellen på framtida observationer eller på andra observationer som inte
var i test datan. De slutliga modellerna testades på sex olika set av testdata, både framtida
observationer och andra observationer på samma tidsperiod.
53

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

5.1 Aktie handlings beslut med djup q-inlärning
Strukturen av algoritmen motsvarar markovs-beslutsprocess ur Sutton och Barto (2018).
Agenten får som indata ett tillstånd och belöningen för förra handlingen i förra tillståndet.
Med denna information bestämmer agenten ifall den ska köpa, sälja eller göra ingenting
vid detta tillstånd.

Figur 13. Strukturen av den förstärk inlärda handelsalgoritmen.
Belöningen av algoritmen tar inspiration från Huang (2019). Huang ger en belöning till
agenten efter varje handling. Belöningen räknas ut med denna formel:
Belöning = (Skillnaden mellan referensportföljens värde och agentportföljens
värde vid det nuvarande tillståndet) - (Skillnaden mellan referensportföljens värde
och agentportföljens värde vid det förra tillståndet)
Storleken på belöningen är således direkt kopplad till ökningen eller minskningen av
skillnaden mellan referensportföljens värde jämfört med agentportföljens värde. Idén
bakom detta är att agenten ska inte bara lära sig vilka handlingar som är bra och dåliga utan
också hur den ska värdesätta olika handlingar.
Referensportföljen motsvarar en passiv investering. Detta betyder att i början av träningen
köper referensportföljen aktier med alla dess likvida medel och säljer dem inte under
simulationen. Idén med referensportföljen blir då att simulera aktiens värdeökning om man
inte handlar aktivt med aktien. Målet med algoritmen är att få en högre avkastning på
samma aktier jämfört med en passiv investering.
54

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Varje tillstånd består av en mängd n aktiedata på daglig nivå. Indata är en tabell med fem
kolumner och n antal rader som representerar de olika dagliga observationerna. Innan
dessa data skickas till agenten normaliseras tabellen för att snabba upp träningen, och för
att göra de olika tabellerna oberoende av varandra. Normaliseringen gör att skillnaden
mellan de olika dagarna är relativa, inte absoluta, i förhållande till de andra dagarna i
tabellen. Tabellerna normaliseras med min ⎯ max-normalisation
Efter att agenten fattat ett beslut skickas handlingen till miljön. Om agenten skickar en 0
betyder det att aktien är svag och alla aktier ska säljas bort. Vid svar 1 händer ingenting,
och vid svar 2 köper agenten aktier för alla sina oanvända medel. I träningsfasen kan
agenten kan inte köpa bara några aktier eller sälja bara en del, utan portföljens alla medel
är investerade i aktier eller inga medel är investerade i aktier.
Agenten kan inte heller blanka. Blankning används för att göra vinst på en sjunkande
marknad genom att sälja aktier man inte äger. Aktier lånas för detta ändamål av en
aktieförmedlare. Vid ett senare skede köps aktierna tillbaka efter att kursen har sjunkit och
man returnerar aktierna till aktieförmedlaren. Som exempel, 1st aktie av Nokia Oyj säljs
för 3,65 och köps tillbaka för 3,55.
Medan algoritmen tränas görs några förenklingar till aktiehandeln för att underlätta
undersökningen. På grund av detta kommer träningens resultat inte motsvara hur
algoritmen skulle fungera om den skulle handla med riktiga aktier. Målet med simulationen
är att endast ge en riktgivande bild över handelsalgoritmens prestation.

Följande förenklingar har gjorts:
1: Köpandet eller säljandet av aktier sker på den sista minuten när börsen håller på
att stänga, i simulationen köps / säljs aktierna till det sista priset som aktien handlats
för en viss dag. I verkligheten är det väldigt svårt att handla för detta pris, men
antagandet är att nu och då får agenten ett bättre/sämre pris för aktien och de jämnas
ut till slut. Detta betyder också att slirande inte heller tas i beaktande.
2: I träningen eller testandet tas inte heller i beaktande någon transaktionsavgift
eftersom flera aktieförmedlare ger gratis aktieförmedling på den amerikanska
55

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
aktiemarknaden. T.ex TD Ameritrade, E*TRADE, Interactive brokers och Alpaca
Markets ger med varierande modeller gratis aktieförmedling.

5.3 Träning och test
I detta kapitel redogör jag över den testmetologin jag använt och exempel på misslyckade
modeller och åtgärder jag gjort för att uppnå en optimal modell. Otaliga modeller har
tränats för att hitta en kombination av de möjliga variablerna som skulle leda till det bästa
resultatet. Att träna en modell tar cirka 14 – 24 timmar. Det finns flera variabler som
påverkar hur snabbt en modell kan tränas, nedan finns några av dessa variabler listade:
● Storleken på neurala nätverket. Ett större nätverk tar överlag ofta en längre
tid att träna.
● Typen av neurala nätverket. Mängden och komplexiteten av beräkningarna
bakom de olika typerna av neurala nätverk som testades skiljer sig från
varann.
● Inlärningshastigheten α. Högre α leder till en kortare träningstid med en för
stor α kan leda till att ingen optimal lösning hittas.
● Hårdvara. Hårdvaran av dator påverkar träningstiden drastiskt. Ett
grafikkort (GPU) är ett måste för att träna motsvarande algoritmer i en
rimlig tid. Snabbare grafikkort med mera inbyggt minne leder i de flesta fall
till en kortare träningstid. I bilagorna finns en beskrivning på den dator som
användes för att träna modellerna i denna avhandling.
Modellerna testas på S&P 500-aktier som inte hör till S&P 100. S&P 100-aktierna hör till
träningsdata, vilket gör att de måste tas bort från testdata. Testdata innehåller cirka 400
aktier efter raderingen av S&P 100 aktierna från S&P 500 aktierna.
Träningen kommer att ske på dagliga data av S&P 100 aktier från 2017-01-03 till 2019-0430. Alla modeller som kommer att testas på sex olika set av testdata:
Testset 1: Aktierna ABMD - FLT. Tidsperiod: 2017-01-03 till 2019-04-30
Testset 2: Aktierna FLIR- PFG. Tidsperiod: 2017-01-03 till 2019-04-30
Testset 3: Aktierna PGR- ZTS. Tidsperiod: 2017-01-03 till 2019-04-30

56

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Testset 1 framtid: Aktierna ABMD - FLT. Tidsperiod: 2019-05-01 till 2020-03-20
Testset 2 framtid: Aktierna FLIR- PFG. Tidsperiod: 2019-05-01 till 2020-03-20
Testset 3 framtid: PGR- ZTS. Tidsperiod: 2019-05-01 till 2020-03-20

Tanken bakom de olika testsetten var att försöka hur algoritmen fungerar på olika typer av
marknader. Den första och längre tidsperioden var så gott som bara en stigande marknad,
ett undantag var i slutet av 2018 då aktierna sjönk på grund av rädsla över ett handelskrig
mellan USA och Kina. Den andra tidsperioden valdes för att den innehåller en skarp
kursnedgång i mars 2020 då rädslan över Covid-19 började. En generaliserad algoritm
borde fungera bra på en stigande och en sjunkande marknad.
Aktierna delades också upp i tre olika grupper på båda tidsperioderna för att underlätta
träningen och för att jämföra om algoritmen fungera bäst på enstaka aktier eller om
resultaten blir jämt fördelade mellan alla aktier. Flera olika iterationer av modellerna har
gjorts men deras resultat redovisas inte här. Nedan är exempel på testmetologin som har
använts för att analysera modellerna.

Tabellerna 4 - 9 ovan visar resultatet av ett träningspass av MLP modeller. Här
analyseras hur mängden indata (i dagar) och nätverkets storlek påverkar
resultaten. Procenten beskriver hur modellen klarar sig jämfört med en passiv
investerare.

57

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Modellerna som analyserades i tabellerna innehåller inte någon optimal lösning. Resultaten
är relativ stokastiska men detta gav mig en inblick i hur jag ska ändra träningen för att
uppnå bättre resultat. De största problemen i modellerna ovan är att så gott som alla har bra
resultat på antingen den första eller den andra tidsperioden, inte båda. Vidare analys av
dessa resultat visar dock att vidare fortsatt krävs för att uppnå en optimal modell.
Vidare analys visar att det uppkommer två stora problem från modellerna ovan, vilket gör
att jag har två problem jag måste lösa. Problem 1 är att vissa modeller har bestämt sig för
att köpa bara aktien hela tiden, vilket leder till att vinsten över passiv investering är nära 0.
Flera av 64x64 modellerna har detta problem. Nedan finns en visualisering på problem 1.

Graf 1: Visualisering av det första problemet. Aktien i grafen är AVGO, testset 1
framtid. Modell 64x64, 20 dagar historiska data. Algoritmen följer bara
aktiekursen, vilket leder till ingen extra vinst.
Det andra problemet, är att algoritmen inte köper aktien en enda gång. Denna “strategi”
fungerar bra på den i de framtida testsetten eftersom aktiekursen faller mot slutet. Så gott
som alla modellerna som har +30% avkastning på de framtida testsetten hamnar ut för detta.
Nedan finns ett exempel på detta.

58

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Graf 2: Visualisering av det andra problemet. Aktien i grafen är AVGO, testset 1
framtid. Modell 128x128, 30 dagar historiska data. Algoritmen handlar inte en
enda gång. Resultaten ser bra ut men i verkligheten fungerar inte algoritmen.

En optimal modell borde kunna ha positiv avkastning på båda tidsperioderna. Den enda
modellen av de som analyserades ovan som lyckades med detta är 256x256 och 128x128
med 20 dagar historiska data. De har bra resultat på testset 1-3 men på testset 1-3 framtid
lyckas de inte handla optimalt. På framtida testsetten är avkastningen väldigt liten eftersom
den följer dels det första problemet.
För att få en optimal modell har jag ändrat på några variabler som har lett till att modellen
lyckas handla bättre på alla tidsserier och aktier. Jag höjde ℽ ( diskonteringen ) till 0.99 från
0.95 och jag lämnade bort dropout-lagren i mina modeller. Dropout-lagren används för att
minska på chansen i ett övervakat inlärningsproblem att modellen ska lära sig datan utantill.
Dessa lager tycks dock ha en negativ inverkan på resultaten i ett förstärkt
inlärningsproblem. På grund av tidsbrist har jag inte tränat lika många modeller med de
nya inställningarna.

59

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Tabell 10 och 11 ovan visar resultaten av träningen efter de ändringar som
nämndes ovan. Procenten beskriver hur modellen klarar sig jämfört med en passiv
investerare.

Ändringarna i inlärningen har lett till bättre resultat, i alla fall för MLP-modellerna.
256x256 MLP-modellen med 20 och 30 dagar som indata kommer att vara de modeller jag
kommer att testa i min portföljsimulation. Modellerna som använde sig av LSTM och CNN
fungerade inte optimalt. CNN-modellen lärde sig endast att köpa och hålla hela tiden,
därför är skillnaden 0% mellan den och den passiva investeraren. Detta motsvarar det första
problemet från tidigare.

Graf 3: Visualisering LSTM modellen efter förbättringen i inlärningen. Aktien i
grafen är AVGO, testset 1. Modell 256x256 LSTM, 30 dagar historisk data.
60

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Graf 4: Visualisering LSTM modellen efter förbättringen i inlärningen. Aktien i
grafen är AVGO, testset 1 framtid. Modell 256x256 LSTM, 30 dagar historisk data.

LSTM-modellen jag testade tycks inte ha samma problem som modellerna tidigare hade
där de inte handlade en enda gång. Denna försiktiga modell tycks dock endast fungera bra
i sjunkande marknader men dåligt vid stigande marknader eftersom den är lite för försiktig
att handla.

61

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Graferna 5 och 6 visar fördelningen av vinst (och förlust) för enskilda aktier jämfört
med en passiv investering. Båda graferna visualiserar 256x256 MLP-modellen med
30 dagar som indata. Graf 5 är testset 2 och graf 6 är testset 2 framtid. Majoriteten
av aktierna har en positiv avkastning men det finns i båda testset flera aktier som
klarar sig sämre jämfört med en passiv investering. Som bilaga 5 finns en
motsvarande större grafer där varje enskilda akties resultat syns.

Graferna ovan visar att även den bästa modellen, i detta fall 256x256 MLP-modellen med
30 dagar som indata klarar inte av att vinna en passiv investerare i varje aktie. Den slutliga
avkastningen är dock positiv eftersom majoriteten av aktierna klarar sig bättre än en passiv
investering och vinsten av de bästa aktierna är större än förlusten av de värsta.

5.4 Portföljsimulation
I portföljsimulationen är det meningen att simulera hur algoritmen skulle klara sig att
optimera avkastningen av en portfolio. Denna simulation motsvarar mycket det tidigare
testandet men algoritmen körs igenom på en längre tidsperiod. Aktierna delas upp på
samma sätt som tidigare till testset 1-3 men den första halvan och andra halva är slagna
ihop. Detta betyder att t.ex. testset 1 är: Aktierna ABMD - FLT. Tidsperiod: 2017-01-03
till 2020-03-20. I denna simulation används de båda bästa MLP modellerna. De är annars
identiska men den andra får 30 dagar som indata medan den andra får 20 dagar.
Två olika sätt att simulera en portfolio testades. I det första alternativet har varje aktie i
början 10 000 dollar allokerad till den och algoritmen ska bestämma ifall denna 10 000 ska
62

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
användas för att köpa aktien med all dess likvida medel, göra ingenting eller att sälja allting.
Denna samma summa är allokerad till en viss aktie genom hela simulationen. Till exempel,
ifall algoritmen i ett senare skede säljer 15 000 dollar värt av AVGO förblir denna 15 000
allokerad som cash till AVGO tills algoritmen bestämmer sig att köpa aktien igen. Detta
leder dock i teorin till en algoritm som använder sina medel relativ ineffektivt.
I verkligheten skulle en portfölj inte ha en viss summa allokerad för en aktie. Vanligtvis
kan en portföljs medel allokeras hur som helst runt i portföljen. Detta försöker det andra
alternativet lösa. Båda alternativen är dock bundna till de förenklingar som beskrivs på sida
s.60 vilket gör att dessa resultat inte kan jämföras med verklig avkastning.

Tabell 12 över resultaten av portföljsimulation, alternativ 1.

Graf 7, resultaten av portföljsimulation, alternativ 1. Modell 256x256 MLP, 30
dagar indata, testset 1. Slutresultatet var en avkastning på 4,9% mer än en passiv
investering.
I det första alternativet lyckas algoritmen få en positiv avkastning. Detta förväntades
eftersom samma modell fick positiv avkastning i testfasen också och logiken bakom
63

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
köpandet är samma. Modellen är dock långt ifrån optimal. Modellen lyckas få ett litet
försprång jämfört med den passiva investeraren vid dippen som hände i årsskiftet mellan
2018 och 2019, men inget annat speciellt händer.
I alternativ två kan algoritmen handla med alla medel i portföljen, idén bakom detta var att
denna lösning skulle leda till en mer effektiv användning av kapitalet. I simulationen märks
dock algoritmen klarade sig inte emot en passiv investerare. Jag tror att största orsaken till
detta var att i första alternativet kunde algoritmen bättre satsa på de företag som hämtade
de hem den största avkastningen. Problemen med det första alternativet är dock att ifall
algoritmen bestämde sig för att likvidera allting så skulle pengarna bara sitta och göra
ingenting till algoritmen bestämmer sig att köpa samma aktie igen. Det andra alternativet
försöker lösa denna ineffektivitet genom att effektivare investera de oanvända medlen.
I det andra alternativet programmerades algoritmen att jämt dela ut ens medel mellan de
aktier som den ansåg att skulle stiga i värde. Efter att en aktie likviderades när algoritmen
bestämde sig för att sälja allting delas dessa medel jämt ut till de andra aktier som aktien
ansåg att skulle stiga i värde. Detta ledde till en mer balanserad portfölj, men för att vinna
över den passiva investeraren borde den kunna allokera medlen smartare mellan de olika
aktierna. Till exempel borde den kunna satsa mera i aktier som den kommer att ge en högre
avkastning än de andra. I detta fall var dock alla stigande aktier likvärdiga i algoritmens
ögon.
Nedan finns en graf som visualiserar resultaten av det andra alternativet. På grund av de
dåliga resultaten testades alternativ inte med andra modeller. För fortsatta studier anser jag
att den bästa lösningen är en kombination av det första och andra alternativet. En lösning
som använder sig av det första alternativets sätt att ha en högre allokering för de aktier som
hämtar hem den största avkastningen och den andra alternativets sätt att mer effektivt
allokera de oanvända medlen vore en bra kombination.

64

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Graf 8 över resultaten av portföljsimulation, alternativ 2. Modell 256x256 MLP, 30
dagar indata, testset 1. Slutresultatet va en avkastning på -8,6%.

65

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

6. SAMMANFATTNING OCH DISKUSSION
Att förutspå en akties prisutveckling är enligt den allmänt accepterade teorin inte möjligt.
Hypotesen om effektiva marknader anser att marknaden är så effektiv att det inte går att få
högre avkastning på ens investeringar utan att öka risken. Random walk-teorin anser att
aktierna rör sig stokastiskt och att det inte således går att förutspå en akties framtida pris.
Den första forskningsfrågan var: Kan en förstärkt inlärd algoritm lära sig att handla aktier
på ett sätt som ökar avkastningen av aktien jämfört med ens egna prisutveckling på
aktiemarknaden? Svaret på detta utifrån denna avhandling är: ja, i vissa fall. Modellerna i
denna avhandling testades på cirka 400 aktier och tidsperioderna var rätt så snäv. DQNalgoritmen med en MLP lyckades att vinna en passiv investerare i alla sex test set, men
marginalen var dock inte stor. Avhandlingen har gjort flera förenklingar som beskrivs i
kapitel 5.1, en noggrannare simulation som tar i beaktande dessa förenklingar kan komma
till andra slutsatser. Ifall simulationen skulle ta i beaktande slirandet och spridningen
kommer en stor del om inte hela vinsten smälta bort. Jag anser dock att det finns mycket
man kan göra för att förbättra resultaten. Dessa förbättringsförslag behandlas noggrannare
i nästa kapitel.
Avhandlingens andra forskningsfråga var: Kan en förstärkt inlärd algoritm öka
avkastningen av en portfolio jämfört med en passiv investering? Svar på denna fråga är
samma som till den första frågan: ja, i vissa fall. Simulationen gjord i denna avhandling har
lyckats få en högre avkastning på ens investeringar jämfört med en passiv investerare. Dock
eftersom flera förenklingar gjorts till simulationen kan dessa resultat inte jämföras med
verkligheten. Jag anser dock att avhandlingen bevisar att det är under vissa omständigheter
möjligt att öka ens avkastning med hjälp av en förstärkt inlärd algoritm. Ämnet kräver
enligt mig fortsatt forskning för att komma fram till en optimal lösning. Ifall denna positiva
avkastning är tillräckligt hög att det överväger risken av att ha en algoritm att göra ens
handelsbeslut är en svårare fråga.
Det svåraste med att träna modellerna var att hitta en modell som skulle fungera bra på
marknader som stiger och sjunker. Detta var en del av orsaken varför jag valde två olika
tidpunkter för testdata. Den första delen var till en stor del en stigande marknad,
66

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
marknaderna sjönk en aning i slutet av 2018 men annars var trenden uppåt. Den andra delen
innehåller början av den senaste börskraschen där aktiernas värde föll drastiskt från och
med mars 2020. Flera av de modellerna som jag tränade i början fungerade bra på antingen
den första eller den andra delen. På grund av detta har jag kommit till den slutsatsen att den
skulle löna sig att träna flera modeller. Minst två stycken, en som fungerar bra i en stigande
marknad som köper alla små svängningar nedåt och en annan som fungerar bra i sjunkande
marknader som säljer vid svaghet.
En annan stor förbättring vore att träna modeller t.ex. på basen av industrin som de ligger
i. Aktierna i ICT-sektorn har under det senaste decenniet rört sig på ett annat sätt jämfört
med en andra mer etablerade industrier, som t.ex. energi. En ännu bättre lösning kunde vara
att träna en modell endast på en aktie, då skulle modellen lära sig att känna till hur en
specifik aktie rör sig. Ett av målen i denna avhandling var att lära en algoritm att känna
igen generella drag ur alla de 100 aktier som algoritmen tränades på. Resultatet av denna
avhandling visar att ja det finns generella drag som gör att algoritmen kan lära sig, men de
är kanske inte så starka eller allmänna som skulle leda till avsevärt höga vinster.
Jag anser också att LSTM-nätverk borde fungera bättre än MLP i förstärkt inlärda problem
med tidsseriedata, men mitt försök med dessa nätverk fungera inte optimalt. Flera andra
forskare som t.ex. Dai et al. (2019), Azhikodan et al. (2019) och Huang (2019) har använt
LSTM-nätverk med stor framgång.
Koden som denna avhandling använde sig av för att skapa de förstärkt inlärda algoritmerna
är som bilaga i slutet av denna avhandling. För att själv köra igång med att träna en algoritm
behövs dessa filer och alla de Python paket som är i requirements.txt filen. Ovanpå detta
skulle jag rekommendera att installera CUDA och cuDNN paketen från NVIDIA för att
snabba upp träningen med hjälp av ett grafikkort. Historiska aktiedata är bifogat i data
mappen, detta behövs inte laddas ner på nytt.
För att komma igång med träningen ska man bara köra main.py filen. Denna fil innehåller
också de flesta inställningarna för algoritmen. Om man vill ändra på mängden noder/lager
i det neurala nätverket kan detta göras på models.py i create_model funktionen. Under
körningen skriver programmet modellen till models mappen med jämna mellanrum. I
logdata mappen skrivs en loggfil över träningen som kan användas för att visualisera
67

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
resultaten med hjälp av plottrainresults.py. Detta ger en motsvarande graf som finns som
bilaga i denna avhandling.
Ifall man vill testa att träna algoritmen på endast en aktie som jag nämnde tidigare, behöver
man endast ändra i main.py “LIMIT_STOCKS” variabeln till 1. Då kommer algoritmen
endast att tränas på AAPL ( APPLE Inc.).
Jag anser att förstärkt inlärning kommer att hitta flera användningsområden i framtiden. En
stor fördel som förstärkt inlärning har jämfört med övervakad inlärning är att det kräver
inte märkta data. Dock tror jag att då när märkta data är tillgängligt kommer övervakad
inlärning vinna över förstärkt inlärning. Hela fältet av maskininlärning kommer säkert att
växa mycket i framtiden.
Om vi ser bakåt i tiden lyckades oövervakad inlärning återuppliva intresset för
djupinlärning enligt LeCun et al. (2015). Detta har dock sedan överskuggats av
framgångarna i övervakad inlärning. LeCun et al (2015) anser att oövervakad inlärning
kommer att bli mycket viktigare på längre sikt eftersom människornas och djurens inlärning
består främst av oövervakad inlärning, nämligen vi upptäcker världens struktur genom att
observera den. LeCun et al (2015) förväntar sig att mycket av de framtida framstegen inom
datorysyn kommer från system som tränas att kombinera faltnigsnätverks med RNNnätverk som använder förstärkt inlärning för att bestämma var man ska titta. Detta är säkert
inte dock det enda fältet som förstärkt inlärning kommer att användas.

6.1 Fortsatta studier
Jag anser att modellerna som denna avhandling kom fram till är inte de bästa möjliga, men
jag anser att de bevisar att förstärkt inlärning kan användas för att förutspå finansiella
medel. Detta är ens med flera andra studier inom området som t.ex. Deng et al. (2016) och
Huang (2019) som också bevisar att man kan få en positiv avkastning med hjälp av förstärkt
inlärning.

68

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
På grund av tidsbrist blev testandet av olika variabler mindre än först uppskattat. Här har
jag listat flera variabler som vore intressanta att testa:
-

DQN algoritmer med nätverk neurala nätverk som innehåller flera lager. Denna
studie använde sig endast av två dolda lager. Deng et al. (2016) har hittat att det
skulle finnas en korrelation med mera lager och vinstprocenten.

-

Fler noder i de olika lagren. Skulle t.ex. användningen av 512x512 noder öka på
vinsten?

-

Mera träningsdata. Denna studie hade endast 100 aktier på en period av 584
handelsdagar. Totalt blir det cirka 58 000 olika observationer. Det finns lätt
tillgängligt mera data, men dessa modeller tog redan som värst ett dygn att träna.

-

Andra förstärkt inlärda algoritmer. T.ex. Proximal Policy Optimization 2 (PPO2),
Generative Adversarial Imitation Learning (GAIL) och Soft Actor Critic (SAC).
Denna avhandling har inte undersökt ifall dessa kunde passa till avhandlingens
problem.

-

Flera dagar som indata. Studien har använt sig endast av 10-30 dagar som indata.
En forsknings idé kunde vara att ifall man ger flera dagar (50 - 200) kunde
algoritmen kanske bättre fokusera på långsiktiga trender. I avhandlingen testade jag
endast med 10 - 30 dagar som indata.

-

Andra tidsintervaller. En observation kunde vara en timme eller en vecka. Jag
fokuserade mig på daglig data i min avhandling.

-

Fokus på LSTM. Denna avhandling har fokuserat största delen av tiden att optimera
MLP nätverk, men andra studier som t.ex. Azhikodan et al. (2019) och Huang
(2019) har visat att LSTM nätverk fungerar i vissa fall bättre.

-

Andra belöningssätt. Sharpekvoten har använts för att räkna belöningen av flera
andra forskare som t.ex. Berotoluzzo & Corazza (2012). Frekvensen av belöningen
kunde också optimeras vidare.

-

Låt algoritmen blanka. I denna avhandling kunde algoritmen blanka.

Förstärkt inlärning har enligt mig potential att användas som stöd för ens
investeringsbeslut. Jag ser också att detta är en möjlighet att en vidare optimerad förstärkt
inlärd algoritm kan handla aktier självmant. Listan ovan täcker säkert endast en bråkdel av
alla de möjliga variabler som kan testas för att förbättra ens algoritm, men jag anser att
detta är ett bra ställe att börja. Jag rekommenderar också vidare optimering av koden för
69

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
att snabba upp inlärningen. Vissa delar av träningen kan säkert köras parallellt och andra
steg kan förenklas.

70

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

KÄLLFÖRTECKNING
Aase Kim-Georg. (2011). Text Mining of News Articles for Stock Price Prediction.
Magisteruppsats i datavetenskap. Department of Computer and Informations Science,
Norwegian University of Science and Technology.
Azhikodan, A. R., Bhat, A. G., & Jadhav, M. V. (2019). Stock trading bot using deep
reinforcement learning. In Innovations in Computer Science and Engineering (pp. 41-49).
Springer, Singapore.
Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient
descent is difficult. IEEE Trans. Neural Networks 5, 157–166 (1994).
Callan, R. (2003). Artificial Intelligence. New York: Palgrave MacMillan.
Dai, Y., Wang C., Wang I och Xu, Y (2019) Reinforcement Learning for FX trading. Från:
http://stanford.edu/class/msande448/2019/Final_reports/gr2.pdf Hämtad 12.3.2020.
De Grauwe, P. (2010). Animal spirits and monetary policy. Economic Theory, 47(2-3),
pp.423-457.
Deepmind (2019) AlphaStar: Mastering the Real-Time Strategy Game StarCraft II www.
Hämtad (13.4.2020) https://deepmind.com/blog/article/alphastar-mastering-

real-time-

strategy-game-starcraft-ii
Deepmind (2017) AlphaGo: Starting from scratch, www. Hämtad (13.4.2020)
https://deepmind.com/research/case-studies/alphago-the-story-so-far
Deng, Y., Bao, F., Kong, Y., Ren, Z., & Dai, Q. (2016). Deep direct reinforcement learning
for financial signal representation and trading. IEEE transactions on neural networks and
learning systems, 28(3), 653-664.
Du, X., Zhai, J., & Lv, K. (2016). Algorithm trading using q-learning and recurrent
reinforcement learning. positions, 1, 1.
Duda, R. O. & Hart, P. E. Pattern Classification and Scene Analysis (Wiley, 1973)
Egeli, B. (2003). Stock market prediction using artificial neural networks. Decision Support
Systems, 22, 171-185.
71

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

Fama, E. (1965). The Behavior of Stock-Market Prices. The Journal of Business, 38(1),
pp.34.
Fama, E. (1970). Efficient Capital Markets: A Review of Theory and Empirical Work. The
Journal of Finance, 25(2), pp.383.
Fama, E. F., & French, K. R. (2004). The capital asset pricing model: Theory and evidence.
Journal of economic perspectives, 18(3), 25-46.
Gori. 2018. Machine Learning. Morgan Kaufmann.
Hastie, T., Tibshirani, R., Friedman, J., The Elements of Statistical Learning: Data Mining,
Inference, and Prediction (Springer, New York, 2011).
Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735–1780
(1997).
Huang, C. Y. (2018). Financial trading as a game: A deep reinforcement learning approach.
arXiv preprint arXiv:1807.02787.
Jangmin, O., Lee, J., Lee, J. W., & Zhang, B. T. (2006). Adaptive stock trading with
dynamic asset allocation using reinforcement learning. Information Sciences, 176(15),
2121-2147.
Jordan, M. I. and Mitchell, T. (2015). Machine learning: Trends, perspectives, and
prospects. Sci- ence, 349(6245):255–260.
Kansal, S., Martin, B. (2019) Reinforcement Q-Learning from Scratch in Python with
OpenAI Gym. Från: https://www.learndatasci.com/tutorials/reinforcement-q-learningscratch-python-openai-gym/ hämtad 12.2.2020

Khadjeh Nassirtoussi, A., Aghabozorgi, S., Ying Wah, T. and Ngo, D. (2014). Text mining
for market prediction: A systematic review. Expert Systems with Applications, 41(16),
pp.7653-7670.

72

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep
convolutional neural networks. In Advances in neural information processing systems (pp.
1097-1105).
LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521:436–444.
Liu, R., & Zou, J. (2018, October). The effects of memory replay in reinforcement learning.
In 2018 56th Annual Allerton Conference on Communication, Control, and Computing
(Allerton) (pp. 478-485). IEEE.
Lo, A. and MacKinlay, A. (2011). A Non-Random Walk Down Wall Street. Princeton:
Princeton University Press.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,
A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,
Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015).
Human-level control through deep reinforcement learning. Nature, 518(7540):529–533.

Nilsson, H., Isaksson, A., & Martikainen, T. (2002). Företgsvärdering med fundamental
analys. Studentlitteratur.
New York Stock Exchange https://www.nyse.com/market-cap hämtad (13.3.2020)
Powell, N., Foo, S. Y., & Weatherspoon, M. (2008, March). Supervised and unsupervised
methods for stock trend forecasting. In 2008 40th Southeastern Symposium on System
Theory (SSST) (pp. 203-205). IEEE.
Russell, S. J., Norvig, P. & Canny, J. F. (2003). Artificial intelligence: A modern approach.
2nd ed. Upper Saddle River (NJ): Prentice Hall.
Schumaker, R. and Chen, H. (2006). Textual analysis of stock market prediction using
breaking financial news. ACM Transactions on Information Systems, 27(2), pp.1-19.
Schumaker, R. P., & Chen, H. (2009). Textual analysis of stock market prediction using
breaking financial news: The AZFin text system. ACM Transactions on Information
Systems (TOIS), 27(2), 1-19.
73

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... &
Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search.
nature, 529(7587), 484.
Shmueli, G., & Koppius, O. R. (2011). Predictive analytics in information systems
research. MIS quarterly, 553-572.
Sushko, V., & Turner, G. (2018). The implications of passive investing for securities
markets. BIS Quarterly Review, March.
Sutton, R. S. & Barto, A. G. (2018). Reinforcement learning: An introduction. Second
edition. Cambridge (MA): MIT Press.
Trippi, R. R., & DeSieno, D. (1992). Trading equity index futures with a neural network.
Journal of Portfolio Management, 19, 27-27.
Van Otterlo, M., & Wiering, M. (2012). Reinforcement learning and markov decision
processes. In Reinforcement Learning (pp. 3-42). Springer, Berlin, Heidelberg.
Wong, W., Manzur, M. och Chew, B. (2003). How rewarding is technical analysis?
Evidence from Singapore stock market. Applied Financial Economics, 13(7), pp.543-551.

74

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

BILAGOR:
Bilaga 1:
Länk till koden för algoritmen:
https://github.com/MioNok/q-stock-exploring

Bilaga 2:
Komponenterna i datorn som användes för att träna modellerna i avhandlingen
CPU: Ryzen 5 3600
RAM: 16GB 3000 MHz
GPU: MSI GeForce GTX 1660 Super
SSD: Samsung 500gb 970 EVO plus
Moderkort : Asus Prime B450-Plus
PSU: 550W Seasonic
OS: Windows 10
Python 3.6.7

Bilaga 3:
Variabler för den förstärkt inlärda algoritmen samt neurala nätverket.
Variabler:
● n: Mängden dagar av historiska aktiedata som algoritmen får göra sina
beslut på.
● Neurala nätverket: Typen av de olika lagren, mängden av lager samt
mängden noder i varje lager varierar.

75

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
Konstanter:
● Replay memory size, 2500: Mängden tidigare händelser som sparas för
vidare träning. Behandlas i kapitel 3.9
● Min batch size, 64: Lägsta mängden händelser i minnet förrän vi börjar träna
modellerna. Detta fungerar också som storleken av partiet som modellen
tränas på en gång.
● α : Alpha, 0,001. Detta är inlärningshastigheten (eng. learning rate).
● 𝜸 : Gamma, 0.99.

Gamma är diskonteringsfaktorn för algoritmen.

Diskonteringen behandlas noggrannare i kapitel 3.5
● 𝛜: Epsilon, 1-0.05. Epsilon är förknippad med utforskning och exploatering,
högre epsilon leder till en högre grad an utforskning. Detta behandlas i
kapitel 3.7. Epsilon minskar varje episod från 1 tills den är 0.05
● Optimeringen, Adam. Simpel stokastisk lutnings (eng. gradient decent)
algoritm för att optimera neurala nätverket.

Bilaga 4:
Aktierna som hör till S&P 100 indexet i januari 2020:

Symbol

Namn

Symbol Namn

Symbol

Namn

AAPL

Apple Inc.

MDT

Medtronic plc

NFLX

Netflix

ABBV

AbbVie Inc.

MET

MetLife Inc.

NKE

Nike, Inc.

ABT

Abbott Laboratories

MMM

3M Company

NVDA

NVIDIA Corp.

ACN

Accenture

MO

Altria Group

ORCL

Oracle Corporation

ADBE

Adobe Inc.

MRK

Merck & Co.

OXY

Occidental Petroleum
Corp.

AGN

Allergan

MS

Morgan Stanley

PEP

PepsiCo

AIG

American International
Group

MSFT

Microsoft

PFE

Pfizer Inc

76

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

ALL

Allstate

NEE

NextEra Energy

PG

Procter & Gamble Co

AMGN

Amgen Inc.

EMR

Emerson Electric Co.

PM

Philip Morris International

AMZN

Amazon.com

EXC

Exelon

PYPL

PayPal Holdings

AXP

American Express

F

Ford Motor Company

QCOM

Qualcomm Inc.

BA

Boeing Co.

FB

Facebook, Inc.

RTN

Raytheon Co.

BAC

Bank of America Corp

FDX

FedEx

SBUX

Starbucks Corp.

BIIB

Biogen

GD

General Dynamics

SLB

Schlumberger

BK

The Bank of New York
Mellon

GE

General Electric

SO

Southern Company

BKNG

Booking Holdings

GILD

Gilead Sciences

SPG

Simon Property Group,
Inc.

BLK

BlackRock Inc

GM

General Motors

T

AT&T Inc

BMY

Bristol-Myers Squibb

GOOG Alphabet Inc. (Class C)

TGT

Target Corporation

BRK.B

Berkshire Hathaway

GOOG
L
Alphabet Inc. (Class A)

TMO

Thermo Fisher Scientific

C

Citigroup Inc

GS

Goldman Sachs

TXN

Texas Instruments

CAT

Caterpillar Inc.

HD

Home Depot

UNH

UnitedHealth Group

CHTR

Charter Communications

HON

Honeywell

UNP

Union Pacific Corporation

CL

Colgate-Palmolive

IBM

International Business
Machines

UPS

United Parcel Service

CMCSA

Comcast Corp.

INTC

Intel Corp.

USB

U.S. Bancorp

COF

Capital One Financial
Corp.

JNJ

Johnson & Johnson

UTX

United Technologies

COP

ConocoPhillips

JPM

JPMorgan Chase & Co.

V

Visa Inc.

COST

Costco Wholesale Corp.

KHC

Kraft Heinz

VZ

Verizon Communications

CSCO

Cisco Systems

KMI

Kinder Morgan

WBA

Walgreens Boots Alliance

CVS

CVS Health

KO

The Coca-Cola Company

WFC

Wells Fargo

CVX

Chevron Corporation

LLY

Eli Lilly and Company

WMT

Walmart

DD

DuPont de Nemours Inc

LMT

Lockheed Martin

XOM

Exxon Mobil Corp.

77

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

DHR

Danaher Corporation

LOW

Lowe's

DIS

The Walt Disney Company MA

MasterCard Inc

DOW

Dow Inc.

MCD

McDonald's Corp

DUK

Duke Energy

MDLZ

Mondelēz International

Bilaga 3:
Aktier i S&P 500 indexet i januari 2020.

Symbol

Namn

Symbol

Namn

Symbol

Namn

MMM

3M Company

ETR

Entergy Corp.

NWS

News Corp. Class B

ABT

Abbott Laboratories

EOG

EOG Resources

NEE

NextEra Energy

ABBV

AbbVie Inc.

EFX

Equifax Inc.

NLSN

Nielsen Holdings

ABMD

ABIOMED Inc

EQIX

Equinix

NKE

Nike

ACN

Accenture plc

EQR

Equity Residential

NI

NiSource Inc.

ATVI

Activision Blizzard

ESS

Essex Property Trust, Inc. NBL

Noble Energy Inc

ADBE

Adobe Inc.

EL

Estée Lauder Companies JWN

Nordstrom

AMD

Advanced Micro Devices
Inc
EVRG

Evergy

NSC

Norfolk Southern Corp.

AAP

Advance Auto Parts

ES

Eversource Energy

NTRS

Northern Trust Corp.

AES

AES Corp

RE

Everest Re Group Ltd.

NOC

Northrop Grumman

AFL

AFLAC Inc

EXC

Exelon Corp.

NLOK

NortonLifeLock

A

Agilent Technologies Inc EXPE

Expedia Group

NCLH

Norwegian Cruise Line
Holdings

APD

Air Products &
Chemicals Inc

Expeditors

NRG

NRG Energy

AKAM

Akamai Technologies Inc EXR

Extra Space Storage

NUE

Nucor Corp.

ALK

Alaska Air Group Inc

XOM

Exxon Mobil Corp.

NVDA

Nvidia Corporation

ALB

Albemarle Corp

FFIV

F5 Networks

NVR

NVR Inc

EXPD

78

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

ARE

Alexandria Real Estate
Equities

ALXN

FB

Facebook, Inc.

ORLY

O'Reilly Automotive

Alexion Pharmaceuticals FAST

Fastenal Co

OXY

Occidental Petroleum

ALGN

Align Technology

FRT

Federal Realty
Investment Trust

ODFL

Old Dominion Freight
Line

ALLE

Allegion

FDX

FedEx Corporation

OMC

Omnicom Group

AGN

Allergan, plc

FIS

Fidelity National
Information Services

OKE

ONEOK

ADS

Alliance Data Systems

FITB

Fifth Third Bancorp

ORCL

Oracle Corp.

LNT

Alliant Energy Corp

FE

FirstEnergy Corp

PCAR

PACCAR Inc.

ALL

Allstate Corp

FRC

First Republic Bank

PKG

Packaging Corporation of
America

GOOGL

Alphabet Inc Class A

FISV

Fiserv Inc

PH

Parker-Hannifin

GOOG

Alphabet Inc Class C

FLT

FleetCor Technologies
Inc

PAYX

Paychex Inc.

MO

Altria Group Inc

FLIR

FLIR Systems

PAYC

Paycom

AMZN

Amazon.com Inc.

FLS

Flowserve Corporation

PYPL

PayPal

AMCR

Amcor plc

FMC

FMC Corporation

PNR

Pentair plc

AEE

Ameren Corp

F

Ford Motor Company

PBCT

People's United Financial

AAL

American Airlines Group FTNT

Fortinet

PEP

PepsiCo Inc.

AEP

American Electric Power FTV

Fortive Corp

PKI

PerkinElmer

AXP

American Express Co

FBHS

Fortune Brands Home &
Security

PRGO

Perrigo

AIG

American International
Group

FOXA

Fox Corporation Class A PFE

Pfizer Inc.

T

AT&T

FOX

Fox Corporation Class B PM

Philip Morris
International

AMT

American Tower Corp.

BEN

Franklin Resources

PSX

Phillips 66

AWK

American Water Works
Company Inc

FCX

Freeport-McMoRan Inc.

PNW

Pinnacle West Capital

AMP

Ameriprise Financial

GPS

Gap Inc.

PXD

Pioneer Natural
Resources

ABC

AmerisourceBergen Corp GRMN

Garmin Ltd.

PNC

PNC Financial Services

AME

AMETEK Inc.

IT

Gartner Inc

PPG

PPG Industries

AMGN

Amgen Inc.

GD

General Dynamics

PPL

PPL Corp.

79

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
APH

Amphenol Corp

GE

General Electric

PFG

Principal Financial Group

ADI

Analog Devices, Inc.

GIS

General Mills

PG

Procter & Gamble

ANSS

ANSYS

GM

General Motors

PGR

Progressive Corp.

ANTM

Anthem

GPC

Genuine Parts

PLD

Prologis

AON

Aon plc

GILD

Gilead Sciences

PRU

Prudential Financial

AOS

A.O. Smith Corp

GL

Globe Life Inc.

PEG

Public Serv. Enterprise
Inc.

APA

Apache Corporation

GPN

Global Payments Inc.

PSA

Public Storage

AIV

Apartment Investment &
Management
GS

Goldman Sachs Group

PHM

PulteGroup

AAPL

Apple Inc.

GWW

Grainger (W.W.) Inc.

PVH

PVH Corp.

AMAT

Applied Materials Inc.

HRB

H&R Block

QRVO

Qorvo

APTV

Aptiv PLC

HAL

Halliburton Co.

PWR

Quanta Services Inc.

ADM

Archer-Daniels-Midland
Co

HBI

Hanesbrands Inc

QCOM

QUALCOMM Inc.

ARNC

Arconic Inc.

HOG

Harley-Davidson

DGX

Quest Diagnostics

ANET

Arista Networks

HIG

Hartford Financial
Svc.Gp.

RL

Ralph Lauren
Corporation

AJG

Arthur J. Gallagher & Co. HAS

Hasbro Inc.

RJF

Raymond James
Financial Inc.

AIZ

Assurant

HCA

HCA Healthcare

RTN

Raytheon Co.

ATO

Atmos Energy Corp

PEAK

Healthpeak Properties

O

Realty Income
Corporation

ADSK

Autodesk Inc.

HP

Helmerich & Payne

REG

Regency Centers
Corporation

ADP

Automatic Data
Processing

HSIC

Henry Schein

REGN

Regeneron
Pharmaceuticals

AZO

AutoZone Inc

HSY

The Hershey Company

RF

Regions Financial Corp.

AVB

AvalonBay Communities,
Inc.
HES

Hess Corporation

RSG

Republic Services Inc

AVY

Avery Dennison Corp

HPE

Hewlett Packard
Enterprise

RMD

ResMed

BKR

Baker Hughes Co

HLT

Hilton Worldwide
Holdings Inc

RHI

Robert Half International

BLL

Ball Corp

HFC

HollyFrontier Corp

ROK

Rockwell Automation
Inc.

BAC

Bank of America Corp

HOLX

Hologic

ROL

Rollins Inc.

BK

The Bank of New York
Mellon Corp.

HD

Home Depot

ROP

Roper Technologies

BAX

Baxter International Inc.

HON

Honeywell Int'l Inc.

ROST

Ross Stores

BDX

Becton Dickinson

HRL

Hormel Foods Corp.

RCL

Royal Caribbean Cruises
Ltd

BRK.B

Berkshire Hathaway

HST

Host Hotels & Resorts

SPGI

S&P Global, Inc.

80

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
BBY

Best Buy Co. Inc.

HPQ

HP Inc.

CRM

Salesforce.com

BIIB

Biogen Inc.

HUM

Humana Inc.

SBAC

SBA Communications

BLK

BlackRock

HBAN

Huntington Bancshares

SLB

Schlumberger Ltd.

BA

Boeing Company

HII

Huntington Ingalls
Industries

STX

Seagate Technology

BKNG

Booking Holdings Inc

IEX

IDEX Corporation

SEE

Sealed Air

BWA

BorgWarner

IDXX

IDEXX Laboratories

SRE

Sempra Energy

BXP

Boston Properties

INFO

IHS Markit Ltd.

NOW

ServiceNow

BSX

Boston Scientific

ITW

Illinois Tool Works

SHW

Sherwin-Williams

BMY

Bristol-Myers Squibb

ILMN

Illumina Inc

SPG

Simon Property Group
Inc

AVGO

Broadcom Inc.

INCY

Incyte

SWKS

Skyworks Solutions

BR

Broadridge Financial
Solutions

IR

Ingersoll Rand

SLG

SL Green Realty

BF.B

Brown-Forman Corp.

INTC

Intel Corp.

SNA

Snap-on

CHRW

C. H. Robinson
Worldwide

ICE

Intercontinental
Exchange

SO

Southern Co.

COG

Cabot Oil & Gas

IBM

International Business
Machines

LUV

Southwest Airlines

CDNS

Cadence Design Systems IP

International Paper

SWK

Stanley Black & Decker

CPB

Campbell Soup

IPG

Interpublic Group

SBUX

Starbucks Corp.

COF

Capital One Financial

IFF

Intl Flavors & Fragrances STT

State Street Corp.

CPRI

Capri Holdings

INTU

Intuit Inc.

STE

STERIS plc

CAH

Cardinal Health Inc.

ISRG

Intuitive Surgical Inc.

SYK

Stryker Corp.

KMX

Carmax Inc

IVZ

Invesco Ltd.

SIVB

SVB Financial

CCL

Carnival Corp.

IPGP

IPG Photonics Corp.

SYF

Synchrony Financial

CAT

Caterpillar Inc.

IQV

IQVIA Holdings Inc.

SNPS

Synopsys Inc.

CBOE

Cboe Global Markets

IRM

Iron Mountain
Incorporated

SYY

Sysco Corp.

CBRE

CBRE Group

JKHY

Jack Henry & Associates TMUS

T-Mobile US

CDW

CDW

J

Jacobs Engineering
Group

TROW

T. Rowe Price Group

CE

Celanese

JBHT

J. B. Hunt Transport
Services

TTWO

Take-Two Interactive

CNC

Centene Corporation

SJM

JM Smucker

TPR

Tapestry, Inc.

CNP

CenterPoint Energy

JNJ

Johnson & Johnson

TGT

Target Corp.

CTL

CenturyLink Inc

JCI

Johnson Controls
International

TEL

TE Connectivity Ltd.

CERN

Cerner

JPM

JPMorgan Chase & Co.

FTI

TechnipFMC

CF

CF Industries Holdings
Inc

JNPR

Juniper Networks

TFX

Teleflex

81

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

SCHW

Charles Schwab
Corporation

CHTR

KSU

Kansas City Southern

TXN

Texas Instruments

Charter Communications K

Kellogg Co.

TXT

Textron Inc.

CVX

Chevron Corp.

KEY

KeyCorp

TMO

Thermo Fisher Scientific

CMG

Chipotle Mexican Grill

KEYS

Keysight Technologies

TIF

Tiffany & Co.

CB

Chubb Limited

KMB

Kimberly-Clark

TJX

TJX Companies Inc.

CHD

Church & Dwight

KIM

Kimco Realty

TSCO

Tractor Supply Company

CI

CIGNA Corp.

KMI

Kinder Morgan

TT

Trane Technologies plc

CINF

Cincinnati Financial

KLAC

KLA Corporation

TDG

TransDigm Group

CTAS

Cintas Corporation

KSS

Kohl's Corp.

TRV

The Travelers Companies
Inc.

CSCO

Cisco Systems

KHC

Kraft Heinz Co

TFC

Truist Financial

C

Citigroup Inc.

KR

Kroger Co.

TWTR

Twitter, Inc.

CFG

Citizens Financial Group LB

L Brands Inc.

TSN

Tyson Foods

CTXS

Citrix Systems

LHX

L3Harris Technologies

UDR

UDR, Inc.

CLX

The Clorox Company

LH

Laboratory Corp. of
America Holding

ULTA

Ulta Beauty

CME

CME Group Inc.

LRCX

Lam Research

USB

U.S. Bancorp

CMS

CMS Energy

LW

Lamb Weston Holdings
Inc

UAA

Under Armour Class A

KO

Coca-Cola Company

LVS

Las Vegas Sands

UA

Under Armour Class C

CTSH

Cognizant Technology
Solutions

LEG

Leggett & Platt

UNP

Union Pacific Corp

CL

Colgate-Palmolive

LDOS

Leidos Holdings

UAL

United Airlines Holdings

CMCSA

Comcast Corp.

LEN

Lennar Corp.

UNH

United Health Group Inc.

CMA

Comerica Inc.

LLY

Lilly (Eli) & Co.

UPS

United Parcel Service

CAG

Conagra Brands

LNC

Lincoln National

URI

United Rentals, Inc.

CXO

Concho Resources

LIN

Linde plc

UTX

United Technologies

COP

ConocoPhillips

LYV

Live Nation
Entertainment

UHS

Universal Health
Services, Inc.

ED

Consolidated Edison

LKQ

LKQ Corporation

UNM

Unum Group

STZ

Constellation Brands

LMT

Lockheed Martin Corp.

VFC

V.F. Corp.

COO

The Cooper Companies

L

Loews Corp.

VLO

Valero Energy

CPRT

Copart Inc

LOW

Lowe's Cos.

VAR

Varian Medical Systems

GLW

Corning Inc.

LYB

LyondellBasell

VTR

Ventas Inc

CTVA

Corteva

MTB

M&T Bank Corp.

VRSN

Verisign Inc.

COST

Costco Wholesale Corp.

M

Macy's Inc.

VRSK

Verisk Analytics

COTY

Coty, Inc

MRO

Marathon Oil Corp.

VZ

Verizon Communications

82

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

CCI

Crown Castle
International Corp.

MPC

Marathon Petroleum

VRTX

Vertex Pharmaceuticals
Inc

CSX

CSX Corp.

MKTX

MarketAxess

VIAC

ViacomCBS

CMI

Cummins Inc.

MAR

Marriott Int'l.

V

Visa Inc.

CVS

CVS Health

MMC

Marsh & McLennan

VNO

Vornado Realty Trust

DHI

D. R. Horton

MLM

Martin Marietta Materials VMC

Vulcan Materials

DHR

Danaher Corp.

MAS

Masco Corp.

WRB

W. R. Berkley
Corporation

DRI

Darden Restaurants

MA

Mastercard Inc.

WAB

Wabtec Corporation

DVA

DaVita Inc.

MKC

McCormick & Co.

WMT

Walmart

DE

Deere & Co.

MXIM

Maxim Integrated
Products Inc

WBA

Walgreens Boots
Alliance

DAL

Delta Air Lines Inc.

MCD

McDonald's Corp.

DIS

The Walt Disney
Company

XRAY

Dentsply Sirona

MCK

McKesson Corp.

WM

Waste Management Inc.

DVN

Devon Energy

MDT

Medtronic plc

WAT

Waters Corporation

FANG

Diamondback Energy

MRK

Merck & Co.

WEC

Wec Energy Group Inc

DLR

Digital Realty Trust Inc

MET

MetLife Inc.

WFC

Wells Fargo

DFS

Discover Financial
Services

MTD

Mettler Toledo

WELL

Welltower Inc.

DISCA

Discovery Inc. Class A

MGM

MGM Resorts
International

WDC

Western Digital

DISCK

Discovery Inc. Class C

MCHP

Microchip Technology

WU

Western Union Co

DISH

Dish Network

MU

Micron Technology

WRK

WestRock

DG

Dollar General

MSFT

Microsoft Corp.

WY

Weyerhaeuser

DLTR

Dollar Tree

MAA

Mid-America Apartments WHR

Whirlpool Corp.

D

Dominion Energy

MHK

Mohawk Industries

WMB

Williams Cos.

DOV

Dover Corp.

TAP

Molson Coors Brewing
Company

WLTW

Willis Towers Watson

DOW

Dow Inc.

MDLZ

Mondelez International

WYNN

Wynn Resorts Ltd

DTE

DTE Energy Co.

MNST

Monster Beverage

XEL

Xcel Energy Inc

DUK

Duke Energy

MCO

Moody's Corp

XRX

Xerox

DRE

Duke Realty Corp

MS

Morgan Stanley

XLNX

Xilinx

DD

DuPont de Nemours Inc

MOS

The Mosaic Company

XYL

Xylem Inc.

DXC

DXC Technology

MSI

Motorola Solutions Inc.

YUM

Yum! Brands Inc

ETFC

E*Trade

MSCI

MSCI Inc

ZBRA

Zebra Technologies

EMN

Eastman Chemical

MYL

Mylan N.V.

ZBH

Zimmer Biomet Holdings

ETN

Eaton Corporation

NDAQ

Nasdaq, Inc.

ZION

Zions Bancorp

EBAY

eBay Inc.

NOV

National Oilwell Varco
Inc.

ZTS

Zoetis

83

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning
ECL

Ecolab Inc.

NTAP

NetApp

EIX

Edison Int'l

NFLX

Netflix Inc.

EW

Edwards Lifesciences

NWL

Newell Brands

EA

Electronic Arts

NEM

Newmont Corporation

EMR

Emerson Electric
Company

NWSA

News Corp. Class A

Bilaga 5:
Graferna visar fördelningen av skillnaden mellan aktier jämfört med en passiv
investering. Båda graferna använder modellen 256x256 MLP med 30 dagar som
indata. Den första är testset 2 framtid och den andra är testset 2.

84

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

85

M. Nokelainen: Aktiv investering med hjälp av förstärkt inlärning

86

