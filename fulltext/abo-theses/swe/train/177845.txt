Mika Nokelainen

Aktiv investering med hj√§lp
av f√∂rst√§rkt inl√§rning
-

Ett test av algoritmisk handlande

Pro gradu-avhandling informationssystem
Handledare: Markku Heikkil√§ och Jozef Mezei
Fakulteten f√∂r samh√§llsvetenskaper och ekonomi
√Öbo Akademi
√Öbo 2020

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

INNEH√ÖLLSF√ñRTECKNING
1. INLEDNING
1.1. PROBLEM
1.2. SYFTE OCH AVGR√ÑNSNINGAR
1.3. DISPOSITION
2. METOD
2.1 METOD I H√ÑNSYN TILL TEORIN
2.3 METODVAL
2.3 DATA
3. MASKININL√ÑRNINGSMETODER
3.1. √ñVERVAKAD INL√ÑRNING
3.2. O√ñVERVAKAD INL√ÑRNING
3.3. F√ñRST√ÑRKT INL√ÑRNING
3.3.1 Grunderna till f√∂rst√§rkt inl√§rning
3.3.2 Markov-beslutsprocess
3.3.3 M√•l och bel√∂ningar
3.3.4 Episoder
3.3.5 Utforskning och exploatering
3.3.6 Erfarenhetsuppspelning
3.4. Q-INL√ÑRNING
3.4.2 Q-inl√§rning exempel
3.5. DJUPINL√ÑRNING
3.5.1 Neurala n√§tverk
3.5.2 Faltningsn√§tverk
3.5.3 Recurrent neural network
3.5.4 Djup f√∂rst√§rkt inl√§rning
4. V√ÑRDERINGEN AV FINANSIELLA MEDEL MED HJ√ÑLP AV
MASKININL√ÑRNINGSMETODER

1
2
2
3
4
4
5
6
8
10
12
13
14
17
19
20
21
23
24
26
30
32
34
37
37
40

4.1. GRUNDER I AKTIEV√ÑRDERING
4.1.1 Teknisk analys
4.1.2 Fundamental analys
4.1.3 Hypotesen om effektiva marknader
4.1.4. Random walk-hypotesen
4.2. HANDLING MED FINANSIELLA INSTRUMENT MED HJ√ÑLP AV F√ñRST√ÑRKT INL√ÑRNING

40
40
43
45
46
47

5. PORTFOLIO OPTIMERING OCH AKTIEHANDEL MED HJ√ÑLP AV F√ñRST√ÑRKT
INL√ÑRNING

53

5.1 AKTIE HANDLINGS BESLUT MED DJUP Q-INL√ÑRNING
5.3 TR√ÑNING OCH TEST
5.4 PORTF√ñLJSIMULATION
6. SAMMANFATTNING OCH DISKUSSION
6.1 FORTSATTA STUDIER

54
56
62
66
68

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
K√ÑLLF√ñRTECKNING

71

BILAGOR:

75

F√∂rkortningar
MLP: Multilayer perceptron
CNN: Convolutional neural network
LSTM: Long short-term memory
DQN: Deep Q-network
AI: Artificial Intelligence
RNN: Recurrent neural network
RSI: Relative strength index
PEG: Price/earnings to growth ratio
AVGO: Aktiesymbol f√∂r Broadcom Inc
S&P 500: Standard and Poors 500 index
MDP: Markov decision process

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Abstrakt
√Ñmne: Informationssystem
F√∂rfattare: Mika Nokelainen
Titel: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Handledare: Markku Heikkil√§ och Jozef Mezei
Abstrakt:
Att investera passivt har visat sig vara ett effektivt s√§tt att investera i aktier. Passiv
investering inneb√§r att f√∂lja en buy-and-hold-investeringsstrategi d√§r man minimerar
aktiv handel av aktier. Motsatsen till detta vore att investera aktivt, d√§r man tillvaratar
kortsiktiga prisf√∂r√§ndringar f√∂r att f√• en h√∂gre avkastning j√§mf√∂rt med marknaden.
Hypotesen om effektiva marknader anser dock att det inte √§r m√∂jligt att f√• h√∂gre
avkastning p√• ens investeringar utan att √∂ka risken med investeringar.
Kan maskininl√§rning, n√§rmare best√§mt f√∂rst√§rkt inl√§rning, vinna √∂ver en passiv
investerare genom att aktivt handla aktier? Syftet med avhandlingen √§r svara p√• denna
fr√•ga. En djupt f√∂rst√§rkt inl√§rd algoritm programmerades i Python f√∂r att handla aktier.
Aktierna som var i fokus i denna avhandling h√∂r till S&P 500-indexet. Avhandlingen
fokuserade sig p√• tv√• olika variabler i de modeller som testas:
‚óè M√§ngden historiska data som algoritmen f√•r f√∂r att fatta handelsbesluten
‚óè M√§ngden noder i neurala n√§tverket i den djupt f√∂rst√§rkta inl√§rda algoritmen.
Modellerna testades p√• dedikerade testdata enskilt och som helhet i en portf√∂lj.
Resultaten av testandet var relativt stokastiska, men avhandlingen lyckades visa att en
f√∂rst√§rkt inl√§rd algoritm kan under specifika omst√§ndigheter l√§ra sig generella drag av
historiska aktiedata som hj√§lper den att aktivt handla aktier.
Nyckelord: Maskininl√§rning, f√∂rst√§rkt inl√§rning, aktiehandel, aktiv investering
Datum: 11.4.2020
Sidor: 90

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

1. INLEDNING
I k√§rnan av v√•r ekonomi finns den finansiella marknaden, och aktiemarknaden √§r en del av
detta. Aktiemarknadens grunduppgift √§r att hj√§lpa f√∂retag samla kapital till deras
verksamhet. P√• den allm√§nna aktiemarknaden kan vem som helst k√∂pa och s√§lja aktier
vilket g√∂r att dem till extremt likvid form av varor vilket i sig leder till att flera av de
noterade f√∂retagens aktier handlas dagligen med h√∂g volym.
New York Stock Exchange √§r v√§rldens st√∂rsta aktiemarknad, och den har ett sammanlagt
marknadskapital p√• cirka 28 biljoner dollar (NYSE, 6/2018). En hel del aktier k√∂ps och
s√§ljs p√• aktiemarknader runtom i v√§rlden varje dag de √§r √∂ppna. Att investera i
aktiemarknaden anses √∂verlag var ett bra s√§tt att placera sina besparingar, det kr√§ver dock
att investeraren √§r medveten om riskerna som f√∂ljer med att investera i aktier. Aktiernas
v√§rde √§r direkt kopplat till hur mycket n√•gon √§r redo att k√∂pa aktien f√∂r. Vid l√•gkonjunktur
brukar aktiernas v√§rde sjunka, men de har hittills alltid √•terh√§mtat sig. Undantag fr√•n detta
√§r f√∂retag som inte klarar sig genom l√•gkonjunkturen, det √§r allts√• m√∂jligt att mista sitt
investerade kapital helt och h√•llet.
Aktier anses vara relativt volatila j√§mf√∂rt med andra v√§rdepapper. Till exempel obligationer
brukar vara ett alternativ f√∂r dem som √§r riskaverta. Obligationer har mycket l√§gre
volatilitet √§n aktiemarknaden. Historiskt sett har dock aktiemarknaden haft en betydligt
h√∂gre avkastning p√• investerat kapital √§n obligationer; risk och avkastning g√•r ofta hand i
hand.
Datorer har anv√§nts redan l√§nge f√∂r att handla aktier. NASDAQ, som √∂ppnade 1971, √§r i
dagens l√§ge v√§rldens n√§st st√∂rsta aktiemarknad. NASDAQ var ocks√• v√§rldens f√∂rsta
elektroniska aktiemarknad. Eftersom aktier i dagens l√§ge kan handlas p√• datorer har det
m√∂jliggjort anv√§ndningen av algoritmisk handel av aktier. Detta har lett till att
maskininl√§rning har b√∂rjats anv√§nda i handlingen av aktier. (Tripp & DeSieno 1992)
Maskininl√§rning har gjort flera framsteg under det senaste decenniet. En f√∂rst√§rkt inl√§rd
algoritm har redan vunnit de b√§sta spelarna i Go (Alphago) och Starcraft 2 (Alphastar).
Krizhevsky et al. (2012) anv√§nde faltningsn√§tverk f√∂r k√§nna igen 1000 olika objekt i
miljontals olika bilder. Mnih et al. (2015) fortsatte p√• detta arbete genom att koppla en
1

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
f√∂rst√§rkt inl√§rd algoritm med ett faltningsn√§tverk f√∂r att l√§ra en maskin att spela Atari 2600spel med revolutionerande resultat.
Vad h√§nder ifall man skulle kombinera aktiemarknaden med den senaste forskningen inom
maskininl√§rning? M√•let med denna studie √§r att unders√∂ka om det g√•r att anv√§nda f√∂rst√§rkt
inl√§rning f√∂r att f√∂rutsp√• en akties framtida prisutveckling med hj√§lp av historisk aktiedata.

1.1. Problem
Enligt hypotesen om effektiva marknader och random walk-hypotesen √§r det inte m√∂jligt
att f√∂rutsp√• hur aktiepriserna kommer att r√∂ra sig, och det √§r inte m√∂jligt att f√• st√∂rre
avkastning j√§mf√∂rt med marknaden utan att √∂ka risken p√• investeringarna. Det antas att all
information om marknaden och de f√∂retag som den h√∂r till √§r tillg√§ngligt f√∂r alla.
Aktiekurserna anpassar sig genast n√§r ny information kommer ut, t.ex. nyheter om f√∂retaget
eller en kvartalsrapport. Ifall det √§r m√∂jligt att f√∂rutsp√• en akties prisutveckling, betyder det
att hypotesen om effektiva marknader samt random walk-hypotesen √§r i alla fall till en del
ogiltiga.
Algoritmen fattar handelsbesluten baserat p√• n antal historiska aktiedata, vid en viss
tidpunkt. Under tr√§ningsfasen har algoritmen sett tusentals olika scenarion med samma
antal n historiska aktiedata. Utifr√•n vad den l√§rt sig under tr√§ningen f√∂rs√∂ker algoritmen
hitta generella drag ur dessa scenarion som hj√§lper algoritmen fatta r√§tt handelsbeslut vid
scenario den inte sett f√∂rr. Eftersom algoritmen ska endast fatta beslut p√• basen en
historiska aktiedata, liknar detta mycket teknisk analys. Om algoritmen lyckas hitta
generella drag fr√•n historiska aktiedata som hj√§lper den fatta bra handelsbeslut, betyder det
att teknisk analys fungerar i alla fall dels.

1.2. Syfte och avgr√§nsningar
Syftet med denna studie √§r att unders√∂ka om det g√•r att f√∂rutsp√• aktiemarknaden med hj√§lp
av olika f√∂rst√§rkta inl√§rda algoritmer. Den f√∂rsta forskningsfr√•gan √§r: Kan en f√∂rst√§rkt
inl√§rd algoritm l√§ra sig att handla aktier p√• ett s√§tt som √∂kar avkastningen av aktien
j√§mf√∂rt med aktiens egna prisutveckling p√• aktiemarknaden? Med andra ord: kan man f√•
2

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
h√∂gre avkastning j√§mf√∂rt med att passivt investera i en aktie genom att aktivt handla aktien?
Med utg√•ngspunkt i aktiens tidigare prisutveckling blir fr√•gan d√•: kommer aktien att g√•
upp eller ner? Detta √§r ett klassifikationsproblem. Utifr√•n av den givna data ska algoritmen
fatta ett handels beslut. Denna studie kommer att inneh√•lla en simulation d√§r dessa
algoritmer anv√§nds f√∂r att aktivt handla aktier. Aktierna som avhandlingen fokuserar p√•
finns i S&P 100 och S&P 500 som inneh√•ller de 100 respektive 500 st√∂rsta f√∂retagen p√•
den amerikanska aktiemarknaden enligt deras marknadsv√§rde.
Avhandlingen kommer ocks√• att f√∂rs√∂ka simulera en portfolio vars aktier handlas baserat
p√• beslut som g√∂rs av algoritmen. Den andra forskningsfr√•gan √§r: Kan en f√∂rst√§rkt inl√§rd
algoritm √∂ka avkastningen av en portfolio j√§mf√∂rt med en passiv investering? Den f√∂rsta
forskningsfr√•gan har enskilda aktier i fokus medan den andra forskningsfr√•gan fokuserar
p√• en portf√∂lj med flera aktier.
Avhandlingen kommer att fokusera sig p√• algoritmer som anv√§nder djupa q-n√§tverk (DQN)
som √§r Q-inl√§rning kombinerat med neurala n√§tverk. Simulationen kommer att
programmeras i Python.

1.3. Disposition
Kapitel 2 kommer att behandla de forskningsmetoder som anv√§nds och vilka data som
kommer anv√§nds. Kapitel 3 behandlar olika maskininl√§rningsmetoder. √ñvervakad och
o√∂vervakad inl√§rning behandlas kort medan mer vikt l√§ggs p√• f√∂rst√§rkt inl√§rning och
djupinl√§rning. Med tanke p√• om att avhandlingen handlar om aktiehandel ger kapitel 4 en
√∂verblick √∂ver hur man v√§rdes√§tter aktier samt lite teori om aktiehandel. Kapitel 4.2
redovisar tidigare forskning om √∂ver f√∂rst√§rkt inl√§rda algoritmer som handlar olika
finansiella instrument. I kapitel 5 behandlas modellerna och simulationen som gjorts f√∂r
denna avhandling i detalj. I kapitel 6 sammanfattas avhandlingen och fortsatt forskning
inom √§mnet diskuteras med personliga √•sikter sammanfattar avhandling och personliga
√•sikter om fortsatt forskning inom √§mnet.

3

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

2. METOD
En djupt f√∂rst√§rkt inl√§rd algoritm som f√∂rutsp√•r en akties prisutveckling programmeras i
denna avhandlingen med hj√§lp av Python. Data som anv√§nds √§r historiska aktiedata av S&P
100 och S&P 500-aktier. Algoritmens f√∂rm√•ga att f√∂rutsp√• en akties prisutveckling testas
med hj√§lp av dedikerade testdata.

2.1 Metod i h√§nsyn till teorin
J√§rvinen (2012) lyfter upp Jenkins (1985) modell om forskningsmetoder. Denna modell har
anv√§nts som riktlinje f√∂r avhandlingen, och f√∂r att svara p√• avhandlingens
forskningsfr√•gor.

Modellen

har

ocks√•

anv√§nts

f√∂r

att

iterera

och

bearbeta

maskininl√§rningsmodellerna i kapitel 5.
1. Idea
2. Library research
3. Research topic
4. Research strategy
5. Experimental design
6. Data capture
7. Data analysis
8. Publish results
Jenkins modell ur J√§rvinen (2012)
Denna avhandling grundar sig p√• id√©n att maskininl√§rning kan anv√§ndas f√∂r att f√∂rb√§ttra
avkastningen av aktieinvesteringar. Flera avhandlingar har anv√§nt sig av √∂vervakad och
o√∂vervakad inl√§rning f√∂r att f√∂rutsp√• aktier. Till dessa h√∂r till exempel Powell et al. (2008),
Egeli (2003) och Shumaker & Chen (2009). Det finns ocks√• en del forskning som anv√§nder
sig av f√∂rst√§rkt inl√§rning f√∂r att f√∂rutsp√• den framtida prisutvecklingen av olika finansiella
medel, som till exempel valutapar och olika index, men f√• forskningar har aktier i fokus.
Denna avhandling kommer att fokusera sig p√• att √∂ka avkastningen av ens investeringar
med hj√§lp av f√∂rst√§rk inl√§rning j√§mf√∂rt med en passiv investerare. Att investera passivt har
visat sig att vara ett effektivt s√§tt att investera i aktier. Passiv investering inneb√§r att f√∂lja
4

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
en buy-and-hold-investeringsstrategi d√§r man minimerar aktiv handel av aktier. Mer om
passivt investerande kan l√§sas fr√•n Sushko & Turner (2018).
J√§rvinen (2012) h√§vdar att n√§r man empiriskt studerar framtiden och de f√∂rflutna kan vi
anv√§nda theory testing-modeller. Denna avhandling faller inom omr√•det av theory testingmodeller p√• J√§rvinens taxonomi om forskning metoder. F√∂r att svara p√• avhandlingens
forskningsfr√•gor anv√§nds kontrollerade experiment. Kontrollerade experiment √§r en metod
som J√§rvinen (2012) n√§mner att man kan anv√§nda f√∂r att till exempel j√§mf√∂ra tv√•
sorteringsalgoritmer. I detta fall j√§mf√∂rs dock flera maskininl√§rningsmodeller med
varandra.
J√§rvinen (2012) n√§mner att kontrollerade experiment anv√§nder sig av beroende och
oberoende

variabler.

Oberoende

variabler

kommer

i

detta

falla

vara

programmeringsspr√•ket, k√∂r milj√∂n, indata och flera andra konstanter f√∂r neurala n√§tverket
som finns uppr√§knade i bilaga 3. De variabler som g√∂r att modellerna i slutskede skiljer
sig fr√•n varan √§r neurala n√§tverkets typ / storlek och m√§ngden indata.
I slutet analyseras hur effektiva maskininl√§rning modellerna √§r j√§mf√∂rt med en passiv
investerare.
Test metodologin som anv√§nds baserar sig p√• Shmueli & Koppious (2011) forskning d√§r
de beskriver empiriska modeller som kan anv√§ndas f√∂r prognoser. Denna modell √§r menad
f√∂r algoritmer som √§r designade f√∂r att f√∂rutsp√• framtida h√§ndelser, i denna avhandling
skulle detta vara en akties framtida prisutveckling. Shmueli & Koppious (2011) anser att
det g√•r att testa hur p√•litlig modellen √§r p√• framtida observationer eller p√• andra
observationer som inte var i testdata. Denna avhandling kommer att testa modellera p√• b√•da
alternativen.

2.3 Metodval
Valet av f√∂rst√§rkt inl√§rning som metod f√∂r avhandlingen baserar sig p√• de senaste
framg√•ngarna inom √§mnet. Bland annat Mnih et al. (2015) med djupa Q-n√§tverk och
5

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
framg√•ngarna av Alphastar och AlphaGo som √§r gjorda av Deepmind (2017 & 2019) har
lyft f√∂rst√§rkt inl√§rning fram med sina respektive f√∂rb√§ttringar.
St√∂rsta f√∂rdelen med f√∂rst√§rkt inl√§rning j√§mf√∂rt med √∂vervakad inl√§rning √§r att de kr√§ver
inte m√§rkta data utan algoritmen l√§r sig genom att r√∂ra sig fram i en milj√∂. Till skillnad fr√•n
√∂vervakad inl√§rning l√§mpar sig f√∂rst√§rkt inl√§rning speciellt d√• n√§r man vill att algoritmen
ska l√§ra sig en kedja av h√§ndelser.
Python valdes som verktyg f√∂r att skapa denna algoritm p√• grund av f√§rdiga kodpaket som
√§r √§gnade f√∂r maskininl√§rning och datahantering som t.ex. Keras, Tensorflow, Pandas och
Numpy. Dessa paket m√∂jligg√∂r en effektiv utveckling av algoritmen. R skulle ocks√• ha
varit ett bra alternativ men jag valde Python p√• grund av att jag har mera erfarenhet av det.
F√∂r att f√• en optimal algoritm testas flera olika variabler som har med inl√§rningen att g√∂ra.
Vissa av variablerna har ett konstant v√§rde, dessa v√§rden √§r tagna ur tidigare forskning,
t.ex. diskonteringen och optimiseraren √§r fr√•n Huang (2019). Flera av de tidigare
f√∂rb√§ttringarna som tidigare forskare inom √§mnet har hittar har anv√§nts i denna avhandling.
Till exempel Huang (2019) har inspirerat det s√§tt som algoritm delar ut bel√∂ningen samt
storleken av uppspelningsminnet. Deng et al. (2016) bevisade att r√•a data kan anv√§ndas f√∂r
att tr√§na algoritmen, inga tekniska indikatorer beh√∂vs. Dessa ovann√§mnda forskningar och
till exempel Azhikodan et al. (2019) har inspirerat mig att pr√∂va p√• hur LSTM-n√§tverk
fungerar i den simulation jag byggt. Neurala n√§tverkets storlek i fr√•ga vid m√§ngden lager
och noder per lager √§r en variabel som kommer att p√•verka resultatet. Deng et al. (2016)
har forskat i detta och deras resultat har inspirerat valet av m√§ngden lager och noder.

2.3 Data
Historiska aktiedata av S&P 100 aktier har anv√§nts f√∂r att tr√§na algoritmen. En lista p√• de
aktier som h√∂r till S&P 100 indexet n√§r avhandlingen finns som bilaga. Algoritmen har
blivit tr√§nad p√• historiska data fr√•n januari 2017 till maj 2019. Dess framg√•ng testas p√•
S&P 500 aktier som inte tillh√∂r till S&P 100. Testdata inneh√•ller observationer fr√•n januari
2017 till mars 2020. Detta ger en inblick i hur algoritmen fungerar p√• data som den inte

6

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
sett f√∂rut. Orsaken till att jag valde S&P 100 och S&P 500 aktier √§r att dessa data √§r l√§tt
tillg√§nglig. Dessa data √§r h√§mtad fr√•n alphavantage.co.
Aktiedata √§r p√• daglig niv√•. Detta ger oss fem olika variabler som kommer att fungera som
indata till algoritmen:
-

Open: Priset p√• aktien n√§r marknaden √∂ppnade

-

High: H√∂gsta priset som aktien handlades med p√• en viss dag

-

Low: L√§gsta priset som aktien handlades med p√• en viss dag

-

Close: Priset p√• aktien n√§r marknaden st√§ngde

-

Volume: M√§ngden aktier som handlades p√• den specifika dagen.

Hypotesen √§r att med hj√§lp av dessa data skulle algoritmen kunna l√§ra sig f√∂rutsp√• den
framtida prisutvecklingen av aktien. Ett antagande √§r att dessa data r√§cker f√∂r att g√∂ra en
simpel tekniska analys p√• aktien, kapitel 4.1.1 beskriver detta noggrannare.

7

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

3. MASKININL√ÑRNINGSMETODER
Varf√∂r ska maskinerna l√§ra sig n√•got, r√§cker det inte med att de l√∂ser ett specifikt problem
som de √§r programmerade att l√∂sa? Ofta r√§cker det, men i flera fall √§r det inte rimligt. I
st√§llet f√∂r att formulera l√∂sningar till ett problem bildas algoritmer vars uppgift √§r att l√§ra
sig att l√∂sa problemet.
Hur kan maskiner l√§ra sig automatiskt genom ens erfarenheter? Detta √§r fr√•gan som Jordan
och Mitchell (2015) anser att forskning inom maskininl√§rning f√∂rs√∂ker svara p√•. Detta √§r i
dagens l√§ge ett av de f√§lt inom datavetenskap som v√§xer snabbast. Fast√§n sj√§lva teorin √§r
r√§tt s√• gammal har maskininl√§rning blivit aktuellt igen p√• 2000-talet. Orsaken √§r att nya
inl√§rningsalgoritmen har tillg√•ng till stora m√§ngder data tack vare internet och f√∂rm√•nlig
h√•rdvara f√•r att g√∂ra de kr√§vda kalkylationerna.
Under de senaste decennierna har maskininl√§rning gjort stora framsteg enligt Jordan och
Mitchell (2015). Det b√∂rjade med sm√• experiment i laboratorier som idag har evolverat och
hittat flera kommersiella problem som har kunnat l√∂sas med hj√§lp av maskininl√§rning.
Inom artificiell intelligens (AI) har maskininl√§rning framst√•tt som den metod som valts f√∂r
att utveckla praktisk programvara f√∂r datorsyn, taligenk√§nning, naturlig spr√•kbehandling
och robotstyrning. M√•nga utvecklare av AI-system inser nu att f√∂r flera problem kan det
vara mycket enklare att tr√§na ett system genom att visa algoritmen ett exempel p√• √∂nskat
inputëÅãoutput-beteende, √§n att programmera det manuellt genom att f√∂rutse √∂nskat svar
f√∂r alla m√∂jliga output.
Ett inl√§rningsproblem kan enligt Jordan och Mitchel (2015) vara till exempel att k√§nna igen
kreditkortsbedr√§geri. Algoritmens uppgift vore d√• att f√• en korttransaktion som input, som
sedan ska klassificeras som antingen bedr√§geri eller icke-bedr√§geri. F√∂r att tr√§na upp
algoritmen beh√∂vs det mycket data om tidigare transaktioner som √§r klassificerade som
bedr√§geri eller icke-bedr√§geri. Detta kallas f√∂r m√§rkta data. Maskininl√§rnings algoritmen
√∂var sig p√• en del av dessa data som kallas f√∂r tr√§ningsdata. Efter √∂vningen testas hur bra
algoritmen fungerar p√• data som den inte sett f√∂rr. Detta kallas kallas testdata.

I

kreditkortsbedr√§geriexemplet kan algoritmen l√§ras att undvika falska negativa resultat,
exempelvis transaktioner d√§r bedr√§geri klassificeras fel som icke-bedr√§geri.
8

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Med hj√§lp av historiska data kan man redan i dagens l√§ge anv√§nda sig av maskininl√§rning
i flera olika branscher. Jordan och Mitchel (2015) n√§mner att man kan till exempel f√∂rutsp√•
vilka mediciner som passar b√§st f√∂r vilka patienter, trafikdata kan anv√§ndas f√∂r att f√∂rutsp√•
rusningar och historiska data √∂ver brott kan anv√§ndas f√∂r att skicka poliser till r√§tta omr√•den
vid ett visst klockslag.
Gori (2018) presenterar ett exempel i sin bok Machine learning, ett problem som kan
anv√§ndas f√∂r att exemplifiera behovet av maskininl√§rning. I detta exempel f√∂rs√∂ker en
maskin l√§ras att k√§nna igen bokst√§ver som √§r skrivna f√∂r hand.

Figur 1. Gori(2018)
Denna bild ska representera en inskannad tv√•a i en 8x8-matris. Hur ska vi l√§ra maskinen
att denna matris ska klassificeras som en tv√•a? F√∂rs konverteras matrisen till en str√§ng med
64 bittar:
0001100000100100000000100000001000000010100001000111110000000011
Str√§ngen l√§ggs till en tabell och den klassificeras som en tv√•a. Problemet skulle d√• bara blir
att leta upp samma sifferkombination i en tabell f√∂r att hitta vad bokstaven ska representera.
Gori (2018) p√•pekar att detta √§r dock tyv√§rr inte rimligt eftersom vi borde ha en rad tabellen
f√∂r alla m√∂jliga kombinationer som en 8x8-tabell kan ha. F√∂r att r√§kna ut hur m√•nga
m√∂jligheter som kan finnas kan utg√• fr√•n hur m√•nga m√∂jliga v√§rden en cell kan ha i fakultet
med hur m√•nga celler som finns. Det blir 2^64, 18446744073709551616 olika m√∂jliga
kombinationer. De vore orimligt att beh√∂va spara en tabell med alla dessa v√§rden bara f√∂rt
att kunna klassificera handskrivna bokst√§ver. Ifall resolutionen p√• bilden √§r √§nnu st√∂rre,
t.ex. 10x10, blir siffran f√∂r m√∂jliga kombinationer redan 2^100.

9

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Ett annat exempel som Gori (2018) tar upp √§r n√§r ett program ska kunna urskilja ord fr√•n
varandra i ett ljudklipp. Detta kan till exempel basera sig p√• en tr√∂skel d√§r en tillr√§cklig
s√§nkning av tonh√∂jden skulle betyda slutet av ett ord och b√∂rjan av ett annat. Exempelvis
yttrandet‚Äúcomputers are attacking the secret of intelligence‚Äù kunde enligt Gori uppfattas
av ett program som: com / pu / tersarea / tta / ckingthesecre / tofin / telligence.
Enligt Gori (2018) √§r ljudsignalen n√§stan noll f√∂re ljuden p, t och k. Ett annat problem √§r
de sammansatta orden, ljudsignalen f√∂rsvagas ofta i mitten av orden vilket skulle leda till
att tr√∂skel underskrids och n√§sta ljud uppfattas som ett nytt ord. En maskininl√§rd algoritm
kan enligt Gori (2018) med tillr√§cklig mycket tr√§ningsdata l√§ra sig att inte s√§rskilja p√• ord
vid p, t och k.
Detta kapitel kommer att redog√∂ra √∂ver olika s√§tt vi kan l√§ra maskiner att l√∂sa olika
problem. Generellt brukas maskininl√§rningsalgoritmer delas i tre olika kategorier enligt
Gori(2018) √§r dessa:
1. √ñvervakat l√§rande. Anv√§nds d√• det finns klart definierad indata och utdata.
Programmet ska kunna l√§ra sig generaliserade regler utifr√•n den givna data f√∂r att
l√∂sa det aktuella problemet.
2. O√∂vervakat l√§rande. H√§r finns det bara indata och programmet ska med hj√§lp av
dessa lyckas l√§ra sig de underliggande strukturerna.
3. F√∂rst√§rkt inl√§rning. En agent l√§r sig genom interaktioner. Genom upprepade f√∂rs√∂k
inom en milj√∂ l√§ra agenten att agera p√• ett visst s√§tt i en viss situation.
Dessa olika s√§tt kan l√∂sa olika problem som till exempel klassifikation, regression och
klustring. I de kommande kapitlen kommer dessa f√∂rklaras noggrannare. Mera vikt
kommer att l√§ggas p√• f√∂rst√§rkt inl√§rning som behandlas i kapitel 3.3.

3.1. √ñvervakad inl√§rning
De vanligaste maskininl√§rningsmetoderna √§r enligt Mitchell och Jordan (2015) √∂vervakade
inl√§rningsmetoder.

Dessa kan inneh√•lla till exempel klassifikations problem d√§r

algoritmen ska fungera som ett spamfilter i ens mailinkorg, k√§nna igen ansikten i bilder
eller att hj√§lpa diagnosticera patienter. Grundprincipen med √∂vervakad inl√§rning √§r att
10

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
tr√§ningsdata inneh√•ller (x, y) d√§r x √§r data som prediktionen ska g√∂ras p√• och y √§r det
f√∂rv√§ntade svaret. M√•let √§r att producera en prediktion y* med hj√§lp av x*.
Input x kan till exempel vara en vektor som inneh√•ller flera variabler eller mer komplexa
dokument som bilder eller grafer. Likvis kan algoritmens output ocks√• ta flera former. Den
vanligaste formen √§r en bin√§r klassifikation d√§r svaret blir antingen 0 eller 1, dessa siffror
representerar tv√• olika svar, som till exempel spam och icke spam. Ofta r√§cker dock inte
bin√§r klassifikation, d√§rf√∂r har mycket forskning gjorts p√• klassifikations problem med
flera m√∂jliga kategorier som svar, samt problem d√§r ett svar kan vara en kombination av
flera kategorier.
√ñvervakade inl√§rningssystem formar √∂verlag ens prediktioner med hj√§lp av inl√§rd
kartl√§ggning av f(x). Det finns flera olika s√§tt att kartl√§gga funktionen f , Jordan och
Mitchell (2015) n√§mner n√•gra exempel, dessa √§r beslutstr√§d, logistisk regression,
st√∂dvektor maskiner och neurala n√§tverk.
Russel och Norvig (2003) tar upp ett exempel d√§r en √∂vervakad algoritm ska l√§ra sig om
den ska bromsa eller inte med hj√§lp av bilder som √§r tagna fr√•n en bil. Detta √§r allts√• ett
bin√§rt klassifikations problem. F√∂r varje bild som ges till algoritmen ska den l√§ra sig att
k√§nna igen om det finns ett person i n√§rheten som √§r i risk att bli √∂verk√∂rd av bilen om den
inte bromsar. Haiste et al. (2011) beskriver att en √∂vervakad inl√§rnings agent har en ‚Äúl√§rare‚Äù
som ber√§ttar algoritmen om den prediktion den gjort √§r r√§tt eller fel. Russell och Norvigs
(2003) exempel med bilen kan ocks√• blir ett regressions problem ifall vi vill ha som output
hur l√•ngt borta personen √§r fr√•n bilen.
LeCun et al. (2015) h√§vdar att den vanligaste formen av maskininl√§rning, djupt eller inte,
√§r √∂vervakad inl√§rning. LeCun et al. (2015) tar upp ett exempel d√§r ett system byggs vars
uppgift √§r att klassificera bilder som kan inneh√•lla ett hus, en bil, en person eller ett husdjur.
F√∂rst samlas en stor m√§ngd av bilder av hus, bilar, m√§nniskor och husdjur. Varje bild m√•ste
vara m√§rkt med ens respektive kategori. Under tr√§ningen visas maskinen en bild, maskinen
producerar en output i form av en vektor av po√§ng, en f√∂r varje kategori. M√•let √§r att den
√∂nskade kategorin f√•r h√∂gsta po√§ngen. Kategorin med h√∂gsta po√§ng anses vara maskinens
prediktion.

11

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
En del av √∂vervakad inl√§rning som har sett stor framg√•ng under de senaste √•ren √§r neurala
n√§tverk. Neurala n√§tverk anv√§nder sig av flera olika lager av noder och vikter som
inneh√•ller tr√∂skelv√§rden. Jordan och Mitchell (2015) h√§vdar att varenda nod anv√§nder sig
av simpla funktioner f√∂r ens input och output. Djupinl√§rning, som neurala n√§tverk h√∂r till,
anv√§nder sig av en gradient nedstigning (eng. gradient decent) f√∂r att √§ndra v√§rden p√•
vikterna mellan noderna. Dessa vikter best√§mmer hur stark l√§nken √§r mellan noderna.
Jordan och Mitchell (2015) p√•pekar att djupinl√§rning kan utnyttja moderna parallella
databehandlingsarkitekturer, s√•som grafikbehandlingsenheter (GPU) som ursprungligen
utvecklats f√∂r videospel. Med hj√§lp av dessa √§r det m√∂jligt att bygga djupa inl√§rningssystem
som inneh√•ller miljarder variabler som kan tr√§nas p√• stora samlingarna av bilder, videor,
och talprov som √§r tillg√§ngliga p√• internet. Djupinl√§rningssystem har haft en stor inverkan
p√• forskningen under de senaste √•ren inom datorsyn (eng. computer vision) och
taligenk√§nning (eng. speech recognition). Djupinl√§rningssystem har producerat stora
m√§tbara framsteg j√§mf√∂rt med tidigare inl√§rning algoritmer som anv√§nds f√∂r att l√∂sa samma
problem. Djupinl√§rning behandlas noggrannare i kapitel 3.5.

3.2. O√∂vervakad inl√§rning
Haiste et al. (2011) beskriver o√∂vervakad inl√§rning som ‚Äúinl√§rning utan en l√§rare‚Äù. I
√∂vervakad inl√§rning finns det en ‚Äúl√§rare‚Äù som konstant ber√§ttar algoritmen om den gjort ett
r√§tt eller fel beslut. I o√∂vervakad inl√§rning finns inte denna l√§rare utan algoritmen ska sj√§lv
l√§ra sig att finna ett samband i de givna data. Haiste et al. (2011) skriver att det √§r inte
m√∂jligt att direkt m√§ta framg√•ngen av o√∂vervakad inl√§rning, de anser att man m√•ste
anv√§nda sig av heurisktiska metoder f√∂r att bed√∂ma kvaliteten av resultaten. En o√∂vervakad
inl√§rningsagent kommer aldrig att l√§ra sig vad den ska g√∂ra f√∂r att den vet inte vad som √§r
en √∂nskad eller o√∂nskad output enligt Russel och Norvig (2003). Den f√∂ljer bara de givna
reglerna.
Jordan och Mitchell (2015) skriver att o√∂vervakad inl√§rning involverar i stort sett en analys
av om√§rkt eller okategoriserade data med hj√§lp av antaganden g√§llande de strukturella
egenskaper f√∂r data (t.ex. algebraisk, kombinatorisk eller probabilistisk). Till exempel kan
12

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
man anta att data ligger p√• ett l√•gdimensionellt f√∂reningsr√∂r (eng. manifold), o√∂vervakade
inl√§rningens uppgift √§r s√•ledes att identifiera detta f√∂reningsr√∂r uttryckligen ur de givna
data. Det finns ocks√• flera metoder f√∂r reduktion av dimensioner som till exempel
principalkomponentanalys,

m√•ngfaldsinl√§rning,

faktoranalys

och

slumpm√§ssiga

projektioner g√∂r som olika specifika antaganden om det underliggande f√∂reningsr√∂ret.
Jordan och Mitchell (2015) forts√§tter med att ta upp klusteranalys som ett annat exempel
av o√∂vervakad inl√§rning. Klusteranalys anv√§nds f√∂r att dela upp data i olika kluster i
fr√•nvaro av explicita etiketter som indikerar en uppdelning. Ett brett spektrum av
klusteranalys algoritmer har utvecklats, dessa √§r baserade p√• specifika antaganden om arten
av ett kluster.

3.3. F√∂rst√§rkt inl√§rning
Jordan och Mitchell (2015) och Du et al. (2016) h√§vdar att till skillnad fr√•n √∂vervakad
inl√§rning, d√§r en algoritm l√§r sig om ens output √§r r√§tt eller fel, spj√§lkas problemet upp i
f√∂rst√§rkt inl√§rning i flera handlingar som leder till en output. I f√∂rst√§rkt inl√§rning l√§r sig
algoritmen de b√§sta handlingarna vid ett givet tillst√•nd med hj√§lp av bel√∂ningar. Till
skillnad fr√•n √∂vervakad inl√§rning vet algoritmen inte genast om den fattat ett bra eller d√•ligt
beslut. I slutet av en episod, som √§r en samling av tillst√•nd och handlingar, kan den dock
v√§rdera om den valda kedjan av handlingar gav en h√∂g bel√∂ning eller inte.
Om en maskin ska l√§ras att spela schack kan man enligt Russell och Norvig (2003) anv√§nda
sig av √∂vervakad inl√§rning. Maskinen kan bli given en lista p√• alla olika stadier som spelet
kan befinna sig i och sedan ge den b√§sta l√∂sningen till detta stadie. Att kartl√§gga alla m√∂jliga
tillst√•nd som spelknapparna kan befinna sig i och best√§mma en vettig handling √§r dock inte
en enkel uppgift. Finns det en b√§ttre l√∂sning om dessa data inte √§r tillg√§ngligt? F√∂r att l√∂sa
problemet kan en algoritm anv√§ndas som spelar spelet och f√∂rs√∂ker att vinna genom
upprepade f√∂rs√∂k och misstag. Detta kallas med andra ord f√∂r f√∂rst√§rkt inl√§rning.

Callan (2003) ber√§ttar att tillst√•nd √§r den indata som agenten f√•r. Tillst√•ndet beskriver till
agenten hur milj√∂n d√§r den befinner sig ser ur. P√• basen av dessa tillst√•nd ska agenten fatta
beslut om vilken handling som √§r den b√§sta. Denna koppling mellan tillst√•nd ger algoritmen
13

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
m√∂jligheten att l√§ra sig vilken kedja av handlingar leder till den h√∂gsta m√∂jliga bel√∂ning.
F√∂rst√§rkt inl√§rning handlar allts√• om att l√§ra sig genom interaktioner. Mer specifikt √§r det
en agent i en milj√∂ enligt Callan (2003). Agentens uppgift √§r att g√∂ra ett eller flera
handlingar f√∂r varje tillst√•nd i milj√∂n. Efter att agenten gjort en handling ger milj√∂n agenten
en bel√∂ning. Om handlingen som agenten gjorde √§r f√∂rv√§ntad, f√•r den en stor bel√∂ning f√∂r
att fr√§mja den att v√§lja samma handling i framtiden. Om handlingen var icke f√∂rv√§ntad √§r
bel√∂ningen negativ. M√•let blir s√•ledes att maximera bel√∂ningen.
Russell och Norvig (2003) skriver att n√§r agenten m√§rker att en viss kedja av handlingar
leder till en h√∂g bel√∂ning f√∂rst√§rker den denna kedja i hopp om att den i framtiden ocks√•
f√•r en h√∂g bel√∂ning fr√•n denna kedja av handlingar. I b√∂rjan √§r dess handlingar
slumpm√§ssiga. Agenten b√∂rjar s√•ledes med att r√∂ra sig i milj√∂n utan att veta utkomsten av
olika handlingar. Detta leder till att i b√∂rjan blir det agentens uppgift att utforska milj√∂n f√∂r
att f√• en bild av handlingsalternativen och milj√∂n. Varje handling i varje tillst√•nd lagras
och det f√∂rv√§ntade beteendet √∂vas in. Sakta men s√§kert kommer agenten att l√§ra sig att
undvika samma misstag f√∂r att komma fram till en optimal l√∂sning.
Sutton och Barto (2018) ber√§ttar att ut√∂ver agenten och milj√∂n kan man identifiera fyra
grundprinciper i f√∂rst√§rkningssystemet: en policy, en bel√∂ning, en v√§rdefunktion och
m√∂jligen en modell av milj√∂n. Dessa grundprinciper behandlas noggrannare i f√∂ljande
kapitel.

3.3.1 Grunderna till f√∂rst√§rkt inl√§rning
En policy definierar enligt Sutton och Barto (2018) inl√§rningsagentens s√§tt att bete sig vid
en viss tidpunkt. √ñverlag √§r en policy en kartl√§ggning fr√•n upplevda milj√∂tillst√•nd till
√•tg√§rder som ska vidtas n√§r agenten befinner sig i dessa tillst√•nd. I vissa fall kan policyn
vara en enkel funktion eller uppslagstabell, medan i andra fall kan det inneb√§ra omfattande
ber√§kningar. Policyn √§r i k√§rnan av en f√∂rst√§rkt inl√§rd agent i den meningen att den kan
ensam best√§mma beteendet av agenten. I allm√§nhet √§r policyn stokastiska i b√∂rjan men
efter att agenten √§r tr√§nad kan den med st√∂rre sannolikhet s√§ga vilken handling leder till
den h√∂gsta bel√∂ningen.

14

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
En bel√∂ning definierar m√•let f√∂r ett f√∂rst√§rkt inl√§rningsproblem. Sutton och Barto (2018)
forts√§tter med att ber√§tta att vid varje steg skickar milj√∂n agenten en siffra som kallas en
bel√∂ning. Agentens enda m√•l √§r att maximera den totala bel√∂ningen den f√•r fr√•n en episod
av milj√∂n. Bel√∂ningen definierar s√•ledes vad som √§r de goda och d√•liga handlingarna f√∂r
agenten. I ett biologiskt system kan vi t√§nka p√• bel√∂ningar som upplevelserna av n√∂je eller
sm√§rta. De √§r de omedelbara och med hj√§lp av dem avg√∂r agenten ifall handlingen ska
upprepas. Bel√∂ningen √§r den prim√§ra grunden f√∂r att √§ndra policyn. Ifall en handling valt
av policyn f√∂ljs av l√•g bel√∂ning, kan policyn √§ndras till att v√§lja n√•gon annan handling i det
samma tillst√•ndet i framtiden vilket hoppeligen ger en h√∂gre bel√∂ning. Detta upprepas tills
den b√§sta l√∂sningen i tillst√•ndet i fr√•gan hittas.

Sutton och Barto (2018) anser att medan bel√∂ningen indikerar vad som √§r bra och d√•liga
handlingar omedelbart, √§r det v√§rdefunktionen som specificerar vilken kedja av handlingar
som √§r bra p√• en l√•ng sikt. Bel√∂ningar best√§mmer det omedelbara v√§rdet av tillst√•ndet,
v√§rdefunktionen tar i beaktande den slutliga bel√∂ningen av kedjan av handlingar. Agenten
v√§ljer allts√• inte handlingar endast utifr√•n bel√∂ningen av den n√§sta handlingen utan p√• basis
av den totala bel√∂ningen av hela kedjan av handlingar. Till exempel kan ett tillst√•nd alltid
ge en l√•g bel√∂ning men fortfarande ha ett h√∂gt v√§rde eftersom det f√∂ljs regelbundet av andra
tillst√•nd med h√∂ga bel√∂ningar. F√∂r att g√∂ra en m√§nsklig analogi √§r bel√∂ningar n√•got som
n√∂je (om det √§r h√∂gt) och sm√§rta (om l√•gt), medan v√§rdefunktionen ger ett v√§rde som
motsvarar en mer f√∂rfinad och framsynt bed√∂mning av hur n√∂jda eller missn√∂jda vi √§r i
framtiden p√• grund av handlingen.
Uppskattningen av v√§rdet av ett tillst√•nd i f√∂rst√§rknings inl√§rnings algoritmen, √§r utan
tvekan enligt Sutton och Barto (2018) det viktigaste som har l√§rts om f√∂rst√§rkt inl√§rning
under senaste decennierna. Utan bel√∂ningar kunde det inte finnas n√•got v√§rde f√∂r ett
tillst√•nd. Det enda syftet med att uppskatta v√§rden √§r att uppn√• en st√∂rre bel√∂ning.
Handlingsval g√∂rs baserat p√• v√§rderingsbed√∂mningar. Vi s√∂ker handlingar som f√∂ljs av
tillst√•nd med h√∂gsta v√§rde, inte h√∂gsta bel√∂ning, eftersom dessa √•tg√§rder f√•r den st√∂rsta
bel√∂ningen f√∂r oss p√• l√•ng sikt. Bel√∂ningar ges direkt av milj√∂n, men v√§rden m√•ste
uppskattas med hj√§lp av att pr√∂va sig fram. Faktum √§r att den viktigaste komponenten i
n√§stan alla f√∂rst√§rknings inl√§rnings algoritmer √§r att uppskatta v√§rden av den framtida
tillst√•nden.
15

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

F√∂rst√§rkt inl√§rning anv√§nder tillst√•nd och bel√∂ningar som indata till policyn och
v√§rdefunktionen. Tillst√•ndet √§r en signal som beskriver hur milj√∂n ser ut till agenten. Mer
grundligt best√§ms detta av Markovs beslutsprocess (eng. Markov decision process) som
kommer behandlas senare i avhandlingen.
F√∂r att f√• en b√§ttre bild √∂ver hur f√∂rst√§rkt inl√§rning fungerar anv√§nder Sutton och Barto
(2018) sig av luffarschack som ett exempel.

Figur 2. Sutton och Barto (2018)
Figur 2 beskriver en simpel sekvens av r√∂relser i ett spel av luffarschack. Position a √§r
starttillst√•ndet. Motst√•ndaren g√∂r sin tur och tillst√•ndet flyttar till b. Nu √∂verv√§ger
algoritmen dess m√∂jliga handlingar, de streckade linjerna √§r olika handlingar som
algoritmen √∂verv√§ger men handlingen som leder till tillst√•nd c har historiskt sett haft den
h√∂gsta bel√∂ningen i slutet. Detta leder till att algoritmen v√§ljer att r√∂ra sig till tillst√•nd c.
Efter att motst√•ndaren har gjort sin tur befinner vi oss i tillst√•nd d. I detta tillst√•nd har
handlingen som leder till e* haft den st√∂rsta bel√∂ningen p√• l√•ngt sikt men algoritmen
best√§mmer sig f√∂r att g√∂ra en stokastisk handling med att v√§lja e ist√§llet. Dessa handlingar
kallas f√∂r utforskande handlingar. Med hj√§lp av utforskande handlingar letar algoritmen
efter b√§ttre l√∂sningar. Det √§r m√∂jligt att e leder till slut till en h√∂gre bel√∂ningen √§n om
algoritmen valt e*. Om denna utforskande handling ledde till en h√∂g bel√∂ning kommer den
att v√§ljas oftare i samma tillst√•nd, men om bel√∂ningen √§r l√•g kommer den att inte att v√§ljas
senare i samma tillst√•nd.

16

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
√Ñven om detta exempel illustrerar ett m√∂jligt anv√§ndningsomr√•de av i f√∂rst√§rkt inl√§rning,
kan det ge intrycket att f√∂rst√§rkt inl√§rning √§r mer begr√§nsat √§n vad det egentligen √§r. Sutto
och Barto (2018) h√§vdar dock att √§ven om luffarschack √§r ett spel mellan tv√• personer, kan
f√∂rst√§rkt inl√§rning ocks√• anv√§ndas f√∂r problem d√§r det inte finns n√•gon extern motst√•ndare.
F√∂rst√§rkt inl√§rning √§r inte heller begr√§nsat till problem d√§r beteendet delas upp i separata
avsnitt, som de separata spelen luffarschack. Det √§r lika till√§mpligt n√§r beteendet forts√§tter
p√• obest√§md tid och n√§r bel√∂ningar av olika storlekar kan erh√•llas n√§r som helst. Allm√§nna
principen inom f√∂rst√§rkt inl√§rning kan ocks√• anv√§ndas i tidskontinuerliga problem.
Modell-fira system beh√∂ver inte n√•gon modell av milj√∂n √∂verhuvudtaget. Modell-fria
system t√§nker inte enligt Sutton och Barto (2018) hur enskilda handlingar p√•verkar milj√∂n.
Luffarschack problemet √§r dels ett modellfritt problem, spelaren har ingen modell √∂ver
motst√•ndaren. Modellerna m√•ste vara relativt p√•litliga f√∂r att vara till n√•gon nytta, modellfria metoder har en f√∂rdel √∂ver metoder med en modell om det blir f√∂r komplicerat att
beskriva milj√∂n till agenten. D√• blir det l√§ttast om agenten sj√§lv l√§r sig hur milj√∂n ser ut
genom de olika handlingar och tillst√•nd.

3.3.2 Markov-beslutsprocess
Sutton och Barto (2018) beskriver att en Markov-beslutsprocess (eng. Markov decision
process) √§r processen som f√∂rs√∂ker l√∂sa ett problem i f√∂rst√§rkt inl√§rda algoritmer. Denna
process inneh√•ller bearbetandet av den indata den f√•r f√∂r att v√§lja en handling de i olika
situationerna. Markov-beslutsprocess √§r en klassisk formalisering av sekventiellt
beslutsfattande, d√§r handlingar inte bara p√•verkar den omedelbara bel√∂ningen utan ocks√•
p√•f√∂ljande tillst√•nd eftersom den tar i beaktande framtida bel√∂ningar. Detta betyder att
denna beslutsprocess kan √∂verv√§ga framtida bel√∂ningar ist√§llet f√∂r att s√∂ka efter kortsiktiga
bel√∂ningar.
Markovs beslutsprocess anv√§nds enligt Sutton och Barto (2018) f√∂r att l√∂sa ett problem
med av att l√§ra algoritmen en kedja av handlingar f√∂r att uppn√• ett m√•l. Den del som l√§r sig
kallas f√∂r agenten och delen utanf√∂r agenten kallas f√∂r milj√∂n. Dessa tv√• delar agerar med
varann hela tiden, agenten g√∂r en handling och milj√∂n ger tillbaka ett nytt tillst√•nd. Milj√∂n
17

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
ger ocks√• tillbaka en bel√∂ning f√∂r handlingen. Som det tidigare n√§mndes spelar inte
bel√∂ningen av en enskild handling en stor inverkan utan m√•let √§r att maximera bel√∂ningen
p√• l√•ng sikt genom de olika handlingarna.

Figur 3. Interaktionen mellan agenten och milj√∂n i en Markov-beslutsprocess, ur
Sutton och Barto (2018)
Vid varje steg f√•r agenten en bel√∂ning, R i figur 3, som best√§ms av milj√∂n. Till exempel en
ov√§ntad handling kan ge -25 som bel√∂ning och en f√∂rv√§ntad handling kan ge 10. I en √§ndlig
Markov-beslutsprocess finns det en √§ndlig m√§ngd olika tillst√•nd som kan finnas, i qinl√§rning √§r alla dessa m√∂jliga tillst√•nd och olika handlingar f√∂r dessa tillst√•nd sparade i en
tabell som uppdateras periodiskt.
Sutton och Barto (2018) beskriver Markov-beslutsprocess som ett abstrakt och flexibelt
ramverk som kan till√§mpas p√• m√•nga olika problem p√• m√•nga olika s√§tt. Tidsstegen
beh√∂ver till exempel inte h√§nvisa till fasta intervaller, de kan h√§nvisa p√• varandra
p√•f√∂ljande steg i beslutsfattande processen. Handlingen som algoritmen fattar kan vara p√•
en l√•g niv√•, till exempel att byta sp√§nningar som appliceras p√• en robots motorer, eller
beslut p√• en h√∂g niv√•, till exempel om man ska √§ta lunch eller skippa lunchen. P√• liknande
s√§tt den kan indata som algoritmen f√•r beskriva tillst√•ndet ta en m√§ngd olika former. Den
kan best√§mma p√• en l√•g niv√•, som direkt avl√§sningen p√• ett v√§rde av en viss sensor. Den
kan ocks√• vara p√• en h√∂g niv√•, som till exempel symboliska beskrivningar av objekt i ett
rum.
Till exempel kan vissa √•tg√§rder best√§mma vad en agent v√§ljer att t√§nka p√• eller var den
fokuserar sin uppm√§rksamhet p√•. I allm√§nhet kan handlingar vara vilket beslut som helst
man vill l√§ra agenten att g√∂ra och tillst√•nden ska inneh√•lla allt man kan t√§nka p√• att kan
vara till nytta f√∂r agenten f√∂r att g√∂ra den f√∂rv√§ntade handlingen.
18

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Gr√§nsen mellan agenten och milj√∂n √§r vanligtvis inte densamma som den fysiska gr√§nsen
f√∂r en robot eller djurens kropp. Till exempel b√∂r motorerna till en robot och dess mjukvara
betraktas som delar av milj√∂n snarare √§n delar av agenten. P√• samma s√§tt, om vi till√§mpar
MDP-ramverket p√• en person eller ett djur, b√∂r muskler, skelett och organ betraktas som
en del av milj√∂n.
Den allm√§nna regeln som vi f√∂ljer √§r att allt som inte kan √§ndras godtyckligt av agenten
anses vara utanf√∂r det och d√§rmed en del av milj√∂ den √§r i. Sutton och Barto (2018) anser
att vi inte ska anta att allt i milj√∂n √§r ok√§nt f√∂r agenten. Till exempel vet agenten ofta hur
dess bel√∂ningar ber√§knas som en funktion av dess handlingar. I vissa fall k√§nner agenten
till allt om hur dess milj√∂ fungerar men den st√•r fortfarande inf√∂r ett inl√§rningsproblem.
Flera k√§nner till exempel till hur en Rubiks kub fungerar och vad m√•let √§r, men lyckas √§nd√•
inte l√∂sa det.

3.3.3 M√•l och bel√∂ningar
Sutton och Barto (2018) beskriver att i f√∂rst√§rkt inl√§rning formuleras agentens syfte eller
m√•l i form av en signal som kallas bel√∂ningen. Milj√∂n ger agenten en bel√∂ning efter varje
handling. Agentens m√•l att maximera den totala bel√∂ningen den f√•r. Detta inneb√§r inte att
maximera den omedelbara bel√∂ningen, utan den kumulativa bel√∂ningen p√• l√•ng sikt.
Anv√§ndningen av en bel√∂ning f√∂r att uppn√• ett m√•l √§r ett av de tydligaste s√§rdragen
i f√∂rst√§rkt inl√§rning. Sutton och Barto (2018)
√Ñven om formulering av m√•l som bel√∂ningar kan till en b√∂rjan verka begr√§nsande, har det
i praktiken visat sig vara flexibelt och till√§mpligt i flera situationer menar Sutton och Barto
(2018). Exempelvis, f√∂r att f√• en robot att l√§ra sig sj√§lv g√• har forskare tilldelat en bel√∂ning
f√∂r varje steg som √§r leder till att roboten r√∂r sig fram√•t. Genom att f√• en robot att l√§ra sig
fly fr√•n en labyrint √§r bel√∂ningen ofta -1 f√∂r varje steg som den tar; detta uppmuntrar
agenten att fly s√• snabbt som m√∂jligt. F√∂r att l√§ra en robot att hitta och samla tomma
l√§skburkar f√∂r √•tervinning, kan man ge den en bel√∂ning av noll st√∂rsta delen av tiden, men
sedan en bel√∂ning av +1 f√∂r varje burk som samlas in. Det √§r ocks√• vanligt att ge roboten
19

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
negativa bel√∂ningar n√§r den g√∂r ov√§ntade handlingar. F√∂r att l√§ra en agent att spela ett spel,
som till exempel schack, √§r de naturliga bel√∂ningarna +1 f√∂r att vinna och -1 f√∂r att f√∂rlora.
Som det n√§mndes tidigare √§r agentens m√•l √§r alltid att maximera sin bel√∂ning sin framtida
kumulativa bel√∂ning.
Bel√∂ningssignalen √§r ett s√§tt att kommunicera till roboten vad du vill att den ska
uppn√•, inte hur du vill att den ska uppn√•s. Sutton och Barto (2018)
Sutton och Barto (2018) ber√§ttar att det allts√• √§r viktigt f√∂r att de bel√∂ningar vi skapar
indikerar vad vi vill uppn√•. Bel√∂ningen ska inte ge agenten f√∂rkunskaper om hur man vill
att den ska uppn√• m√•let. Till exempel b√∂r en schackspelande agent bel√∂nas bara f√∂r att
vinna, inte f√∂r att uppn√• underliggande m√•l som till exempel att √§ta motst√•ndarens bitar
eller att f√• kontroll √∂ver spelplattans mittpunkt. Yuxi (2018) inst√§mmer med Sutton och
Barto (2018) och n√§mner att f√∂r att l√§ra en f√∂rst√§rkt inl√§rd algoritm att spela Go f√•r den
ocks√• bel√∂ningen f√∂rst till slut. Om att uppn√• delmoment bel√∂nades, kan agenten hitta ett
s√§tt att uppn√• dem utan att uppn√• det verkliga m√•let, vilket var att vinna spelet. Bel√∂nings
signalen √§r ett s√§tt att kommunicera till roboten vad man vill att den ska uppn√•, inte hur
man vill att den ska uppn√•s.

3.3.4 Episoder
Agentens m√•l √§r att maximera den kumulativa bel√∂ningen den f√•r p√• l√•ng sikt. Om
sekvensen av bel√∂ningar som mottas efter varje handling betecknas:ùëÖùë°+1 , ùëÖùë°+2 , ùëÖùë°+3 ,
vilken aspekt av denna sekvens ska maximeras? Sutton och Barto (2018) ber√§ttar att i
allm√§nheten str√§vas efter att maximera den f√∂rv√§ntade avkastningen, d√§r avkastningen,
betecknad Gt, definieras som n√•gon specifik funktion i bel√∂ningssekvensen. I det enklaste
fallet √§r avkastningen summan av bel√∂ningarna:

T beskriver det sista steget eller med andra ord den sista en handling som agenten utf√∂r.
Enligt Sutton och Barto (2018) fungerar denna formel i applikationer d√§r det finns en
20

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
naturlig uppfattning om det sista steget eller handlingen, det vill s√§ga n√§r agentmilj√∂interaktion bryter naturligt till slutet av en episod. Ett exempel p√• detta kunde vara ett
spel av luffarschack eller ett f√∂rs√∂k att hitta ut ur en labyrint. Varje episod slutar i ett
tillst√•nd som kallas terminal tillst√•ndet. Detta f√∂ljs av en √•terst√§llning till ett standard start
tillst√•nd eller ett stokastisk tillst√•nd som f√∂ljer de givna reglerna i milj√∂n. √Ñven om episoder
slutar p√• olika s√§tt, som att vinna och f√∂rlora ett spel, b√∂rjar n√§sta episod i ett tillst√•nd som
√§r oberoende av hur det f√∂reg√•ende slutade.
I andra fall bryts inte agent-milj√∂interaktion naturligt i olika identifierbara episoder, utan
den forts√§tter o√§ndligt. Dessa kallas f√∂r kontinuerliga problem, ett exempel p√• detta kunde
vara en algoritm som √∂vervakar en process kontinuerligt. Bel√∂nings funktionen kan inte
anv√§ndas p√• samma s√§tt h√§r som i exemplet ovan eftersom T kommer vara o√§ndligt. F√∂r att
r√§kna ut bel√∂ningen f√∂r en algoritm som fungerar p√• detta s√§tt beh√∂vs en mer komplicerad
formel enligt Sutton och Barto (2018).
En annat koncept som beh√∂vs √§r diskontering (eng. discounting). Diskonteringen
presenteras av symbolen ·µß, som √§r ett v√§rde mellan 0 och 1. Med diskonteringen √§ndras
formeln f√∂r att ber√§kna avkastningen:

Sutton och Barto (2018) h√§vdar att m√§ngden av diskonteringen best√§mmer hur stort v√§rde
som l√§ggs p√• de framtida bel√∂ningarna. Ifall v√§rdet n√§rmar sig 0 kommer agenten bara att
fokusera sig p√• kortsiktiga bel√∂ningar, bel√∂ningen i framtiden spelar inte en stor roll. Om
v√§rdet n√§rmar sig 1 kommer agenten att v√§rdera framtida bel√∂ningar i mycket h√∂gre grad.
Agenten v√§rdes√§tter stora bel√∂ningar i framtiden i st√§llet f√∂r stora bel√∂ningar genast. I flesta
fall √§r detta det f√∂rv√§ntade resultatet.

3.3.5 Utforskning och exploatering
Om agenten skulle k√§nna till milj√∂n p√• f√∂rhand menar Otterlo & Wiering (2012) att det g√•r
att r√§kna ut den optimala policyn till problemet. Men problem som l√∂ses med f√∂rst√§rkt
inl√§rning finns inte denna kunskap √∂ver milj√∂n. D√• blir det n√∂dv√§ndigt f√∂r att interagera
21

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
med milj√∂n f√∂r att l√§ra agenten en policy genom f√∂rs√∂k och misstag metoden. Agenten
m√•ste utforska milj√∂n genom att utf√∂ra handlingar och f√∂rst√• handlingarnas konsekvenser.
Otterlo & Wiering (2012) menar att agenten m√•ste utforska milj√∂n f√∂r att se om det finns
m√∂jliga f√∂rb√§ttringar av dess policy, allts√• en b√§ttre l√∂sning till problemet. D√• m√•ste den
pr√∂va sig fram och forska milj√∂n slumpm√§ssigt. Detta kan sluta i s√§mre prestanda eftersom
√•tg√§rderna kan ocks√• vara s√§mre √§n den nuvarande policyn. Men utan att pr√∂va olika
handlingar hittar den aldrig m√∂jliga f√∂rb√§ttringar.
Russel och Norvig (2003) tar upp att i vissa fall kan algoritmen hitta en l√∂sning till
problemet som den fastnar f√∂r, men denna l√∂sning √§r inte den optimala. Om denna situation
uppst√•r kallas agenten f√∂r en girig agent. En girig agent hittar s√§llan den optimala l√∂sningen
till problemet utan fastnar vi det f√∂rsta som fungerar.
Eftersom algoritmen inte k√§nner igen den milj√∂ den befinner sig i fastnar den l√§tt f√∂r en
icke optimal l√∂sning. Russel och Norvig (2003) anser att f√∂r att undvika att en agent inte
ska bli girig ska den programmeras att konstant √∂verv√§ga mellan utforskning och
exploatering. Exploatering √§r d√• n√§r algoritmen f√∂ljer vad dess policy s√§ger kommer att
leda till den h√∂gsta bel√∂ningen. Detta leder till att se p√• problemet kortsiktigt och att st√∂da
sig p√• de tidigare h√§ndelserna. Utforskning √§r d√• algoritmen nu och d√• tar en stokastisk
handling ist√§llet f√∂r den handling som ens policy s√§ger. P√• detta vis √§r det meningen att
algoritmen kan sakta men s√§kert hitta nya l√∂sningar till problemet som hoppeligen √§r b√§ttre
√§n den gamla. Utforskning g√∂rs allts√• f√∂r att m√∂jligen √∂ka den framtida bel√∂ningen.
Russel och Norvig (2003) skriver att endast exploatering leder till giriga agenter som inte
hittar den optimala l√∂sningen och endast utforskning leder till att agenten l√§r sig aldrig
n√•got. Vad √§r d√• den b√§sta l√∂sningen, en gyllene mellanv√§g? Russel och Norvig (2003)
h√§vdar att det har forskats mycket inom detta √§mne. Kortfattat ska agenten vara girig men
√§nd√• utforska o√§ndligt. En vanlig f√∂rkortning som anv√§nds f√∂r detta √§r GILE (eng. greedy
in the limit of infinite exploration). Om agenten f√∂ljer GILE schema ska den f√∂rs√∂ka sig p√•
varenda m√∂jliga alternativ f√∂r att hitta de b√§sta l√∂sningarna och att vara s√§ker att man inte
missat en b√§ttre l√∂sning.

22

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
En agent som anv√§nder sig av GILE schemat kommer enligt Russel och Norvig (2003) till
slut att l√§ra sig hur milj√∂n den befinner sig i ser ut. Till slut m√•ste agenten sluta utforska
eftersom vi kan anta att den redan har hittar den optimala l√∂sningen. Ett s√§tt att f√• agenten
att sluta utforska √§r att minska p√• chansen att agenten v√§ljer en utforskande handling, som
Sutton och Barto (2018) n√§mner, tills den blir 0 eller n√§ra 0.

3.3.6 Erfarenhetsuppspelning
Liu & Zou (2017) skriver att vid f√∂rst√§rkt inl√§rning observerar agenten en str√∂m av
h√§ndelser och anv√§nder varje h√§ndelse f√∂r att uppdatera sin interna uppfattning √∂ver milj√∂n
och vilken handling leder till vad. Till exempel kan en upplevelse vara en tupel av (tillst√•nd,
handling, bel√∂ning, nytt tillst√•nd) och agenten anv√§nder varje upplevelse f√∂r att uppdatera
dess v√§rdefunktion.
I flera f√∂rst√§rkt inl√§rda algoritmer kastas h√§ndelsen bort omedelbart efter att det har anv√§nts
f√∂r en uppdatering. De senaste genombrotten i f√∂rst√§rkt inl√§rning har f√∂rt med sig en viktig
teknik som kallas erfarenhetsuppspelning (eng. experience replay memory). Upplevelser
√§r lagrade i en minnesbuffert av en best√§md storlek; n√§r bufferten √§r full, kastas de √§ldsta
minnen bort. Vid varje steg samplas ett slumpm√§ssigt antal upplevelser fr√•n bufferten f√∂r
att uppdatera agentens parametrar. Liu & Zou (2017) bevisar i deras unders√∂kning att
storleken av denna minnesbufferten kan spela en stor roll, b√•de f√∂r liten och f√∂r stor buffert
p√•verkar hur snabbt agenten l√§r sig. De fann att det finns fall d√§r den b√§sta agenten l√§r sig
snabbast med att inte kasta bort alls gamla h√§ndelser men i flera fall snabbas upp
inl√§rningen om inte denna minnesbuffert inte √§r allt f√∂r stor.
Det har bevisats att erfarenhetsuppspelning spelar en viktig roll i att f√∂rb√§ttra resultaten i
f√∂rst√§rkt inl√§rning. Till exempel Silver et.al (2016) och Mnih et al. (2015) har anv√§nt sig
av erfarenhetsuppspelning med stor framg√•ng.

23

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

3.4. Q-inl√§rning
Markovs beslutsprocess som behandlades i kapitel 3.3 ger en modell att ber√§kna
sannolikheten att r√∂ra sig fr√•n ett stadium till ett annat f√∂r att maximera v√§rdefunktionen.
Detta skulle kallas f√∂r en modellbaserad algoritm. Callan (2003) menar att denna modell
av milj√∂n inte alltid finns tillg√§nglig, d√• kallas algoritmen f√∂r modell-fri. Den modell-fria
agentens uppgift √§r att sj√§lv l√§ra sig den optimala policyn f√∂r att maximera v√§rdefunktionen
genom att utforska milj√∂n. Agenten observerar och minns resultatet efter varje interaktion
med milj√∂n. Detta minne √§r viktigt f√∂r att agentens l√§roprocess.
I verkligheten menar Callan (2003) att detta skulle betyda att i b√∂rjan r√∂r sig algoritmen
slumpm√§ssigt fr√•n ett stadium till ett annat. Efter flera episoder skulle algoritmen
slumpm√§ssigt snubbla √∂ver en l√∂sning som ger en stor vinst. Beroende p√• balansen mellan
utforskande och exploatering, som behandlades i 3.3.5, kommer algoritmen sedan att
balansera mellan att r√∂ra mot de stadier som har en garanterad stor vinst och nya,
outforskade stadier i hopp om att hitta en l√∂sning med √§nnu st√∂rre vinst.

Figur 4. Ett simpelt problem f√∂r en f√∂rst√§rkt inl√§rd algoritm ur Callan (2003)
Figur 4 representerar ett enkelt problem som en kan l√∂sas med hj√§lp av en f√∂rst√§rkt
inl√§rning. Ruta 8 √§r start stadiet och ruta 11 √§r v√•rt m√•l, ruta 7 och 9 √§r v√§ggar och ruta 10
√§r en sj√∂ som √§r sv√•r att passera. M√•let √§r att r√∂ra sig fr√•n start till m√•l med s√• f√• steg som
24

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
m√∂jligt. Vi ger algoritmen -1 po√§ng f√∂r varje steg, ett undantag √§r att r√∂ra sig genom sj√∂n
som ger -2 po√§ng. M√•let get algoritmen 10 po√§ng. En algoritm kan f√∂rst hitta en l√∂sning
l√§ngs den streckade linjen, denna l√∂sning skulle ge -6 po√§ng om vi summerar varje r√∂relse
och sedan 10 po√§ng fr√•n m√•let vilket betyder att slutresultatet skulle vara 4 po√§ng. Med
hj√§lp av utforskning kan algoritmen f√∂rs√∂ka hitta andra l√∂sningar. Hoppeligen skulle den
f√∂rr eller senare snubbla p√• en annan l√∂sning, en rutt l√§ngs den andra pilen i figur 4 som
ger 5 po√§ng till sist och slut.
M√•let med f√∂rst√§rkt inl√§rning √§r att fr√§mja de handlingar vid varje stadie som i slutet av
episoden maximerar m√§ngden po√§ng som agenten f√•r. Beroende p√• resultatet i slutet av
episoden vill vi √∂ka nyttov√§rden f√∂r de handlingar som leder till h√∂ga po√§ng i slutet och
minska p√• nyttov√§rden p√• de handlingar som leder till l√•ga po√§ng i slutet av episoden.
Callan (2003) ber√§ttar att ifall inl√§rningstakten ùõº √§r konstant kan det h√§nda att algoritmen
misslyckas att konvergera till ett optimalt resultat. F√∂r att undvika detta kan vi best√§mma
att ùõº minskar vid varje f√∂rs√∂k av handling i ett visst stadie. Detta kan enkelt ber√§knas ifall
man h√•ller reda p√• hur m√•nga g√•nger som agenten gjort en viss handling i ett visst stadie.
Callan (2003) ber√§ttar att q-inl√§rnings algoritmen har en tabell som f√∂rvarar q-v√§rden f√∂r
varje m√∂jliga handling i varje m√∂jliga stadie. N√§r agenten i algoritmen best√§mmer sig f√∂r
att r√∂ra sig fr√•n ett stadie till ett annat kollar den upp q-v√§rden i tabellen f√∂r stadiet den
ligger i. De kan till exempel se ut som {0.1, 0.4, 0.7, 0.3}, i detta fall skulle agenten v√§lja
handling nr 3 p√• grund av att 0.7 hade det h√∂gsta q-v√§rdet.

Formel 1. Formeln f√∂r att uppdatera Q-v√§rden i Q-inl√§rning ur Callan (2003)
Callan (2003) ber√§ttar att i b√∂rjan √§r alla q-v√§rden i tabellen 0, men de uppdateras med hj√§lp
av formeln ovan. Algoritmen som √§r presenterad nedan b√∂rjar med att √§ndra varje v√§rde i
tabellen till noll. Sedan b√∂rjar algoritmen att utforska sin milj√∂ d√§r den v√§ljer alltid den
handling i som har det h√∂gsta q-v√§rdet. En sak som fattas √§r ett kriterium f√∂r n√§r inl√§rningen
ska sluta. Callan (2003) menar att ofta brukar man kolla ifall ett kriterium √§r uppfylt f√∂r att

25

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
sluta med inl√§rningen, till exempel medeltalet av po√§ngen av de successiva episoderna √§r
tillr√§ckligt h√∂gt.

Formel 2. En algoritm f√∂r Q-inl√§rning ur Callan (2003)
Denna algoritm som presenteras ovan √§r inte programmerad att utforska slumpm√§ssigt nu
och d√•, vilket √§r viktigt f√∂r att algoritmen inte ska stanna vid den f√∂rsta l√∂sningen den hittar,
allts√• ett lokalt minimum. Callan (2003) rekommenderar att i b√∂rjan kan agenten utforska
ofta men sakta men s√§kert ska den b√∂rja mer och mer f√∂redra h√§ndelserna med h√∂gre Qv√§rde.

3.4.2 Q-inl√§rning exempel
I detta kapitel visar jag ett exempel fr√•n n√§tsidan www.learndatasci.com skriven av Kansal
& Martin (2019) som beskriver hur man kan programmera en f√∂rst√§rkt inl√§rd algoritm som
anv√§nder sig av q-inl√§rning i python. Problemet som de f√∂rs√∂ker l√∂sa √§r att l√§ra en taxi
plocka upp en passagerare och sl√§ppa av den vart den vill. Jag kommer att referera till
tidigare kapitlen f√∂r att f√• en mer detaljerad bild √∂ver de olika delmomenten.
Som figur 3 (sida 20) visar best√•r sig inl√§rningsprocessen av en milj√∂ och en agent. Figur
7 √§r en mer detaljerad bild av samma process. Milj√∂n ger ett tillst√•nd till agenten samt en
bel√∂ning som baserar sig p√• slutresultatet av den f√∂rra handlingen. Agenten f√•r ett tillst√•nd
26

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
som indata. Efter detta √§r dess uppgift att fatta ett beslut som hoppeligen ger st√∂rsta m√∂jliga
bel√∂ning. Inl√§rningsprocessen kan s√•ledes delas in i sex olika steg enligt Kansal & Martin
(2019):
1. Observera milj√∂n
2. Best√§m hur det l√∂nar sig agera enligt en best√§md strategi
3. Agenten g√∂r ett beslut
4. F√• en bel√∂ning (eller straff)
5. L√§r dig fr√•n h√§ndelsen och optimera strategin
6. Upprepa tills en optimal l√∂sning hittas

Figur 4 Inl√§rningsprocessen f√∂r en f√∂rst√§rkt inl√§rning. Kansal & Martin (2019)
Problemet som vi f√∂rs√∂ker l√∂sa √§r att l√§ra en taxi att hitta samt plocka upp en passagerare
och f√∂ra den till sin destination med s√• f√• steg som m√∂jligt. Det kommer ocks√• att finnas
n√•gra v√§ggar som taxin m√•ste l√§ra sig att undvika.

Figur 5. Visualisering av milj√∂n, ur Kansal & Martin (2019)
27

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Figur 5 √§r en visualisering av den milj√∂ som taxin kommer att kunna r√∂ra sig i. Milj√∂n
best√•r av ett 5x5 rutf√§lt och fyra stycken punkter d√§r passageraren kan bli plockad upp eller
l√§mnad (R, Y, B, G). I figur 5 √§r det meningen att taxin ska plocka upp passageraren fr√•n
ruta (0,4) och hen ska bli l√§mnad i ruta (0,0). Kansal & Martin (2019) ber√§ttar att detta
betyder att det finns 25 olika st√§llen var taxin kan befinna sig. Fem olika st√§llen var taxin
passageraren kan finnas, i (R, Y, B, G) eller inne i taxin samt fyra m√∂jliga destinationer f√∂r
passageraren om vi antar att den finns en m√∂jlighet att hen vill bli sl√§ppt av i samma ruta
som hen blev plockad upp.
Efter summering av alla de m√∂jliga tillst√•nden, 5 x 5 x 5 x 4 = 500, betyder det att det finns
500 olika m√∂jliga tillst√•nd denna milj√∂. Milj√∂ som koncept √§r noggrannare beskrivet i
kapitel 3.1.
Efter fastst√§llningen av milj√∂n samt problemet m√•ste det best√§mmas hur taxin kan r√∂ra sig
i milj√∂n. F√∂r att r√∂ra sig m√•ste taxin g√∂ra en handling, handlingar √§r noggrannare beskrivna
i kapitel 3.1 och 3.2. I detta problem har taxin 6 olika handlingar som den kan fatta:
1. R√∂ra sig mot syd
2. R√∂ra sig mot nord
3. R√∂ra sig mot √∂st
4. R√∂ra sig mot v√§st
5. Plocka upp passageraren
6. Sl√§ppa av passageraren
Vid varje tillst√•nd i milj√∂n m√•ste taxin allts√• v√§lja mellan dessa handlingar. Olika
handlingar leder till olika m√§ngder av po√§ng f√∂r taxin. Eftersom m√•let √§r att l√§ra taxin att
l√∂sa problemet, kommer de f√∂rv√§ntade handlingarna bel√∂nas och de ov√§ntade straffas. Ett
exempel p√• en ov√§ntad handling kunde vara till exempel om taxin f√∂rs√∂ker √•ka √∂ver v√§ggen
eller om den f√∂rs√∂ker sl√§ppa av passageraren vid fel st√§lle. M√•let √§r att taxin ska hitta en
l√∂sning med s√• f√• steg som m√∂jligt ger vi ocks√• ett litet straff f√∂r varje handling. Agenten
f√•r en stor bel√∂ning d√• den har lyckats plocka upp passageraren och f√∂rt hen till r√§tt plats.

28

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
I tillst√•ndet som taxin befinner sig i figur 5 m√•ste den v√§lja mellan de sex olika
handlingarna som √§r givna till den. I b√∂rjan √§r sannolikheten lika stor f√∂r varje handling
men efter att agenten har blivit tr√§nad kommer dessa att √§ndra f√∂r att reflektera de b√§sta
alternativen i de givna tillst√•nden. Kansal & Martin (2019) p√•pekar att i detta stadie har
taxin en v√§gg till v√§st, ifall taxin skulle f√∂rs√∂ka g√• in i v√§ggen kommer den bli bestraffad
och den kommer inte att r√∂ra sig. P√• grund av att vi l√§r taxin att l√∂sa problemet i s√• f√• steg
som m√∂jligt kommer taxin att b√∂rja undvika v√§ggarna eftersom de leder till en bestraffning
men inget framsteg.
F√∂ att l√∂sa problemet i fr√•gan anv√§nds q-inl√§rning. Q-inl√§rning anv√§nder sig av en Q-tabell
var den f√∂rvarar sannolikheten att v√§lja olika alternativ vid varje tillst√•nd. Tabell 1 och 2
illustrerar hur sannolikheten f√∂r varje m√∂jliga handling f√∂rvaras i tabellen f√∂re agenten har
blivit tr√§nad och efter. Till exempel i stadie nr 328 som taxin befinner sig i tabell 2 kommer
agenten att v√§lja handling nr 1 (r√∂r sig mot nord) eftersom det har visat sig i tr√§ningen att
denna handling ger den h√∂gsta m√∂jliga totala bel√∂ningen i slutet av episoden ifall
passageraren √§r i ruta (0,4) och taxin befinner sig i ruta (1, 3). F√∂r att uppdatera de olika
v√§rden i Q-tabellen anv√§nds formel i formel 1 i kapitel 4.

Tabell 1 och 2 . Visualisering av bel√∂nings tabellen i Q-inl√§rning f√∂re tr√§ningen
och efter. Ur Kansal & Martin (2019).
29

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Tre vanliga variabler som anv√§nds f√∂r att optimera inl√§rningen √§r:
Œ±: Alpha, som ocks√• syns i formel 1. Detta √§r inl√§rnings hastigheten (eng. learning
rate) som anv√§nds ocks√• i √∂vervakad inl√§rning.
‚ÑΩ:

Gamma.

Gamma

√§r

diskonteringsfaktorn

f√∂r

algoritmen.

En

h√∂g

diskonteringsfaktor (n√§ra 1) anv√§nds f√∂r att taxin skulle v√§lja l√∂sningar som get p√•
l√•ng sikt den h√∂gsta m√∂jliga bel√∂ningen. Ifall gamma √§r n√§ra 0 tar taxin bara i
beaktande de l√∂sningar som ger h√∂gsta bel√∂ning p√• kort sikt. Diskonteringen
behandlas noggrannare i kapitel 3.4.
ùõú: Epsilon. Epsilon √§r f√∂rknippad med utforskning och exploatering, h√∂gre epsilon
leder till en h√∂gre grad an utforskning. Detta behandlas i kapitel 3.6.
I b√∂rjan av tr√§ningen kommer agenten att g√∂ra flera misstag och bli bestraffad i form av
minuspo√§ng men sakta men s√§kert kommer den att r√∂ra sig mot en optimal l√∂sning.
Taxi problemet kan l√∂sas med q-inl√§rning, men nackdelen med q-inl√§rning √§r att den kr√§ver
en tabell som den kan referera till. Ifall m√§ngden tillst√•nd och m√∂jliga handlingar blir st√∂rre
blir denna tabell ocks√• exponentiellt st√∂rre. Till exempel om taxin skulle befinna sig i ett
10x10 rutf√§lt skulle det finnas redan 10 x 10 x 5 x 4 = 2000 olika m√∂jliga tillst√•nd som
taxin kan befinna sig i.
F√∂r att undvika en massiv q-tabell kan ett neuralt n√§tverk anv√§ndas ist√§llet f√∂r Q-tabellen
f√∂r att f√∂rvara resultaten agentens f√∂rflutna handlingar i de olika tillst√•nden. Detta kallas
f√∂r djup q-n√§tverk och behandlas i kapitel 3.5.4.1.

3.5. Djupinl√§rning
Med hj√§lp av djupinl√§rning √§r det enligt LeCun et al. (2015) m√∂jligt att g√∂ra
ber√§kningsmodeller som best√•r av flera bearbetningslager f√∂r att l√§ra sig m√∂nster ur data
med flera abstraktionsniv√•er. Dessa metoder har drastiskt f√∂rb√§ttrat den senaste tekniken
inom taligenk√§nning, objektigenk√§nning i bilder och m√•nga andra dom√§ner s√•som
l√§kemedels uppteckning. Djupinl√§rning uppt√§cker invecklade struktur i stora datam√§ngder
30

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
med hj√§lp av backpropagation algoritmer. Backpropagation anv√§nds f√∂r att indikera hur en
maskin ska √§ndra sina interna parametrar (vikterna mellan noderna) som anv√§nds f√∂r att
ber√§kna representationen i varje lager fr√•n representationen i f√∂reg√•ende lager.
Konventionella maskininl√§rningstekniker var begr√§nsade i deras f√∂rm√•ga att bearbeta data
fr√•n naturen i sin r√•a form. Om man ville g√∂ra ett m√∂nsterigenk√§nningssystem eller
maskininl√§rningssystem under de senaste decennier kr√§vde det enligt LeCun et al. (2015)
ett noggrant konstruerat funktions extraktor (eng. feature extractor). Denna omvandlar den
r√•a naturdata (som en bilds pixelv√§rden) till en l√§mplig representation eller funktionsvektor.
Med hj√§lp av detta kan inl√§rningssystemet, ofta en klassificerare, uppt√§cka eller klassificera
m√∂nster av dess input.
Djupinl√§rningsmetoder

√§r

metoder

f√∂r

l√§rande

av

representation

med

flera

representationsniv√•er. Djupinl√§rning best√•r s√•ledes av flera enkla icke-linj√§ra moduler som
var och en omvandlar representationen p√• en niv√• (b√∂rjar med indata) till en representation
p√• en h√∂gre, en mer abstrakt niv√•. Med hj√§lp av tillr√§ckligt m√•nga av s√•dana moduler kan
en maskin l√§ra sig mycket komplexa funktioner. F√∂r klassificeringsproblem f√∂rst√§rker
h√∂gre skikt av representation aspekter av indata som √§r viktiga f√∂r diskriminering
irrelevanta variationer. (LeCun et al. (2015))
En bild kan till exempel vara i form av en matris med pixelv√§rden. De inl√§rda funktionerna
i det f√∂rsta representationslagret representerar vanligtvis n√§rvaron eller fr√•nvaron av kanter
vid s√§rskilda riktningar p√• platser i bilden. Det andra lagret uppt√§cker vanligtvis motiv
genom att uppt√§cka speciella kantarrangemang, oavsett sm√• variationer i l√§gen av kanterna.
Det tredje lagret kan montera motiv i st√∂rre kombinationer som motsvarar delar av k√§nda
f√∂rem√•len, de efterf√∂ljande lagerna skulle uppt√§cka objekt som √§r en kombination av dessa
delar. (LeCun et al. (2015))
Djupinl√§rning har visat sig vara bra p√• att uppt√§cka strukturer i h√∂gdimensionella data och
√§r d√§rf√∂r till√§mpar sig p√• m√•nga omr√•den inom vetenskap, f√∂retag och myndigheter.
I ett klassificerings problem f√∂rs√∂ker maskinen till exempel para ihop r√§tt bild med r√§tt
kategori. Bilden visas flera g√•nger till maskinen och varje g√•ng ger den ut en po√§ngvektor
f√∂r varje klass. Maskinen √§ndrar sedan de interna justerbara parametrarna efter varje
31

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
visning f√∂r att minska m√§ngden fel klassificeringar. Dessa justerbara parametrar kallas ofta
f√∂r vikter. Dessa √§r i verkligheten siffror som kan ses som 'knoppar' vilka definieras av
maskinens ing√•ngs- och utg√•ngsfunktion. I ett typiskt djupinl√§rningssystem kan det finnas
hundratals miljoner av dessa justerbara vikter och hundratals miljoner m√§rkta exempel som
man kan tr√§na maskinen. (LeCun et al. (2015))

3.5.1 Neurala n√§tverk
En neuron √§r en cell i hj√§rnan vars huvudsakliga uppgift √§r att samla och bearbeta elektriska
signaler. Hj√§rnans f√∂rm√•ga att bearbeta information anses vara kopplad till f√∂rm√•gan att
bilda flera n√§tverk av dess neuroner. P√• grund av denna uppfattning av hj√§rnans
funktionalitet, skriver Russel och Norvig (2003), riktades den tidigaste forskningen inom
artificiell intelligens in p√• att bilda artificiella neurala n√§tverk. Russel och Norvig skriver
att redan 1943 har McCulloch och Pitts ritat upp ett schema √∂ver hur dessa artificiella
neuroner ska fungera. Grundprinciperna i deras modell var att neuronen kommer att
aktiveras n√§r dess input √∂verstiger en given gr√§ns.
Russel och Norvig (2003) ber√§ttar att ett neuralt n√§tverk best√•r av flera noder som √§r
anslutna till varan genom riktade l√§nkar. L√§nken mellan noderna binder dem till varan, och
l√§nkens funktion √§r ocks√• att f√∂ra signalen vidare f√∂r att aktivera n√§sta nod. Varje l√§nk har
en specifik parameter som kallas f√∂r vikt, och denna vikt best√§mmer styrkan p√• l√§nken.
Varje nod ber√§knar f√∂rst summan av dess input med h√§nsyn till l√§nkens vikt som signalen
kommer ifr√•n. Denna summa l√§ggs in i en aktiveringsfunktion (eng. activation function)
som sedan avg√∂r om noden ska aktiveras.
Aktiveringsfuntkionen har tv√• uppgifter enligt Russel och Norvig (2003). F√∂rst och fr√§mst
√§r det meningen att noden √§r aktiv, vilket betyder n√§ra +1, ifall en r√§tt input √§r given, och
n√§ra 0 ifall en fel input √§r given. Resultatet f√∂rv√§ntas vara icke linj√§rt. F√∂r att uppn√• detta
anv√§nder vi till exempel oss av en tr√∂skelfunktion (eng. threshold function) eller en
sigmoidfunktion.

32

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Figur 6. En tr√∂skelfunktion (a) och en sigmoidfunktion (b) ur Russel och Norvig
(2003)
B√•da funktionerna har ett tr√∂skelv√§rde, antingen h√•rt (tr√∂skelfunktionen) eller mjukt
(sigmoidfunktionen) ber√§ttar Russel och Norvig (2003). Vikten av l√§nken till noden m√•ste
dock tas i h√§nsyn n√§r man r√§knar v√§rdet f√∂r input och kollar det i mot den best√§mda
tr√∂skeln.
Russel och Norvig (2003) n√§mner tv√• typer av neurala n√§tverk, n√§mligen fram√•triktade
n√§tverk (eng. feed-forward networks) och √•terkommande n√§tverk (eng. recurrent
networks). Ett fram√•triktat n√§tverk representeras endast av en funktion av dess nuvarande
input, detta betyder att den inte har n√•got internt stadie annat √§n dess vikter. Ett
√•terkommande n√§tverk √§r en aning mer komplicerat, den matar ens output tillbaka till ens
√§gna inputs. Detta kan leda till ett stabilt n√§tverk men i vissa fall kan slutresultatet var
n√§stan kaotiskt enlig Russel och Norvig (2003). Denna typs n√§tverk har dock ocks√•
f√∂rdelar, de kan n√§mligen han ett korttidsminne ocks√•, detta √§r i motsats till fram√•triktade
n√§tverk som inte kan.

Figur 7. Enkelt neuralt n√§tverk ur Russel och Norvig (2003)

33

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Figur 5 representerar ett enkelt fram√•triktat neuralt n√§tverk med tv√• input v√§rden (1,2), tv√•
dolda enheter (3,4) samt v√•r output 5. W √§r vikten mellan dessa tv√• enheter.

Ber√§kningarna ovan visar att n√§tverkets output r√§knas ut genom att r√§kna summan av
vikterna och enheternas input vid varje nod. V√§rdet f√∂r nod 5, v√•r output, √§r summan av
resultatet av nod 3 och 4. N√§tverket l√§r sig enligt Russel och Norvig (2015) genom att √§ndra
vikterna mellan enheterna, eftersom d√• √§ndrar nodens output.
Det vanligaste s√§ttet att r√§kna ut sluttningsvektor som anv√§nds f√∂r att √§ndra v√§rden p√•
vikterna √§r enligt LeCun et al. (2015) stokastisk gradientstigning (eng. stochastic gradient
decent). Detta best√•r av att visa inmatningsvektorn n√•gra exempel, och p√• basen av dess
output justera vikterna i enligt resultatet. Efter tr√§ningen av algoritmen med hj√§lp av
tr√§ningsupps√§ttningen testas dess prestanda med hj√§lp av en testupps√§ttning som best√•r av
exempel som algoritmen inte sett f√∂rr. Detta g√∂rs f√∂r att testa hur bra algoritmen √§r att
generalisera, vilket betyder att hur bra algoritmen kan l√∂sa dess givna problem med data
som den inte sett under tr√§ningen.
En vanlig form av ett fram√•triktat n√§tverk som Russel och Norvig (2015) n√§mnde kallas
f√∂r Multi layer perceptron. F√∂rkortas ofta till MLP. MLP kallas ofta f√∂r den vanligaste
formen av neurala n√§tverket. I MLP-n√§tverk √§r varje nod kopplat till alla andra noder i
f√∂ljande lager. Dessa lager kallas f√∂r fullt kopplade lager (eng. fully connected layers).
Dense layers √§r ocks√• en vanlig ben√§mning f√∂r dessa lager.

3.5.2 Faltningsn√§tverk
Faltningsn√§tverk (eng. Convolutional neural network, CNN) √§r en annan vanlig form av
neurala n√§tverk. Enligt LeCun (2015) √§r dessa n√§tverk skapade f√∂r att bearbeta indata som
kommer i form av flera matriser, till exempel en f√§rgbild som √§r sammansatt av tre
34

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
tv√•dimensionella-vektorer som inneh√•ller v√§rdet f√∂r pixeln i de tre f√§rgkanalerna (r√∂d,
gr√∂n, bl√•). Flera typer av data formas med hj√§lp av flera matriser: en dimension f√∂r signaler
och sekvenser, inklusive spr√•k; tv√• dimensioner f√∂r bilder eller ljudspektrogram; och tre
dimensioner f√∂r video. Det finns fyra viktiga id√©er bakom f√§ltningsn√§tverk som drar nytta
av egenskaperna hos naturliga signaler: lokala f√∂rbindelser, sammanslagning,
anv√§ndningen av m√•nga lager och delade vikter.
Arkitekturen av ett vanligt faltningsn√§tverk liknar ofta arkitekturen som avbildas i figur 8.
De f√∂rsta stegen √§r inneh√•ller tv√• typer lager: faltningslager och samlingslager.
Faltningslagrens uppgift √§r att k√§nna igen lokala motiv, medan samlingslagrens uppgift √§r
att sl√• samman likadana motiv. Data i matriser, som till exempel bilder, formar ofta l√§tt
igenk√§nnbara lokala motiv. I faltningsn√§tverk spelar det ingen roll var i bilden dessa
igenk√§nnbara motiv uppkommer, detta √§r viktigt p√• grund av att till exempel √∂gat av en
hund kommer knappast vara p√• samma st√§lle i varje bild. (LeCun et al. 2015)
Ofta finns det enligt LeCun et al. (2015) tv√• till tre lager av faltningslager som f√∂ljs av ett
samlings lager, dessa f√∂lja av mer faltningslager och till sist ett lager som har som output
en po√§ngvektor. Som flera andra neurala n√§tverk anv√§nder faltningsn√§tverk sig ocks√• av
bak√•tpropagering. Dessa faltningslager och samlingslagren i faltningsn√§tverket har blivit
inspirerade av hur synceller fungerar med simpla celler och komplexa celler.
LeCun et al (2015) och Duda och Hart (1973) h√§vdar att sedan 1960 talet har vi vetat att en
linj√§r klassifikations algoritm kan dela dess input i bara n√•gra enkla omr√•den. Problem
uppst√•r d√• bilden och ljud kr√§ver en ing√•ngs- och utg√•ngsfunktion som inte bryr sig om
enkel variation som st√§llning, lutning eller belysningen av ett objekt. Samtidigt m√•ste
maskinen vara dock sensitiv f√∂r sm√• skillnader som till exempel problemet att skilja p√• en
vit varg som en Samoyed hund som LeCun et al (2015) tar upp.
LeCun et al. (2015) tar upp ett exempel med tv√• Samoyed hundar och i olika st√§llningar
och milj√∂er. De menar att hundarna kan se olika ut, medan en vit varg och en Samoyed
hund i samma milj√∂ och position kan likna varan till en h√∂g grad. En klassifikations
algoritm som kollar endast p√• pixelv√§rden kan k√§mpa med att urskilja p√• djuren i det senare
exemplet. P√• grund av detta beh√∂vs en funktions extraktor som tar i h√§nsyn speciella
aspekter p√• djuren som √§r distinkta f√∂r den givna rasen. Vanligtvis har dessa funktions
35

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
extraktorer gjorts f√∂r hand menar LeCun et al. (2015), men detta kan undvikas i dagens
l√§ge om man anv√§nder algoritmer med allm√§nt syfte som sj√§lv l√§r sig dessa specifika
detaljerna. Detta √§r en av djupinl√§rnigens m√§rkv√§rdigaste f√∂rdelar.

Figur 8 beskriver stegen i ett vanligt faltningsn√§tverk. Bilderna representerar varje
niv√•s output vid de g√∂mda enheterna. Bilderna inneh√•ller de inl√§rna
k√§nneteckande s√§rdrag f√∂r att k√§nna igen olika djur. Niv√•n h√∂gst upp letar efter
olika kanter, niv√•n under den letar efter en samling av kanter osv. Ur LeCun et al.
(2015)

LeCun et al. (2015) forts√§tter med att ber√§tta att faltningsn√§tverk har anv√§nts med stor
framg√•ng sedan b√∂rjan av 2000 talet i uppt√§ckande och igenk√§nnande av olika objekt och
omr√•den i bilder. I b√∂rjan anv√§ndes dessa fr√§mst till problem som redan hade en hel del
kategoriserade data, som till exempel att k√§nna igen trafikm√§rken. Nyligen har den senaste
framg√•ngen varit i att k√§nna igen ansikten.
Faltningsn√§tverk blev dock inte popul√§ra f√∂rr√§n ImageNet t√§vlingen √•r 2012. N√§tverk
tr√§nades med 1000 olika klasser och en miljon olika bilder, de b√§sta l√∂sningarna halverade
m√§ngden fel klassificeringar j√§mf√∂rt med t√§vlande teknologier enligt Krizhevsky et al.
(2012). Krizhevsky kom med flera f√∂rb√§ttringar som till exempel en ny teknik som kallades
dropout d√§r vissa noder st√§ngs ner f√∂r att f√∂rb√§ttra n√§tverkets chanser att generalisera data
ist√§llet f√∂r att minnas det utantill. En annan viktig f√∂rb√§ttring var att generera mera
tr√§ningsdata med hj√§lp av att f√∂rvr√§nga de existerande exempel. Denna t√§vling startade en
36

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
revolution i datorsyn. Faltningsn√§tverk √§r √§n i denna dag den dominerande algoritmen f√∂r
att k√§nna igen och uppt√§cka objekt.

3.5.3 Recurrent neural network
Efter att back√•tpropagering introducerades anv√§ndes det i b√∂rjan till f√∂r att l√§ra RNNn√§tverk (eng. recurrent neural network). LeCun et al. (2015) menar att f√∂r problem som har
en sekventiell input som till exempel talprov och text √§r det ofta b√§st att anv√§nda ett RNN.
De processar ens input ett element i taget. Samtidigt kommer n√§tverkets dolda enheter √•t
en vektor som inneh√•ller information av alla tidigare element i sekvensen. N√§tverket har
med hj√§lp av denna vektor ett minne av de tidigare input v√§rden.
LeCun et al. (2015) forts√§tter med att p√•st√• att RNN har visat sig vara ett v√§ldigt kraftfullt
system. Att tr√§na dem har dock visat sig vara problematiskt eftersom den bak√•tpropagerade
lutningen √∂kar eller minskar vid varje steg vilket leder till att lutningen kan enligt LeCun
et al. (2015) ‚Äúexplodera eller f√∂rsvinna‚Äù. Med hj√§lp av framsteg i deras arkitektur och olika
s√§tt att tr√§na n√§tverket har det visat sig att RNN kan vara v√§ldigt bra i att f√∂rutsp√• n√§sta
bokstaven i ett ord eller n√§sta ordet i en mening.
Fast√§n RNN-n√§tverkets uppgift √§r att l√§ra sig samband p√• l√•ng sikt, finns det flera
unders√∂kningar som bevisar att det √§r sv√•rt att lagra information i n√§tverk p√• l√•ng sikt. Ett
exempel √§r Bengio et al. (1994) som skriver att det √§r sv√•rt att l√§ra samband p√• l√•ng sikt
med hj√§lp av gradientnedstigning. F√∂r att fixa detta problem tar LeCun et al (2015) upp en
unders√∂kning gjord av Hochreiter och Schmidhuber (1997) var de introducerade LSTM
n√§tverk. LSTM st√•r f√∂r l√•ngt-korttidsminne (eng. long short term memory). Dessa n√§tverk
skiljer sig fr√•n RNN med att de skulle ha speciella dolda enheter var uppgift var att minnas
input under en l√•ng tid. LeCun et al. (2015) menar att LSTM n√§tverk har visat sig vara mer
effektiva √§n konventionella RNN.

3.5.4 Djup f√∂rst√§rkt inl√§rning
Q-inl√§rning har samma problem som andra simpla maskininl√§rningsalgorimer, om
datam√§ngden blir f√∂r stor eller komplex blir ber√§kningarna extremt komplexa. Till exempel
nackdelen med Q-inl√§rning √§r att alla stadier och m√∂jliga handlingar m√•ste lagras i en
37

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
tabell. Komplexa problem med flera variabler kan leda till en tabell som inte ryms p√• en
dators RAM-minne.
F√∂r att l√∂sa detta problem kan djup f√∂rst√§rkt inl√§rning anv√§ndas. Yuxi (2018) ber√§ttar att
ist√§llet f√∂r en massiv tabell f√∂r q-v√§rden, kan vi anv√§nda ett neuralt n√§tverk som uppskattar
nyttov√§rden och vinstfunktionen f√∂r de olika handlingarna. Ist√§llet f√∂r att √§ndra v√§rden i en
tabell √§ndras vikterna i n√§tverket. En stor f√∂rdel med att anv√§nda ett neuralt n√§tverk ist√§llet
f√∂r en stor tabell √§r att n√§tverket kan uppskatta nyttov√§rdet f√∂r handlingar i ett visst tillst√•nd
fast√§n agenten inte har bes√∂kt det specifika tillst√•ndet √§nnu. Detta √§r p√• grund av att
n√§tverket lagrar resultatet av de tillst√•nd den har sett och detta p√•verkar nyttov√§rden av
motsvarande tillst√•nd. I q-inl√§rning √§ndras v√§rden i q-tabellen endast d√• agenten bes√∂kt
tillst√•ndet och valt en specifik handling. Q-inl√§rning kr√§ver enligt Callan (2003) att agenten
bes√∂ker varje tillst√•nd flera g√•nger f√∂r att √§ndra q-v√§rden i tillst√•ndet. I extremt komplexa
problem √§r det m√∂jligt att algoritmen bes√∂ker vissa s√§llsynta tillst√•nd v√§ldigt s√§llan vilket
g√∂r att nyttov√§rden f√∂r det tillst√•ndet inte konvergerar till en optimal handling.

3.5.4.1 Djupa Q-n√§tverk
Mnih et al. (2015) har utvecklat en f√∂rst√§rkt inl√§rd algoritm som de kallar f√∂r ett djupt qn√§tverk (DQN). Denna algoritm kombinerar ett faltninsn√§tverk med q-inl√§rning som √§r
specialiserad f√∂r att hantera m√•ngdimensionella data, som till exempel bilder. Sutton och
Barto (2018) skriver att f√∂re Mnih et al. (2015) hade neurala n√§tverk och faltningsn√§tverk
producerat imponerande resultat men att kombinera dem med en f√∂rst√§rktinl√§rning var
s√§llsynt.
M√•let f√∂r Mnih et al. (2015) var att skapa en f√∂rst√§rkt inl√§rd agent som kan l√∂sa flera olika
problem utan att √§ndra parametrarna i algoritmen eller att ge den problem specifika input
data. De demonstrerade detta med att l√•ta deras DQN-algoritm spela 49 olika Atari 2600spel med hj√§lp av en emulator. Mnih et al. (2015) valde att kombinera deras
faltningsn√§tverk med Q-inl√§rning enligt Sutton och Barto (2018) p√• grund av att Qinl√§rning √§r modell-fri och den har ingen best√§md policy.

38

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Figur 9. Schemat f√∂r DQN algoritmen som Mnih et al (2015) anv√§nde. Den best√•r
av tre lager av faltninsn√§tverk som √§r kopplade med tv√• helt kopplade lager som
till sist leder till output lagret som √§r kopplat med emulatorn f√∂r att r√∂ra sig i spelen.
Ur Mnih et al. (2015)
Som input anv√§nde Mnih et al. (2015) endast bilder direkt ur spelen. Bilderna fr√•n spelen
var 210 x 160 pixlar, men de f√∂rminskade dessa bilder ner till 84x84 pixlar f√∂r att minska
m√§ngden dimensioner samt f√∂r att f√∂rsnabba tr√§ningen, mindre bilder kr√§ver mindre
ber√§kningar.
DQN-algoritmen spelade enligt Mnih et al. (2015) varje spel f√∂r en tidsperiod som skulle
motsvara 38 dagar. Algoritmen l√§rde sig att spela dessa spel b√§ttre √§n alla tidigare f√∂rst√§rkt
inl√§rda algoritmer i all utom 6 spel. De j√§mf√∂rde ocks√• hur algoritmen klarade sig j√§mf√∂rt
med en m√§nniska. Ifall algoritmen fick 75% eller mera po√§ng i ett visst spel ans√•g de att
den var lika bra eller b√§ttre p√• spelet som en m√§nniska. Mnih et al (2015) konstaterade att
DQN-algoritmen klarade att sig lika bra eller b√§ttre √§n en m√§nniska i 29 av de 49 spel som
testades.
Sutton & Barto (2018) menar att det √§r imponerande att n√•gon inl√§rd algoritm klarade sig
s√• bra som denna, men vad som g√∂r detta √§nnu mer imponerande √§r att det var samma
inl√§rningssystem som anv√§ndes f√∂r att spela de olika spelen. Inga specifika modifikationer
gjordes och inga specifika instruktioner f√∂r respektive spel gavs till algoritmen.

39

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

4. V√ÑRDERINGEN AV FINANSIELLA MEDEL MED
HJ√ÑLP AV MASKININL√ÑRNINGSMETODER
4.1. Grunder i aktiev√§rdering
Detta kapitel handlar om grunderna till aktiev√§rdering och hur man kan v√§rdera f√∂retag.
Teknisk analys baserar sig p√• en analys av en akties historiska prisutveckling i hopp om att
det ska ge en inblick i dess framtida prisutveckling. Fundamental analys fokuserar mera p√•
f√∂retaget och dess nyckeltal. Kapitlet behandlar ocks√• hypotesen om effektiva marknader
och random walk-teorin. De √§r tv√• av de mest k√§nda teorierna som h√§vdar att det inte √§r
m√∂jligt att f√∂rutsp√• en akties framtida prisutveckling och att man inte kan √∂ka avkastningen
p√• investeringar utan att √∂ka risken av investeringarna.

4.1.1 Teknisk analys
Enligt Nassitroussi et al. (2014) finns det tv√• k√§nda metoder f√∂r att f√∂rutsp√• hur
aktiemarknaden kommer att r√∂ra sig. Det f√∂rsta s√§ttet kallas f√∂r teknisk analys och det andra
s√§ttet √§r fundamental analys som behandlas i n√§sta kapitel. Teknisk analys baserar sig
prediktionerna p√• historiska aktiedata f√∂r att hitta trender som kan fungera som st√∂d och
motst√•nds punkter f√∂r kurssv√§ngningarna. Till detta s√§tt h√∂r ocks√• f√∂ljandet och att
unders√∂ka olika analytiska modeller som glidande medelv√§rde och relativa styrkeindexet.
Vid anv√§ndning av teknisk analys √§r en graf √∂ver aktiekursens historiska utveckling en
aktiehandlarens fr√§msta verktyg. Denna graf √§r ofta uppritad antingen som en linje eller
ljusstakediagram.

Figur 10 Ljusstakediagramkomponenter ur Nesbitt och Barass (2004)

40

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Figur 10 illustrerar en ljusstake i ett ljusstakediagram. Varje ljusstake representerar en
tidsperiod, som till exempel en minut eller en dag. Toppen av ljusstaken (high) √§r det h√∂gsta
v√§rdet under tidsperioden och svansen av ljusstaken (low) √§r det l√§gsta v√§rdet under
tidsperioden. N√§r en ny period b√∂rjar ritas det f√∂rsta strecket. Detta v√•gr√§ta streck (open)
√§r det f√∂rsta uppm√§tta v√§rde p√• den nya tidsserien. Det sista v√§rdet p√• tidsserien illustreras
med ett till v√•gr√§tt streck (close). Beroende p√• om v√§rdet g√•tt upp eller ner √§r det sista
v√•gr√§ta strecket ovanf√∂r eller nedanf√∂r det f√∂rsta strecket. I figur 10 illustreras en uppg√•ng
av priset illustrerat med en vit ljusstake och en nedg√•ng med en svart.
Glidande medelv√§rde anv√§nder, enligt Wong, Mazur och Chew (2010), en historisk period
‚Äún‚Äù, till exempel 100 dagar eller 20 minuter. Efter att man valt sin period p√• till exempel
100 dagar, r√§knar man medeltalet av aktiens st√§ngningspriser f√∂r de senaste 100 dagarna.
Denna punkt ritas p√• en graf tillsammans med aktiens v√§rde. Efter att denna process
upprepas f√∂r varje dag, leder det till att en linje b√∂rjar bildas p√• grafen som visar aktiens
glidande medelv√§rde p√• den givna perioden. Denna linje kan fungera som st√∂d och
motst√•nd f√∂r kurssv√§ngningarna, vilket i sin tur kan leda till k√∂p och s√§ljsignaler. Till
exempel ifall aktiens v√§rde har r√∂rt sig ovanf√∂r ett visst glidande medelv√§rde och historiskt
alltid n√§r aktiekursen tangerar det glidande medelv√§rdet har kursen skjutit i h√∂jd, d√• kan
detta ses som st√∂d f√∂r kurssv√§ngningarna och som en k√∂psignal ifall kursen h√•ller sig
ovanf√∂r medelv√§rdet. Ifall kursen sjunker under medelv√§rdet kan det ses som en s√§ljsignal
f√∂r att aktiekursen har tappat sin motst√•ndpunkt.
Wong et al. (2010) skriver att det relativa styrkeindexet (RSI, relative strength index) kan
anv√§ndas f√∂r att unders√∂ka om en viss aktie √§r ‚Äúoversold‚Äù eller ‚Äúoverbought‚Äù. Dessa √§r
ben√§mningar f√∂r en aktie ifall den √§r relativt svag eller stark. F√∂r att r√§kna ut RSI beh√∂vs
det p√• samma s√§tt som med glidande medelv√§rde v√§ljas en viss tidsperiod. Indexet r√∂r sig
mellan 0 och 100, och beroende p√• vad indexet visar kan man dra slutsatser om en aktie
kommer att g√• upp eller ner. Ett exempel kunde vara d√• man valt en period p√• 14 dagar och
RSI b√∂rjar stiga √∂ver 70, vilket indikerar att under de senaste 14 dagarna har aktien varit
relativt stark och att en korrigering ned√•t i priset kommer att intr√§ffa snart. Det motsatta
intr√§ffar d√• RSI varit svag och n√§rmar sig 30 under samma period. Detta √§r en signal p√• att
en korrigering upp√•t √§r att v√§nta.

41

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Figur 11 Exempel p√• m√∂nster som anv√§nds i teknisk analys ur Nesbitt och Barass
(2004)
Enligt Nesbitt och Barass (2004) anv√§nds flera m√∂nster som upprepas ofta ocks√• som k√∂p
och s√§ljsignaler f√∂r handlare. I figur 2 illustrerar a och b ett m√∂nster som ofta leder till en
negativ v√§rdef√∂r√§ndring, dessa m√∂nster kallas f√∂r head and shoulders och double top. De
tv√• f√∂ljande m√∂nstren, c och d, indikerar en positiv v√§rdef√∂r√§ndring, dessa kallas f√∂r flags.
Aase (2011) har r√§knat upp tre principer f√∂r teknisk analys och dessa √§r: marknaden tar i
beaktande allting, priserna f√∂ljer trender och att historien brukar upprepa sig sj√§lv. Den
f√∂rsta principen, att marknaden tar i beaktande allting, betyder att allt som h√§nder, vare sig
det √§r fundamentalt, politiskt eller annat kommer det att reflekteras i priset. Den andra
principen, att priserna f√∂ljer trender, √§r grunden till teknisk analys. Marknaden har alltid en
upp√•tg√•ende, ned√•tg√•ende eller en sidl√§nges trend. Priserna f√∂ljer dessa trender tills n√•got
utomst√•ende h√§nder som p√•verkar p√• utbudet och efterfr√•gan av aktien. M√•let med att f√∂lja
trenden √§r att hoppa p√• den och sedan s√§lja n√§r man ser tecken p√• att den b√∂rjar att avta.
Den sista principen √§r att historien brukar upprepa sig sj√§lv. De som anv√§nder sig av teknisk
analys tror att investerare kommer kollektivt upprepa samma beteende som investerare f√∂re
dem, och detta leder till m√∂nster som g√•r att f√∂rutsp√•. Nyckeln till att f√∂rst√• framtiden √§r
s√•ledes g√∂md i historien.

42

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

4.1.2 Fundamental analys
En fundamental analys unders√∂ker ett f√∂retags finansiella data f√∂r att analysera om
f√∂retaget √§r √∂verprissatt eller underprissatt. Detta sker genom att analysera f√∂retagets
nuvarande och historiska fundamentala data f√∂r att se hur de har utvecklats. Utifr√•n detta
g√∂rs en prognos om f√∂retagets framtid. Enligt Aase (2011) g√∂rs denna analys i tre olika
steg: ekonomisk analys, industrianalys och f√∂retagsanalys. Om det ber√§knade v√§rdet p√•
f√∂retaget i analysen √§r h√∂gre √§n vad f√∂retaget √§r v√§rderat till p√• marknaden, anses det som
en k√∂psignal. Det motsatta intr√§ffar d√• det ber√§knade v√§rdet √§r mindre √§n vad f√∂retaget √§r
v√§rderat till p√• marknaden.
F√∂retag som √§r b√∂rsnoterade kommer varje kvartal ut med flera rapporter, och bland dessa
√§r

de

viktigaste

enligt

Aase

(2011)

balansr√§kningen,

resultatr√§kningen,

kassafl√∂desanalysen och utbetalda dividender. Den fundamentala analysen av ett f√∂retags
finansiella tillst√•nd b√∂rjar med att unders√∂ka dessa rapporter. Utifr√•n rapporterna r√§knas
olika nyckeltal ut som t.ex. skulds√§ttningsgraden, P/E (eng. price/earnings) och PEG (eng.
price/earnings to growth ratio). P/E √§r priset p√• en aktie delat med f√∂retagets vinst per aktie.
PEG motsvarar P/E, men det beaktar ocks√• den f√∂rv√§ntade tillv√§xten av f√∂retaget. PEG
anv√§nds f√∂r f√∂retag som v√§xer i snabb takt s√• att de inte verkar √∂verv√§rderade j√§mf√∂rt med
andra aktier som v√§xer i en l√•ngsammare takt.
Enligt fundamental analys kan marknaden v√§rdes√§tta en aktie fel p√• kort sikt, men detta
kommer att korrigeras p√• l√•ng sikt. Vinst kan g√∂ras d√• man investerar i en aktie som √§r
v√§rdesatt fel och sedan v√§ntar man p√• att marknaden korrigerar sig sj√§lv och ompriss√§tter
aktien.
Nilsson, Isaksson och Martikainen (2002) skriver att den fundamentala analysen kan delas
in i tre olika huvudmoment som illustreras nedan.

43

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Figur 12 Fundamentala analysens huvudmoment ur Nilsson et al (2002)
I strategisk analys analyseras enligt Nilsson et al. (2002) huvudsakligen ett f√∂retags
vinstdrivare eller framg√•ngsfaktorer. Det √§r ocks√• viktigt att identifiera riskerna med
f√∂retagets verksamhet. Slutresultatet av en strategisk analys √§r en uppskattning om
f√∂retagets framtida marknadssituation. Detta g√∂rs oftast i form av en uppskattning av den
framtida oms√§ttningen och dess framtida marknadsandelar.
F√∂ljande steg √§r redovisningsanalysen d√§r analysens syfte enligt Nilsson et al. (2002) √§r att
bed√∂ma i vilken grad som f√∂retagets redovisning avspeglar f√∂retagets underliggande
verksamhet. M√•let med analysen √§r att bed√∂ma kvaliteten p√• den redovisade resultat- och
balansr√§kningen med hj√§lp av att djupare analysera vissa poster i f√∂retaget d√§r
v√§rdes√§ttningen ger utrymme f√∂r flexibilitet. Till exempel kan man studera noggrannare
f√∂retagets avskrivningar f√∂r att f√• en inblick i om resultatet √§r onormalt h√∂gt p√• grund av
l√•ga avskrivningar som inte avspeglar den egentliga f√∂rbrukningen
F√∂ljande steg √§r redovisningsanalysen d√§r analysens syfte enligt Nilsson et al. (2002) √§r att
bed√∂ma i vilken grad som f√∂retagets redovisning avspeglar f√∂retagets underliggande
verksamhet. M√•let med analysen √§r att bed√∂ma kvaliteten p√• den redovisade resultat- och
balansr√§kningen med hj√§lp av att djupare analysera vissa poster i f√∂retaget d√§r
v√§rdes√§ttningen ger utrymme f√∂r flexibilitet. Till exempel kan man noggrannare studera
f√∂retagets avskrivningar f√∂r att f√• en inblick i om resultatet √§r onormalt h√∂gt p√• grund av
l√•ga avskrivningar som inte avspeglar den egentliga f√∂rbrukningen.
Efter dessa tre delmoment g√∂rs en prognos √∂ver f√∂retagets framtid i form av en proforma.
Proformamodellen inneb√§r enligt Nilsson et al. (2002) att man konstruerar en
44

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
prognostiserad resultat- och balansr√§kning. Proforma anv√§nds fr√§mst till att f√∂rutsp√• v√§rdet
av framtida betalningsstr√∂mmar. Den sista delen √§r sj√§lva v√§rderingen av f√∂retaget, och hit
h√∂r enligt Nilsson et al. (2002) tv√• olika ansatser, n√§mligen substansv√§rdering och
avkastningsv√§rdering. Substansv√§rderingen utg√∂r en v√§rdering av f√∂retagets befintliga
skulder samt tillg√•ngar. Avkastningsv√§rderingen g√∂rs utifr√•n de prognostiserade
avkastningsm√•tten, kassafl√∂den och residualvinster. Med hj√§lp av dessa kan f√∂retagets
framtida v√§rde uppskattas genom en nuv√§rdesber√§kning.

4.1.3 Hypotesen om effektiva marknader
Nassitroussi et al. (2014) skriver om hypotesen om effektiva marknader. Enligt denna
hypotes √§r alla marknader effektiva i att ta in all information och sedan justera aktiepriset
omedelbart upp eller ner f√∂r att reflektera det nya v√§rdet av aktien. Enligt denna hypotes √§r
det allts√• om√∂jligt att f√• h√∂gre avkastning p√• sitt kapital utan st√∂rre risker j√§mf√∂rt med
marknaden n√§r man har samma information som resten av marknaden vid
investeringsbeslutet.
Fama (1965) formulerade hypotesen om effektiva marknader 1965, men hen √§ndrade den
1970. Den nya hypotesen bestod av 3 olika niv√•er, vilka √§r stark, semistark och svag (Fama,
1970). Enligt Nassitroussi et al. (2014) kan marknadens effektivitet direkt korreleras med
hur tillg√§nglig informationen √§r. Detta betyder att all information ska vara tillg√§nglig f√∂r
alla f√∂r att en marknad ska rankas som stark effektiv. I svagt effektiva marknader √§r det
m√∂jligt att f√∂rutsp√• med god sannolikhet hur marknaden kommer att utvecklas. N√§r ny
information sl√§pps ut, absorberas den av marknaden som justerar aktiekursen enligt den
nya informationen, varefter den nya informationen blir redundant. Hur snabbt denna
information absorberas √§r dock nyckelfaktorn i om en marknad √§r starkt eller svagt effektiv.
Aase (2011) anser att ifall en marknad √§r svag finns det en m√∂jlighet att f√• h√∂gre avkastning
med hj√§lp av fundamental analys eftersom man har en chans att investera innan marknaden
r√∂r sig. Teknisk analys kommer inte att fungera eftersom en svag marknad betyder att
framtida priser inte g√•r att f√∂rutsp√• p√• l√•ng sikt med hj√§lp av analys av historiska priser. P√•
svaga marknader r√∂r sig aktiekurserna inte enligt n√•got m√∂nster utan de f√∂ljer
slumpvandring ifall det inte uppkommer n√•gon ny fundamental information. Detta betyder
45

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
enligt Aase (2011) dock inte att aktiekurserna genast skulle f√∂r√§ndras n√§r ny fundamental
information kommer ut, vilket betyder att analys av nyhetsartiklar kunde ge h√∂gre
avkastning.
Aase (2011) ber√§ttar att vid semistarka marknader √§r det inte enligt hypotesen om effektiva
marknader m√∂jligt att p√• l√•ng sikt f√• h√∂gre avkastning eftersom aktiekursen reflekterar all
ny information snabbt men opartiskt. Detta leder till att det inte √§r p√•litligt att anv√§nda
varken fundamental eller teknisk analys. Vid starka marknader s√§ger hypotesen att
aktiekursen reflekterar fullt all information och ingen kan f√• h√∂gre avkastning √§n
marknaden utom vissa f√• som r√•kar ha bra tur.
Hypotesen om effektiva marknader har dock enligt Aase (2011) f√•tt mycket kritik eftersom
den anser att alla investerare handlar rationellt. I verkligheten finns det flera kognitiva
biaser som till exempel bandwagoneffekt och konfirmeringsbias. Ett annat exempel kunde
vara Grauwe (2010) som skriver om John Kaynes djuriska instinkter vilka √§r definierade
som v√•gor av optimism och pessimism som ocks√• p√•verkar marknader orationellt.

4.1.4. Random walk-hypotesen
Schumaker och Chen (2006) skriver om random walk-hypotesen som liknar mycket den
semistarka delen av hypotesen om effektiva marknader. Enligt hypotesen √§r all information
tillg√§nglig f√∂r alla och p√• grund av detta anses det vara ineffektivt att f√∂rutsp√• framtida
kurssv√§ngningar eftersom priserna fluktuerar slumpm√§ssigt. Aase (2011) skriver att enligt
hypotesen √§r den b√§sta investeringsstrategin k√∂p och h√•ll, allts√• att passivt investera, detta
betyder att man k√∂per aktier och h√•ller dem p√• l√•ng sikt. Motsatsen till detta √§r att anv√§nda
sig av aktivt investerande d√§r man utnyttjar kurssv√§ngningar f√∂r att maximera avkastningen
med hj√§lp av att s√§lja n√§r aktien √§r √∂verprissatt och att k√∂pa n√§r den √§r underprissatt.
Aase (2011) skriver om ett experiment som gjordes f√∂r att st√∂da hypotesen. I detta
experiment ritades en graf √∂ver en akties prissv√§ngningar, var kurssv√§ngningarna
best√§mdes slumpm√§ssigt genom att singla slant. Efter att grafen ritats unders√∂kte en
analytiker grafen var hen hittade k√∂psignaler. Detta anv√§ndes som bevis p√• att
kurssv√§ngningarna √§r lika slumpm√§ssiga som att singla slant. Aase (2011) ber√§ttar att det
finns flera studier som har gjorts f√∂r att motbevisa random walk-hypotesen. Ett exempel p√•
46

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
detta √§r A non-random walk down wall street skriven av Lo och MacKinley (1999) som
inneh√•ller signifikanta empiriska motbevisa f√∂r random walk-hypotesen.

4.2. Handling med finansiella instrument med hj√§lp av
f√∂rst√§rkt inl√§rning
Detta kapitel √§r en kort genomg√•ng av tidigare forskning om handling med finansiella
medel med hj√§lp av f√∂rst√§rkt inl√§rning. Fokus ligger p√• annan forskning som har anv√§nt sig
av djupt f√∂rst√§rkt inl√§rning f√∂r att handla finansiella instrument. De finansiella
instrumenten i fr√•ga √§r valutamarknaden, index och aktier.
Jangmin et al. (2005) menar att under de senaste decennierna har flera algoritmer anv√§nts
f√∂r att handla aktier. Flera av dem har anv√§nt komplexa matematiska formler f√∂r att
evaluera aktier inf√∂r handel. Dessa matematiska formler brukar basera sig p√• antingen
teknisk analys, som behandlas i kapitel 4.1.1, p√• en fundamental analys, som behandlas i
kapitel 4.1.2. Med hj√§lp av formlerna f√∂rs√∂ker algoritmerna hitta underv√§rderade aktier
med potential f√∂r h√∂g avkastning. Det finns dock en hel del forskning som anser att det inte
√§r m√∂jligt att hitta underv√§rderade aktier, som till exempel hypotesen om effektiva
marknader som behandlas i kapitel 4.1.3. Enligt hypotesen √§r det inte m√∂jligt att f√• en h√∂gre
avkastning utan att √∂ka risken med investeringen, och att all information om marknaden
och de f√∂retag som den h√∂r till √§r tillg√§nglig f√∂r alla. Aktiekurserna anpassar sig genast n√§r
ny information kommer ut. Detta har dock inte stoppat flera fr√•n att f√∂rs√∂ka.

Azhikodan et al. (2019) har unders√∂kt om man kan anv√§nda en djupt f√∂rst√§rkt inl√§rd
algoritm f√∂r att handla aktier. Azhikodan et al. (2019) har fokuserat p√• att g√∂ra en
sentimentanalys p√• aktien genom att analysera aktiens trend med hj√§lp av ett RNN-n√§tverk.
M√•let med unders√∂kningen var att visa att en f√∂rst√§rkt inl√§rd algoritm kan l√§ra sig de
generella dragen fr√•n tr√§ningsdata som kr√§vs f√∂r att handla aktier.
Algoritmen som Azhikodan et al. (2019) skapade f√∂rs√∂ker inte handla aktier dagligen utan
m√•let √§r att den ska h√•lla aktien i n√•gra dagar. Figuren nedan illustrerar en √∂verblick av
deras algoritm:

47

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Figur 13. En √∂verblick √∂ver algoritmen ur Azhikodan et al. (2019)
Algoritmens indata √§r rubriker fr√•n nyhetsartiklar samt historiska aktiedata. Algoritmen
fattar beslut som grundar sig p√• dessa data om det l√∂nar sig att k√∂pa eller s√§lja aktien.
Azhikodan et al. (2019) tar upp tv√• exempel fall d√§r de simulerade anv√§ndningen av deras
algoritm p√• tv√• olika aktier, Google och General Electric. Algoritmen tr√§nades p√• en
m√•nads data varefter m√•let f√∂r algoritmen var att maximera portf√∂ljens v√§rde med hj√§lp av
effektiv handel av aktierna. I b√•da fallen lyckades algoritmen √∂ka v√§rdet p√• portf√∂ljen
j√§mf√∂rt med att passivt investera i aktien, men det √§r oklart hur bra denna algoritm skulle
fungera p√• andra aktier som den inte har blivit tr√§nad p√•. Azhikodan et al. (2019) menar
dock deras unders√∂kning visar att man kan l√§ra en f√∂rst√§rkt inl√§rd algoritm att handla aktier
kr√§ver vidare arbete f√∂r att f√• b√§ttre resultat.
Dai et al. (2019) har skrivit en vitbok d√§r de anv√§nde f√∂rst√§rkt inl√§rning f√∂r att handla p√•
valutamarknaden. De valde valutamarknaden p√• grund av dess h√∂ga likviditet. En h√∂g
likviditet l√§mpar sig bra till algoritmer som handlar ofta, likvida instrument har ocks√• ofta
en liten spridning (eng. spread) som g√∂r att man tappar endast lite vinster p√• grund av
slirande ( eng. slippage). Spridningen √§r skillnaden mellan k√∂p- och s√§ljkursen av aktien
vid en viss tidpunkt. Aktier som handlas aktivt har ofta en liten spridning. Slirande har att
g√∂ra med m√§ngden aktier som erbjuds att s√§lja eller k√∂pa vid en viss tidpunk. Till exempel,
om man vill k√∂pa 100 st aktier av Nokia Oyj, och det finns ett bud p√• endast 50 st aktier
till priser 3,56, kan man endast k√∂pa 50 st aktier av den f√∂rsta f√∂rs√§ljaren och 50 st
resterande aktier k√∂ps av n√§sta s√§ljare, till exempel till priset 3,57. D√• blir medelpriset av
uppk√∂pet 3,565 fast√§n det b√§sta budet var vid k√∂ptillf√§llet 3,56. Slirande och spridningen
48

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
p√•verkar investeraren s√•ledes endast n√§r man vill k√∂pa eller s√§lja aktier. Detta betyder att
det √§r viktigare f√∂r aktiv investerare att ha en l√•g spridning och lite slirande √§n en passiv
investerare.
Dai et al. (2019) anv√§nder sig av data f√∂r tre olika valutapar p√• sekund niv√•. De tr√§nar f√∂rst
algoritmen p√• 3 veckor v√§rt av data och sedan testar de modellen p√• de tv√• f√∂ljande dagarna
efter tr√§ningsperioden. De ber√§ttar att de fokuserar sig endast p√• att optimera ett av dessa
valutapar, s√•ledes √§r resultatet av de tv√• andra paren en bra indikator hur bra algoritmen √§r
att generalisera.
Algoritmen har tre olika m√∂jligheter, den satsar antingen p√• att valutans v√§rde kommer att
g√• ner, upp eller att den √§r of√∂r√§ndrad (1 /0/-1). Vid varje tillst√•nd anv√§nder de sig av en
timme v√§rt av historiska data som indata. I b√∂rjan anv√§nder de sig av en relativt stor
uppspelnings buffert men mot slutet av tr√§ningen minskar de p√• storleken av den f√∂r att
prioritera nyligen intr√§ffade h√§ndelser.
Slutsatsen av forskningen gjort av Dai et al. (2019) var att deras djupt f√∂rst√§rkt inl√§rda
algoritm inte lyckades hitta en optimal l√∂sning. Algoritmen l√§rde sig att v√§lja en neutral
position i de flesta fall, allts√• att den anser att valutaparets v√§rde skulle f√∂rbli of√∂r√§ndrat.
De anser att orsaken till detta √§r att algoritmen har sv√•rt att hitta bra s√§rdrag fr√•n deras
tr√§ningsdata som skulle hj√§lpa algoritmen att fatta bra handelsbeslut.
Deng et al. (2016) har ocks√• f√∂rs√∂kt tackla detta problem. I deras unders√∂kning har de som
de tidigare forskningarna i detta kapitel anv√§nt sig av f√∂rst√§rkt inl√§rning kopplat med djup
inl√§rning. Problemet som de har fokuserat sig p√• √§r hur man ska summera situationen p√•
den finansiella marknaden p√• ett s√§tt att algoritmen kan tolka det och anv√§nda sig av det
f√∂r att fattahandelsbeslut. R√• finansiella data klassas enligt Deng et al. (2016) att inneh√•lla
mycket st√∂rningar som kan negativt p√•verka hur algoritmen kan l√§ra sig fr√•n dessa indata.
F√∂r att motk√§mpa detta brukar man anv√§nda olika finansiell indikatorer som till exempel
glidande medelv√§rde f√∂r att klarg√∂ra trenden av ett finansiellt instrument.
Deng et al. (2016) menar att till exempel glidande medelv√§rde √§r ett bra s√§tt att klarg√∂ra
trenden av ett instrument men det √§r ofta en d√•lig indikator eftersom den reagerar inte

49

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
tillr√§ckligt snabbt vid trendbyte. Detta leder till att algoritmen fattar d√•liga handelsbeslut.
P√• grund av detta har man valt att anv√§nda r√• finansiella data trots dess nackdelar.
Deng et al. (2016) fokuserar p√• S&P 500-indexet i sin unders√∂kning. Deras indata √§r
dagliga data av S&P 500-indexet. Till skillnad fr√•n andra liknande unders√∂kningar,
best√§mde de sig ocks√• f√∂r att anv√§nda data fr√•n andra stora index runtom i v√§rlden som
indata. Orsaken till detta √§r att de anser att S&P 500-indexet √§r starkt p√•verkat av
sv√§ngningar p√• den globala marknaden. Till exempel om de andra indexen i v√§rlden miskan
i v√§rde, kommer s&p 500-indexet s√§kert ocks√• att f√∂lja med.

Tabell 3. Tabellen visar hur m√§ngden lager och m√§ngden noder p√•verkar vinsten.
TP √§r ‚ÄúTotal profit‚Äù vilket betyder ‚Äúden totala vinsten‚Äù. Ur Deng et al. (2016)
Tabell 3 visar att Deng et al. (2016) har unders√∂kt hur storleken och formen p√• det neurala
n√§tverket som anv√§nds i deras f√∂rst√§rkta inl√§rda algoritm p√•verkar resultatet. Slutsatsen √§r
att m√§ngden lager och noder tycks korrelera positivt med den totala vinsten. Den st√∂rsta
vinsten f√•s med ett n√§tverk med 5 lager som har 256 noder var.
Det viktigaste slutsatserna ur Deng et al. (2016) √§r:
1. R√• aktiedata kan r√§cka, och √§r i vissa fall √§r det b√§ttre √§n att anv√§nda f√§rdiga
tekniska indikatorer som glidande medelv√§rde. Det √§r b√§ttre att l√•ta algoritmen sj√§lv
hitta s√§rdragen att som den f√∂ljer.
2. Storleken p√• n√§tverket kan korrelera positivt med vinsterna.

50

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Huang (2018) har unders√∂kt ett motsvarande problem som behandlas i denna avhandling.
Huang anv√§nder sig likt Dai et al. (2019) av valutamarknaden som k√§lla av tr√§ningsdata.
Valutaparenas data √§r h√§mtad fr√•n 2012 - 2017 i 15 minuters intervall. Fokus i
avhandlingen av Huang (2018) ligger p√• att f√∂rs√∂ka g√∂ra en handels algoritm som f√∂rs√∂ker
handla diverse valutapar.
Likt Dai et al. (2019) och Azhikodan et al. (2019) anv√§nder Huang (2019) ocks√• av LSTM
n√§tverk. Huang (2019) anv√§nder sig av tre lager av 256 noder i LSTM lager, fj√§rde lagret
√§r helt kopplat (eng. Fully connected) med bara 3 noder. De sista tre noderna motsvarar de
tre olika handlingsalternativen som algoritmen har.
Var Huang (2019) skiljer sig fr√•n de andra √§r med tv√• klara f√∂rb√§ttringar till strukturen av
handelsalgoritmen.
1. I vanliga fall brukar f√∂rst√§rkt inl√§rda algoritmer f√• deras bel√∂ning efter ett antal
h√§ndelser, eller i slutet av episoden. Detta motsvarar schack exemplet som
behandlas i kapitel 3.3. Huang (2019) har kommit fram till att det √§r b√§ttre att ge
en bel√∂ning till algoritmen oftare n√§r man anv√§nder sig av finansiell data som aktier
eller valutapar. Enligt Huang l√§r sig algoritmen snabbare och b√§ttre om man ger en
bel√∂ning efter varje handelsbeslut. T.ex. +10 f√∂r en bra handling och -10 f√∂r en
d√•lig.
2. Uppspelningsminnet √§r relativt litet j√§mf√∂rt med de andra f√∂rst√§rkt inl√§rda
arkitekturer. Detta hj√§lper algoritmen att fokusera sin tr√§ning endast p√• de senaste
h√§ndelserna.
Du et al. (2016) och Berotoluzzo & Corazza (2012) har ocks√• forskat inom detta √§mne.
Forskningen av Du et al. (2016) har fokuserat p√• att optimera ens portf√∂lj med hj√§lp av en
f√∂rst√§rkt inl√§rd algoritm. Algoritmens m√•l √§r att balansera portf√∂lj med i h√§nsyn till Capital
Asset Pricing Model-teorin (CAPM). CAPM tar i enligt Fama & French (2004) i beaktande
sambandet mellan risken och avkastningen av investeringen. Berotoluzzo & Corazza
(2012) har f√∂rs√∂kt med hj√§lp av en f√∂rst√§rkt inl√§rd algoritm att f√∂rutsp√• det framtida v√§rdet
av Banca Intensa som √§r en aktie p√• en Italienska aktiemarknaden.

51

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Det som binder Du et al. (2016) och Berotoluzzo & Corazza (2012) ihop √§r att de har b√•da
anv√§nt sig av sharpekvoten (eng. Sharpe ratio) f√∂r att r√§kna ut bel√∂ningen f√∂r en handling.
Orsaken varf√∂r sharpekvoten anv√§nds √§r f√∂r att ta i beaktande risken av investeringen n√§r
man r√§knar v√§rdet av avkastningen. En h√∂g avkastning √§r inte alltid det b√§sta alternativet
ifall risken i investeringen √§r stor.

52

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

5. PORTFOLIO OPTIMERING OCH AKTIEHANDEL MED
HJ√ÑLP AV F√ñRST√ÑRKT INL√ÑRNING
F√∂r att svara p√• avhandlingens forskningsfr√•gor har jag l√§rt en f√∂rst√§rk inl√§rd algoritm att
analysera aktier baserat p√• n antal historiska aktiedata. Detta kapitel kommer att f√∂rklara
hur denna algoritm √§r uppbyggd, hur den √§r tr√§nad och vad resultaten √§r i h√§nsyn till
forskningsfr√•gorna. Algoritmen √§r skriven i Python. L√§nk till den fullst√§ndiga koden finns
som bilaga.
F√∂r att uppn√• en optimal modell anv√§nde jag mig av Jenkins modell, mer noggrant stegen
5,6,7 vika var Experimental design, Data capture och Data analysis. Jag b√∂rjade med en
m√∂jlig design av algoritmen, tr√§nade den och testade den, och till slut analyserade jag
resultaten. P√• basen av denna analys gjorde jag f√∂r√§ndringar till algoritmen och milj√∂n. Jag
itererade ofta √∂ver dessa tre steg. Till exempel jobbade jag mycket med att optimera
frekvensen av bel√∂ningen och storleken av bel√∂ningen.
I b√∂rjan fick algoritmen en bel√∂ning f√∂rst efter att den k√∂rt igenom alla aktier en g√•ng, f√∂r
att f√∂rb√§ttra resultaten √§ndrade jag bel√∂ningsfrekvensen s√• att agenten fick en bel√∂ning efter
att den k√∂rt igenom en aktie. Efter varje f√∂rb√§ttring blev resultaten sakta men s√§kert b√§ttre,
men jag m√§rkte dock ocks√• att algoritmen blev mycket mera f√∂rsiktig med sina handlingar.
D√• jag gav en bel√∂ning f√∂rst efter att algoritmen k√∂rt igenom en hel aktie handlade
algoritmen mera men variansen i resultaten var mycket h√∂gre. F√∂r vissa aktier kunde
algoritmen vinna referensportf√∂ljen med 100% men i andra fall f√∂rlorade den mot
referensportf√∂ljen med 80%. Efter detta blev jag inspirerad av Huang (2019) och jag valde
att ge en bel√∂ning efter varje steg. Variansen av resultaten minskade, och i medeltal hade
algoritmen en h√∂gre vinstprocent j√§mf√∂rt med referensportf√∂ljen √§n med de tidigare
bel√∂ningsmetoderna.
I kapitel 2 n√§mndes att test metodologin som anv√§ndas baserar sig p√• Shmueli & Koppious
(2011) forskning om empiriska modeller f√∂r prognoser. Shmueli & Koppious (2011) anser
att man kan testa modellen p√• framtida observationer eller p√• andra observationer som inte
var i test datan. De slutliga modellerna testades p√• sex olika set av testdata, b√•de framtida
observationer och andra observationer p√• samma tidsperiod.
53

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

5.1 Aktie handlings beslut med djup q-inl√§rning
Strukturen av algoritmen motsvarar markovs-beslutsprocess ur Sutton och Barto (2018).
Agenten f√•r som indata ett tillst√•nd och bel√∂ningen f√∂r f√∂rra handlingen i f√∂rra tillst√•ndet.
Med denna information best√§mmer agenten ifall den ska k√∂pa, s√§lja eller g√∂ra ingenting
vid detta tillst√•nd.

Figur 13. Strukturen av den f√∂rst√§rk inl√§rda handelsalgoritmen.
Bel√∂ningen av algoritmen tar inspiration fr√•n Huang (2019). Huang ger en bel√∂ning till
agenten efter varje handling. Bel√∂ningen r√§knas ut med denna formel:
Bel√∂ning = (Skillnaden mellan referensportf√∂ljens v√§rde och agentportf√∂ljens
v√§rde vid det nuvarande tillst√•ndet) - (Skillnaden mellan referensportf√∂ljens v√§rde
och agentportf√∂ljens v√§rde vid det f√∂rra tillst√•ndet)
Storleken p√• bel√∂ningen √§r s√•ledes direkt kopplad till √∂kningen eller minskningen av
skillnaden mellan referensportf√∂ljens v√§rde j√§mf√∂rt med agentportf√∂ljens v√§rde. Id√©n
bakom detta √§r att agenten ska inte bara l√§ra sig vilka handlingar som √§r bra och d√•liga utan
ocks√• hur den ska v√§rdes√§tta olika handlingar.
Referensportf√∂ljen motsvarar en passiv investering. Detta betyder att i b√∂rjan av tr√§ningen
k√∂per referensportf√∂ljen aktier med alla dess likvida medel och s√§ljer dem inte under
simulationen. Id√©n med referensportf√∂ljen blir d√• att simulera aktiens v√§rde√∂kning om man
inte handlar aktivt med aktien. M√•let med algoritmen √§r att f√• en h√∂gre avkastning p√•
samma aktier j√§mf√∂rt med en passiv investering.
54

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Varje tillst√•nd best√•r av en m√§ngd n aktiedata p√• daglig niv√•. Indata √§r en tabell med fem
kolumner och n antal rader som representerar de olika dagliga observationerna. Innan
dessa data skickas till agenten normaliseras tabellen f√∂r att snabba upp tr√§ningen, och f√∂r
att g√∂ra de olika tabellerna oberoende av varandra. Normaliseringen g√∂r att skillnaden
mellan de olika dagarna √§r relativa, inte absoluta, i f√∂rh√•llande till de andra dagarna i
tabellen. Tabellerna normaliseras med min ‚éØ max-normalisation
Efter att agenten fattat ett beslut skickas handlingen till milj√∂n. Om agenten skickar en 0
betyder det att aktien √§r svag och alla aktier ska s√§ljas bort. Vid svar 1 h√§nder ingenting,
och vid svar 2 k√∂per agenten aktier f√∂r alla sina oanv√§nda medel. I tr√§ningsfasen kan
agenten kan inte k√∂pa bara n√•gra aktier eller s√§lja bara en del, utan portf√∂ljens alla medel
√§r investerade i aktier eller inga medel √§r investerade i aktier.
Agenten kan inte heller blanka. Blankning anv√§nds f√∂r att g√∂ra vinst p√• en sjunkande
marknad genom att s√§lja aktier man inte √§ger. Aktier l√•nas f√∂r detta √§ndam√•l av en
aktief√∂rmedlare. Vid ett senare skede k√∂ps aktierna tillbaka efter att kursen har sjunkit och
man returnerar aktierna till aktief√∂rmedlaren. Som exempel, 1st aktie av Nokia Oyj s√§ljs
f√∂r 3,65 och k√∂ps tillbaka f√∂r 3,55.
Medan algoritmen tr√§nas g√∂rs n√•gra f√∂renklingar till aktiehandeln f√∂r att underl√§tta
unders√∂kningen. P√• grund av detta kommer tr√§ningens resultat inte motsvara hur
algoritmen skulle fungera om den skulle handla med riktiga aktier. M√•let med simulationen
√§r att endast ge en riktgivande bild √∂ver handelsalgoritmens prestation.

F√∂ljande f√∂renklingar har gjorts:
1: K√∂pandet eller s√§ljandet av aktier sker p√• den sista minuten n√§r b√∂rsen h√•ller p√•
att st√§nga, i simulationen k√∂ps / s√§ljs aktierna till det sista priset som aktien handlats
f√∂r en viss dag. I verkligheten √§r det v√§ldigt sv√•rt att handla f√∂r detta pris, men
antagandet √§r att nu och d√• f√•r agenten ett b√§ttre/s√§mre pris f√∂r aktien och de j√§mnas
ut till slut. Detta betyder ocks√• att slirande inte heller tas i beaktande.
2: I tr√§ningen eller testandet tas inte heller i beaktande n√•gon transaktionsavgift
eftersom flera aktief√∂rmedlare ger gratis aktief√∂rmedling p√• den amerikanska
55

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
aktiemarknaden. T.ex TD Ameritrade, E*TRADE, Interactive brokers och Alpaca
Markets ger med varierande modeller gratis aktief√∂rmedling.

5.3 Tr√§ning och test
I detta kapitel redog√∂r jag √∂ver den testmetologin jag anv√§nt och exempel p√• misslyckade
modeller och √•tg√§rder jag gjort f√∂r att uppn√• en optimal modell. Otaliga modeller har
tr√§nats f√∂r att hitta en kombination av de m√∂jliga variablerna som skulle leda till det b√§sta
resultatet. Att tr√§na en modell tar cirka 14 ‚Äì 24 timmar. Det finns flera variabler som
p√•verkar hur snabbt en modell kan tr√§nas, nedan finns n√•gra av dessa variabler listade:
‚óè Storleken p√• neurala n√§tverket. Ett st√∂rre n√§tverk tar √∂verlag ofta en l√§ngre
tid att tr√§na.
‚óè Typen av neurala n√§tverket. M√§ngden och komplexiteten av ber√§kningarna
bakom de olika typerna av neurala n√§tverk som testades skiljer sig fr√•n
varann.
‚óè Inl√§rningshastigheten Œ±. H√∂gre Œ± leder till en kortare tr√§ningstid med en f√∂r
stor Œ± kan leda till att ingen optimal l√∂sning hittas.
‚óè H√•rdvara. H√•rdvaran av dator p√•verkar tr√§ningstiden drastiskt. Ett
grafikkort (GPU) √§r ett m√•ste f√∂r att tr√§na motsvarande algoritmer i en
rimlig tid. Snabbare grafikkort med mera inbyggt minne leder i de flesta fall
till en kortare tr√§ningstid. I bilagorna finns en beskrivning p√• den dator som
anv√§ndes f√∂r att tr√§na modellerna i denna avhandling.
Modellerna testas p√• S&P 500-aktier som inte h√∂r till S&P 100. S&P 100-aktierna h√∂r till
tr√§ningsdata, vilket g√∂r att de m√•ste tas bort fr√•n testdata. Testdata inneh√•ller cirka 400
aktier efter raderingen av S&P 100 aktierna fr√•n S&P 500 aktierna.
Tr√§ningen kommer att ske p√• dagliga data av S&P 100 aktier fr√•n 2017-01-03 till 2019-0430. Alla modeller som kommer att testas p√• sex olika set av testdata:
Testset 1: Aktierna ABMD - FLT. Tidsperiod: 2017-01-03 till 2019-04-30
Testset 2: Aktierna FLIR- PFG. Tidsperiod: 2017-01-03 till 2019-04-30
Testset 3: Aktierna PGR- ZTS. Tidsperiod: 2017-01-03 till 2019-04-30

56

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Testset 1 framtid: Aktierna ABMD - FLT. Tidsperiod: 2019-05-01 till 2020-03-20
Testset 2 framtid: Aktierna FLIR- PFG. Tidsperiod: 2019-05-01 till 2020-03-20
Testset 3 framtid: PGR- ZTS. Tidsperiod: 2019-05-01 till 2020-03-20

Tanken bakom de olika testsetten var att f√∂rs√∂ka hur algoritmen fungerar p√• olika typer av
marknader. Den f√∂rsta och l√§ngre tidsperioden var s√• gott som bara en stigande marknad,
ett undantag var i slutet av 2018 d√• aktierna sj√∂nk p√• grund av r√§dsla √∂ver ett handelskrig
mellan USA och Kina. Den andra tidsperioden valdes f√∂r att den inneh√•ller en skarp
kursnedg√•ng i mars 2020 d√• r√§dslan √∂ver Covid-19 b√∂rjade. En generaliserad algoritm
borde fungera bra p√• en stigande och en sjunkande marknad.
Aktierna delades ocks√• upp i tre olika grupper p√• b√•da tidsperioderna f√∂r att underl√§tta
tr√§ningen och f√∂r att j√§mf√∂ra om algoritmen fungera b√§st p√• enstaka aktier eller om
resultaten blir j√§mt f√∂rdelade mellan alla aktier. Flera olika iterationer av modellerna har
gjorts men deras resultat redovisas inte h√§r. Nedan √§r exempel p√• testmetologin som har
anv√§nts f√∂r att analysera modellerna.

Tabellerna 4 - 9 ovan visar resultatet av ett tr√§ningspass av MLP modeller. H√§r
analyseras hur m√§ngden indata (i dagar) och n√§tverkets storlek p√•verkar
resultaten. Procenten beskriver hur modellen klarar sig j√§mf√∂rt med en passiv
investerare.

57

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Modellerna som analyserades i tabellerna inneh√•ller inte n√•gon optimal l√∂sning. Resultaten
√§r relativ stokastiska men detta gav mig en inblick i hur jag ska √§ndra tr√§ningen f√∂r att
uppn√• b√§ttre resultat. De st√∂rsta problemen i modellerna ovan √§r att s√• gott som alla har bra
resultat p√• antingen den f√∂rsta eller den andra tidsperioden, inte b√•da. Vidare analys av
dessa resultat visar dock att vidare fortsatt kr√§vs f√∂r att uppn√• en optimal modell.
Vidare analys visar att det uppkommer tv√• stora problem fr√•n modellerna ovan, vilket g√∂r
att jag har tv√• problem jag m√•ste l√∂sa. Problem 1 √§r att vissa modeller har best√§mt sig f√∂r
att k√∂pa bara aktien hela tiden, vilket leder till att vinsten √∂ver passiv investering √§r n√§ra 0.
Flera av 64x64 modellerna har detta problem. Nedan finns en visualisering p√• problem 1.

Graf 1: Visualisering av det f√∂rsta problemet. Aktien i grafen √§r AVGO, testset 1
framtid. Modell 64x64, 20 dagar historiska data. Algoritmen f√∂ljer bara
aktiekursen, vilket leder till ingen extra vinst.
Det andra problemet, √§r att algoritmen inte k√∂per aktien en enda g√•ng. Denna ‚Äústrategi‚Äù
fungerar bra p√• den i de framtida testsetten eftersom aktiekursen faller mot slutet. S√• gott
som alla modellerna som har +30% avkastning p√• de framtida testsetten hamnar ut f√∂r detta.
Nedan finns ett exempel p√• detta.

58

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Graf 2: Visualisering av det andra problemet. Aktien i grafen √§r AVGO, testset 1
framtid. Modell 128x128, 30 dagar historiska data. Algoritmen handlar inte en
enda g√•ng. Resultaten ser bra ut men i verkligheten fungerar inte algoritmen.

En optimal modell borde kunna ha positiv avkastning p√• b√•da tidsperioderna. Den enda
modellen av de som analyserades ovan som lyckades med detta √§r 256x256 och 128x128
med 20 dagar historiska data. De har bra resultat p√• testset 1-3 men p√• testset 1-3 framtid
lyckas de inte handla optimalt. P√• framtida testsetten √§r avkastningen v√§ldigt liten eftersom
den f√∂ljer dels det f√∂rsta problemet.
F√∂r att f√• en optimal modell har jag √§ndrat p√• n√•gra variabler som har lett till att modellen
lyckas handla b√§ttre p√• alla tidsserier och aktier. Jag h√∂jde ‚ÑΩ ( diskonteringen ) till 0.99 fr√•n
0.95 och jag l√§mnade bort dropout-lagren i mina modeller. Dropout-lagren anv√§nds f√∂r att
minska p√• chansen i ett √∂vervakat inl√§rningsproblem att modellen ska l√§ra sig datan utantill.
Dessa lager tycks dock ha en negativ inverkan p√• resultaten i ett f√∂rst√§rkt
inl√§rningsproblem. P√• grund av tidsbrist har jag inte tr√§nat lika m√•nga modeller med de
nya inst√§llningarna.

59

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Tabell 10 och 11 ovan visar resultaten av tr√§ningen efter de √§ndringar som
n√§mndes ovan. Procenten beskriver hur modellen klarar sig j√§mf√∂rt med en passiv
investerare.

√Ñndringarna i inl√§rningen har lett till b√§ttre resultat, i alla fall f√∂r MLP-modellerna.
256x256 MLP-modellen med 20 och 30 dagar som indata kommer att vara de modeller jag
kommer att testa i min portf√∂ljsimulation. Modellerna som anv√§nde sig av LSTM och CNN
fungerade inte optimalt. CNN-modellen l√§rde sig endast att k√∂pa och h√•lla hela tiden,
d√§rf√∂r √§r skillnaden 0% mellan den och den passiva investeraren. Detta motsvarar det f√∂rsta
problemet fr√•n tidigare.

Graf 3: Visualisering LSTM modellen efter f√∂rb√§ttringen i inl√§rningen. Aktien i
grafen √§r AVGO, testset 1. Modell 256x256 LSTM, 30 dagar historisk data.
60

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Graf 4: Visualisering LSTM modellen efter f√∂rb√§ttringen i inl√§rningen. Aktien i
grafen √§r AVGO, testset 1 framtid. Modell 256x256 LSTM, 30 dagar historisk data.

LSTM-modellen jag testade tycks inte ha samma problem som modellerna tidigare hade
d√§r de inte handlade en enda g√•ng. Denna f√∂rsiktiga modell tycks dock endast fungera bra
i sjunkande marknader men d√•ligt vid stigande marknader eftersom den √§r lite f√∂r f√∂rsiktig
att handla.

61

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Graferna 5 och 6 visar f√∂rdelningen av vinst (och f√∂rlust) f√∂r enskilda aktier j√§mf√∂rt
med en passiv investering. B√•da graferna visualiserar 256x256 MLP-modellen med
30 dagar som indata. Graf 5 √§r testset 2 och graf 6 √§r testset 2 framtid. Majoriteten
av aktierna har en positiv avkastning men det finns i b√•da testset flera aktier som
klarar sig s√§mre j√§mf√∂rt med en passiv investering. Som bilaga 5 finns en
motsvarande st√∂rre grafer d√§r varje enskilda akties resultat syns.

Graferna ovan visar att √§ven den b√§sta modellen, i detta fall 256x256 MLP-modellen med
30 dagar som indata klarar inte av att vinna en passiv investerare i varje aktie. Den slutliga
avkastningen √§r dock positiv eftersom majoriteten av aktierna klarar sig b√§ttre √§n en passiv
investering och vinsten av de b√§sta aktierna √§r st√∂rre √§n f√∂rlusten av de v√§rsta.

5.4 Portf√∂ljsimulation
I portf√∂ljsimulationen √§r det meningen att simulera hur algoritmen skulle klara sig att
optimera avkastningen av en portfolio. Denna simulation motsvarar mycket det tidigare
testandet men algoritmen k√∂rs igenom p√• en l√§ngre tidsperiod. Aktierna delas upp p√•
samma s√§tt som tidigare till testset 1-3 men den f√∂rsta halvan och andra halva √§r slagna
ihop. Detta betyder att t.ex. testset 1 √§r: Aktierna ABMD - FLT. Tidsperiod: 2017-01-03
till 2020-03-20. I denna simulation anv√§nds de b√•da b√§sta MLP modellerna. De √§r annars
identiska men den andra f√•r 30 dagar som indata medan den andra f√•r 20 dagar.
Tv√• olika s√§tt att simulera en portfolio testades. I det f√∂rsta alternativet har varje aktie i
b√∂rjan 10 000 dollar allokerad till den och algoritmen ska best√§mma ifall denna 10 000 ska
62

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
anv√§ndas f√∂r att k√∂pa aktien med all dess likvida medel, g√∂ra ingenting eller att s√§lja allting.
Denna samma summa √§r allokerad till en viss aktie genom hela simulationen. Till exempel,
ifall algoritmen i ett senare skede s√§ljer 15 000 dollar v√§rt av AVGO f√∂rblir denna 15 000
allokerad som cash till AVGO tills algoritmen best√§mmer sig att k√∂pa aktien igen. Detta
leder dock i teorin till en algoritm som anv√§nder sina medel relativ ineffektivt.
I verkligheten skulle en portf√∂lj inte ha en viss summa allokerad f√∂r en aktie. Vanligtvis
kan en portf√∂ljs medel allokeras hur som helst runt i portf√∂ljen. Detta f√∂rs√∂ker det andra
alternativet l√∂sa. B√•da alternativen √§r dock bundna till de f√∂renklingar som beskrivs p√• sida
s.60 vilket g√∂r att dessa resultat inte kan j√§mf√∂ras med verklig avkastning.

Tabell 12 √∂ver resultaten av portf√∂ljsimulation, alternativ 1.

Graf 7, resultaten av portf√∂ljsimulation, alternativ 1. Modell 256x256 MLP, 30
dagar indata, testset 1. Slutresultatet var en avkastning p√• 4,9% mer √§n en passiv
investering.
I det f√∂rsta alternativet lyckas algoritmen f√• en positiv avkastning. Detta f√∂rv√§ntades
eftersom samma modell fick positiv avkastning i testfasen ocks√• och logiken bakom
63

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
k√∂pandet √§r samma. Modellen √§r dock l√•ngt ifr√•n optimal. Modellen lyckas f√• ett litet
f√∂rspr√•ng j√§mf√∂rt med den passiva investeraren vid dippen som h√§nde i √•rsskiftet mellan
2018 och 2019, men inget annat speciellt h√§nder.
I alternativ tv√• kan algoritmen handla med alla medel i portf√∂ljen, id√©n bakom detta var att
denna l√∂sning skulle leda till en mer effektiv anv√§ndning av kapitalet. I simulationen m√§rks
dock algoritmen klarade sig inte emot en passiv investerare. Jag tror att st√∂rsta orsaken till
detta var att i f√∂rsta alternativet kunde algoritmen b√§ttre satsa p√• de f√∂retag som h√§mtade
de hem den st√∂rsta avkastningen. Problemen med det f√∂rsta alternativet √§r dock att ifall
algoritmen best√§mde sig f√∂r att likvidera allting s√• skulle pengarna bara sitta och g√∂ra
ingenting till algoritmen best√§mmer sig att k√∂pa samma aktie igen. Det andra alternativet
f√∂rs√∂ker l√∂sa denna ineffektivitet genom att effektivare investera de oanv√§nda medlen.
I det andra alternativet programmerades algoritmen att j√§mt dela ut ens medel mellan de
aktier som den ans√•g att skulle stiga i v√§rde. Efter att en aktie likviderades n√§r algoritmen
best√§mde sig f√∂r att s√§lja allting delas dessa medel j√§mt ut till de andra aktier som aktien
ans√•g att skulle stiga i v√§rde. Detta ledde till en mer balanserad portf√∂lj, men f√∂r att vinna
√∂ver den passiva investeraren borde den kunna allokera medlen smartare mellan de olika
aktierna. Till exempel borde den kunna satsa mera i aktier som den kommer att ge en h√∂gre
avkastning √§n de andra. I detta fall var dock alla stigande aktier likv√§rdiga i algoritmens
√∂gon.
Nedan finns en graf som visualiserar resultaten av det andra alternativet. P√• grund av de
d√•liga resultaten testades alternativ inte med andra modeller. F√∂r fortsatta studier anser jag
att den b√§sta l√∂sningen √§r en kombination av det f√∂rsta och andra alternativet. En l√∂sning
som anv√§nder sig av det f√∂rsta alternativets s√§tt att ha en h√∂gre allokering f√∂r de aktier som
h√§mtar hem den st√∂rsta avkastningen och den andra alternativets s√§tt att mer effektivt
allokera de oanv√§nda medlen vore en bra kombination.

64

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Graf 8 √∂ver resultaten av portf√∂ljsimulation, alternativ 2. Modell 256x256 MLP, 30
dagar indata, testset 1. Slutresultatet va en avkastning p√• -8,6%.

65

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

6. SAMMANFATTNING OCH DISKUSSION
Att f√∂rutsp√• en akties prisutveckling √§r enligt den allm√§nt accepterade teorin inte m√∂jligt.
Hypotesen om effektiva marknader anser att marknaden √§r s√• effektiv att det inte g√•r att f√•
h√∂gre avkastning p√• ens investeringar utan att √∂ka risken. Random walk-teorin anser att
aktierna r√∂r sig stokastiskt och att det inte s√•ledes g√•r att f√∂rutsp√• en akties framtida pris.
Den f√∂rsta forskningsfr√•gan var: Kan en f√∂rst√§rkt inl√§rd algoritm l√§ra sig att handla aktier
p√• ett s√§tt som √∂kar avkastningen av aktien j√§mf√∂rt med ens egna prisutveckling p√•
aktiemarknaden? Svaret p√• detta utifr√•n denna avhandling √§r: ja, i vissa fall. Modellerna i
denna avhandling testades p√• cirka 400 aktier och tidsperioderna var r√§tt s√• sn√§v. DQNalgoritmen med en MLP lyckades att vinna en passiv investerare i alla sex test set, men
marginalen var dock inte stor. Avhandlingen har gjort flera f√∂renklingar som beskrivs i
kapitel 5.1, en noggrannare simulation som tar i beaktande dessa f√∂renklingar kan komma
till andra slutsatser. Ifall simulationen skulle ta i beaktande slirandet och spridningen
kommer en stor del om inte hela vinsten sm√§lta bort. Jag anser dock att det finns mycket
man kan g√∂ra f√∂r att f√∂rb√§ttra resultaten. Dessa f√∂rb√§ttringsf√∂rslag behandlas noggrannare
i n√§sta kapitel.
Avhandlingens andra forskningsfr√•ga var: Kan en f√∂rst√§rkt inl√§rd algoritm √∂ka
avkastningen av en portfolio j√§mf√∂rt med en passiv investering? Svar p√• denna fr√•ga √§r
samma som till den f√∂rsta fr√•gan: ja, i vissa fall. Simulationen gjord i denna avhandling har
lyckats f√• en h√∂gre avkastning p√• ens investeringar j√§mf√∂rt med en passiv investerare. Dock
eftersom flera f√∂renklingar gjorts till simulationen kan dessa resultat inte j√§mf√∂ras med
verkligheten. Jag anser dock att avhandlingen bevisar att det √§r under vissa omst√§ndigheter
m√∂jligt att √∂ka ens avkastning med hj√§lp av en f√∂rst√§rkt inl√§rd algoritm. √Ñmnet kr√§ver
enligt mig fortsatt forskning f√∂r att komma fram till en optimal l√∂sning. Ifall denna positiva
avkastning √§r tillr√§ckligt h√∂g att det √∂verv√§ger risken av att ha en algoritm att g√∂ra ens
handelsbeslut √§r en sv√•rare fr√•ga.
Det sv√•raste med att tr√§na modellerna var att hitta en modell som skulle fungera bra p√•
marknader som stiger och sjunker. Detta var en del av orsaken varf√∂r jag valde tv√• olika
tidpunkter f√∂r testdata. Den f√∂rsta delen var till en stor del en stigande marknad,
66

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
marknaderna sj√∂nk en aning i slutet av 2018 men annars var trenden upp√•t. Den andra delen
inneh√•ller b√∂rjan av den senaste b√∂rskraschen d√§r aktiernas v√§rde f√∂ll drastiskt fr√•n och
med mars 2020. Flera av de modellerna som jag tr√§nade i b√∂rjan fungerade bra p√• antingen
den f√∂rsta eller den andra delen. P√• grund av detta har jag kommit till den slutsatsen att den
skulle l√∂na sig att tr√§na flera modeller. Minst tv√• stycken, en som fungerar bra i en stigande
marknad som k√∂per alla sm√• sv√§ngningar ned√•t och en annan som fungerar bra i sjunkande
marknader som s√§ljer vid svaghet.
En annan stor f√∂rb√§ttring vore att tr√§na modeller t.ex. p√• basen av industrin som de ligger
i. Aktierna i ICT-sektorn har under det senaste decenniet r√∂rt sig p√• ett annat s√§tt j√§mf√∂rt
med en andra mer etablerade industrier, som t.ex. energi. En √§nnu b√§ttre l√∂sning kunde vara
att tr√§na en modell endast p√• en aktie, d√• skulle modellen l√§ra sig att k√§nna till hur en
specifik aktie r√∂r sig. Ett av m√•len i denna avhandling var att l√§ra en algoritm att k√§nna
igen generella drag ur alla de 100 aktier som algoritmen tr√§nades p√•. Resultatet av denna
avhandling visar att ja det finns generella drag som g√∂r att algoritmen kan l√§ra sig, men de
√§r kanske inte s√• starka eller allm√§nna som skulle leda till avsev√§rt h√∂ga vinster.
Jag anser ocks√• att LSTM-n√§tverk borde fungera b√§ttre √§n MLP i f√∂rst√§rkt inl√§rda problem
med tidsseriedata, men mitt f√∂rs√∂k med dessa n√§tverk fungera inte optimalt. Flera andra
forskare som t.ex. Dai et al. (2019), Azhikodan et al. (2019) och Huang (2019) har anv√§nt
LSTM-n√§tverk med stor framg√•ng.
Koden som denna avhandling anv√§nde sig av f√∂r att skapa de f√∂rst√§rkt inl√§rda algoritmerna
√§r som bilaga i slutet av denna avhandling. F√∂r att sj√§lv k√∂ra ig√•ng med att tr√§na en algoritm
beh√∂vs dessa filer och alla de Python paket som √§r i requirements.txt filen. Ovanp√• detta
skulle jag rekommendera att installera CUDA och cuDNN paketen fr√•n NVIDIA f√∂r att
snabba upp tr√§ningen med hj√§lp av ett grafikkort. Historiska aktiedata √§r bifogat i data
mappen, detta beh√∂vs inte laddas ner p√• nytt.
F√∂r att komma ig√•ng med tr√§ningen ska man bara k√∂ra main.py filen. Denna fil inneh√•ller
ocks√• de flesta inst√§llningarna f√∂r algoritmen. Om man vill √§ndra p√• m√§ngden noder/lager
i det neurala n√§tverket kan detta g√∂ras p√• models.py i create_model funktionen. Under
k√∂rningen skriver programmet modellen till models mappen med j√§mna mellanrum. I
logdata mappen skrivs en loggfil √∂ver tr√§ningen som kan anv√§ndas f√∂r att visualisera
67

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
resultaten med hj√§lp av plottrainresults.py. Detta ger en motsvarande graf som finns som
bilaga i denna avhandling.
Ifall man vill testa att tr√§na algoritmen p√• endast en aktie som jag n√§mnde tidigare, beh√∂ver
man endast √§ndra i main.py ‚ÄúLIMIT_STOCKS‚Äù variabeln till 1. D√• kommer algoritmen
endast att tr√§nas p√• AAPL ( APPLE Inc.).
Jag anser att f√∂rst√§rkt inl√§rning kommer att hitta flera anv√§ndningsomr√•den i framtiden. En
stor f√∂rdel som f√∂rst√§rkt inl√§rning har j√§mf√∂rt med √∂vervakad inl√§rning √§r att det kr√§ver
inte m√§rkta data. Dock tror jag att d√• n√§r m√§rkta data √§r tillg√§ngligt kommer √∂vervakad
inl√§rning vinna √∂ver f√∂rst√§rkt inl√§rning. Hela f√§ltet av maskininl√§rning kommer s√§kert att
v√§xa mycket i framtiden.
Om vi ser bak√•t i tiden lyckades o√∂vervakad inl√§rning √•teruppliva intresset f√∂r
djupinl√§rning enligt LeCun et al. (2015). Detta har dock sedan √∂verskuggats av
framg√•ngarna i √∂vervakad inl√§rning. LeCun et al (2015) anser att o√∂vervakad inl√§rning
kommer att bli mycket viktigare p√• l√§ngre sikt eftersom m√§nniskornas och djurens inl√§rning
best√•r fr√§mst av o√∂vervakad inl√§rning, n√§mligen vi uppt√§cker v√§rldens struktur genom att
observera den. LeCun et al (2015) f√∂rv√§ntar sig att mycket av de framtida framstegen inom
datorysyn kommer fr√•n system som tr√§nas att kombinera faltnigsn√§tverks med RNNn√§tverk som anv√§nder f√∂rst√§rkt inl√§rning f√∂r att best√§mma var man ska titta. Detta √§r s√§kert
inte dock det enda f√§ltet som f√∂rst√§rkt inl√§rning kommer att anv√§ndas.

6.1 Fortsatta studier
Jag anser att modellerna som denna avhandling kom fram till √§r inte de b√§sta m√∂jliga, men
jag anser att de bevisar att f√∂rst√§rkt inl√§rning kan anv√§ndas f√∂r att f√∂rutsp√• finansiella
medel. Detta √§r ens med flera andra studier inom omr√•det som t.ex. Deng et al. (2016) och
Huang (2019) som ocks√• bevisar att man kan f√• en positiv avkastning med hj√§lp av f√∂rst√§rkt
inl√§rning.

68

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
P√• grund av tidsbrist blev testandet av olika variabler mindre √§n f√∂rst uppskattat. H√§r har
jag listat flera variabler som vore intressanta att testa:
-

DQN algoritmer med n√§tverk neurala n√§tverk som inneh√•ller flera lager. Denna
studie anv√§nde sig endast av tv√• dolda lager. Deng et al. (2016) har hittat att det
skulle finnas en korrelation med mera lager och vinstprocenten.

-

Fler noder i de olika lagren. Skulle t.ex. anv√§ndningen av 512x512 noder √∂ka p√•
vinsten?

-

Mera tr√§ningsdata. Denna studie hade endast 100 aktier p√• en period av 584
handelsdagar. Totalt blir det cirka 58 000 olika observationer. Det finns l√§tt
tillg√§ngligt mera data, men dessa modeller tog redan som v√§rst ett dygn att tr√§na.

-

Andra f√∂rst√§rkt inl√§rda algoritmer. T.ex. Proximal Policy Optimization 2 (PPO2),
Generative Adversarial Imitation Learning (GAIL) och Soft Actor Critic (SAC).
Denna avhandling har inte unders√∂kt ifall dessa kunde passa till avhandlingens
problem.

-

Flera dagar som indata. Studien har anv√§nt sig endast av 10-30 dagar som indata.
En forsknings id√© kunde vara att ifall man ger flera dagar (50 - 200) kunde
algoritmen kanske b√§ttre fokusera p√• l√•ngsiktiga trender. I avhandlingen testade jag
endast med 10 - 30 dagar som indata.

-

Andra tidsintervaller. En observation kunde vara en timme eller en vecka. Jag
fokuserade mig p√• daglig data i min avhandling.

-

Fokus p√• LSTM. Denna avhandling har fokuserat st√∂rsta delen av tiden att optimera
MLP n√§tverk, men andra studier som t.ex. Azhikodan et al. (2019) och Huang
(2019) har visat att LSTM n√§tverk fungerar i vissa fall b√§ttre.

-

Andra bel√∂ningss√§tt. Sharpekvoten har anv√§nts f√∂r att r√§kna bel√∂ningen av flera
andra forskare som t.ex. Berotoluzzo & Corazza (2012). Frekvensen av bel√∂ningen
kunde ocks√• optimeras vidare.

-

L√•t algoritmen blanka. I denna avhandling kunde algoritmen blanka.

F√∂rst√§rkt inl√§rning har enligt mig potential att anv√§ndas som st√∂d f√∂r ens
investeringsbeslut. Jag ser ocks√• att detta √§r en m√∂jlighet att en vidare optimerad f√∂rst√§rkt
inl√§rd algoritm kan handla aktier sj√§lvmant. Listan ovan t√§cker s√§kert endast en br√•kdel av
alla de m√∂jliga variabler som kan testas f√∂r att f√∂rb√§ttra ens algoritm, men jag anser att
detta √§r ett bra st√§lle att b√∂rja. Jag rekommenderar ocks√• vidare optimering av koden f√∂r
69

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
att snabba upp inl√§rningen. Vissa delar av tr√§ningen kan s√§kert k√∂ras parallellt och andra
steg kan f√∂renklas.

70

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

K√ÑLLF√ñRTECKNING
Aase Kim-Georg. (2011). Text Mining of News Articles for Stock Price Prediction.
Magisteruppsats i datavetenskap. Department of Computer and Informations Science,
Norwegian University of Science and Technology.
Azhikodan, A. R., Bhat, A. G., & Jadhav, M. V. (2019). Stock trading bot using deep
reinforcement learning. In Innovations in Computer Science and Engineering (pp. 41-49).
Springer, Singapore.
Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient
descent is difficult. IEEE Trans. Neural Networks 5, 157‚Äì166 (1994).
Callan, R. (2003). Artificial Intelligence. New York: Palgrave MacMillan.
Dai, Y., Wang C., Wang I och Xu, Y (2019) Reinforcement Learning for FX trading. Fr√•n:
http://stanford.edu/class/msande448/2019/Final_reports/gr2.pdf H√§mtad 12.3.2020.
De Grauwe, P. (2010). Animal spirits and monetary policy. Economic Theory, 47(2-3),
pp.423-457.
Deepmind (2019) AlphaStar: Mastering the Real-Time Strategy Game StarCraft II www.
H√§mtad (13.4.2020) https://deepmind.com/blog/article/alphastar-mastering-

real-time-

strategy-game-starcraft-ii
Deepmind (2017) AlphaGo: Starting from scratch, www. H√§mtad (13.4.2020)
https://deepmind.com/research/case-studies/alphago-the-story-so-far
Deng, Y., Bao, F., Kong, Y., Ren, Z., & Dai, Q. (2016). Deep direct reinforcement learning
for financial signal representation and trading. IEEE transactions on neural networks and
learning systems, 28(3), 653-664.
Du, X., Zhai, J., & Lv, K. (2016). Algorithm trading using q-learning and recurrent
reinforcement learning. positions, 1, 1.
Duda, R. O. & Hart, P. E. Pattern Classification and Scene Analysis (Wiley, 1973)
Egeli, B. (2003). Stock market prediction using artificial neural networks. Decision Support
Systems, 22, 171-185.
71

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

Fama, E. (1965). The Behavior of Stock-Market Prices. The Journal of Business, 38(1),
pp.34.
Fama, E. (1970). Efficient Capital Markets: A Review of Theory and Empirical Work. The
Journal of Finance, 25(2), pp.383.
Fama, E. F., & French, K. R. (2004). The capital asset pricing model: Theory and evidence.
Journal of economic perspectives, 18(3), 25-46.
Gori. 2018. Machine Learning. Morgan Kaufmann.
Hastie, T., Tibshirani, R., Friedman, J., The Elements of Statistical Learning: Data Mining,
Inference, and Prediction (Springer, New York, 2011).
Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735‚Äì1780
(1997).
Huang, C. Y. (2018). Financial trading as a game: A deep reinforcement learning approach.
arXiv preprint arXiv:1807.02787.
Jangmin, O., Lee, J., Lee, J. W., & Zhang, B. T. (2006). Adaptive stock trading with
dynamic asset allocation using reinforcement learning. Information Sciences, 176(15),
2121-2147.
Jordan, M. I. and Mitchell, T. (2015). Machine learning: Trends, perspectives, and
prospects. Sci- ence, 349(6245):255‚Äì260.
Kansal, S., Martin, B. (2019) Reinforcement Q-Learning from Scratch in Python with
OpenAI Gym. Fr√•n: https://www.learndatasci.com/tutorials/reinforcement-q-learningscratch-python-openai-gym/ h√§mtad 12.2.2020

Khadjeh Nassirtoussi, A., Aghabozorgi, S., Ying Wah, T. and Ngo, D. (2014). Text mining
for market prediction: A systematic review. Expert Systems with Applications, 41(16),
pp.7653-7670.

72

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep
convolutional neural networks. In Advances in neural information processing systems (pp.
1097-1105).
LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521:436‚Äì444.
Liu, R., & Zou, J. (2018, October). The effects of memory replay in reinforcement learning.
In 2018 56th Annual Allerton Conference on Communication, Control, and Computing
(Allerton) (pp. 478-485). IEEE.
Lo, A. and MacKinlay, A. (2011). A Non-Random Walk Down Wall Street. Princeton:
Princeton University Press.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,
A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,
Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015).
Human-level control through deep reinforcement learning. Nature, 518(7540):529‚Äì533.

Nilsson, H., Isaksson, A., & Martikainen, T. (2002). F√∂retgsv√§rdering med fundamental
analys. Studentlitteratur.
New York Stock Exchange https://www.nyse.com/market-cap h√§mtad (13.3.2020)
Powell, N., Foo, S. Y., & Weatherspoon, M. (2008, March). Supervised and unsupervised
methods for stock trend forecasting. In 2008 40th Southeastern Symposium on System
Theory (SSST) (pp. 203-205). IEEE.
Russell, S. J., Norvig, P. & Canny, J. F. (2003). Artificial intelligence: A modern approach.
2nd ed. Upper Saddle River (NJ): Prentice Hall.
Schumaker, R. and Chen, H. (2006). Textual analysis of stock market prediction using
breaking financial news. ACM Transactions on Information Systems, 27(2), pp.1-19.
Schumaker, R. P., & Chen, H. (2009). Textual analysis of stock market prediction using
breaking financial news: The AZFin text system. ACM Transactions on Information
Systems (TOIS), 27(2), 1-19.
73

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... &
Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search.
nature, 529(7587), 484.
Shmueli, G., & Koppius, O. R. (2011). Predictive analytics in information systems
research. MIS quarterly, 553-572.
Sushko, V., & Turner, G. (2018). The implications of passive investing for securities
markets. BIS Quarterly Review, March.
Sutton, R. S. & Barto, A. G. (2018). Reinforcement learning: An introduction. Second
edition. Cambridge (MA): MIT Press.
Trippi, R. R., & DeSieno, D. (1992). Trading equity index futures with a neural network.
Journal of Portfolio Management, 19, 27-27.
Van Otterlo, M., & Wiering, M. (2012). Reinforcement learning and markov decision
processes. In Reinforcement Learning (pp. 3-42). Springer, Berlin, Heidelberg.
Wong, W., Manzur, M. och Chew, B. (2003). How rewarding is technical analysis?
Evidence from Singapore stock market. Applied Financial Economics, 13(7), pp.543-551.

74

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

BILAGOR:
Bilaga 1:
L√§nk till koden f√∂r algoritmen:
https://github.com/MioNok/q-stock-exploring

Bilaga 2:
Komponenterna i datorn som anv√§ndes f√∂r att tr√§na modellerna i avhandlingen
CPU: Ryzen 5 3600
RAM: 16GB 3000 MHz
GPU: MSI GeForce GTX 1660 Super
SSD: Samsung 500gb 970 EVO plus
Moderkort : Asus Prime B450-Plus
PSU: 550W Seasonic
OS: Windows 10
Python 3.6.7

Bilaga 3:
Variabler f√∂r den f√∂rst√§rkt inl√§rda algoritmen samt neurala n√§tverket.
Variabler:
‚óè n: M√§ngden dagar av historiska aktiedata som algoritmen f√•r g√∂ra sina
beslut p√•.
‚óè Neurala n√§tverket: Typen av de olika lagren, m√§ngden av lager samt
m√§ngden noder i varje lager varierar.

75

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
Konstanter:
‚óè Replay memory size, 2500: M√§ngden tidigare h√§ndelser som sparas f√∂r
vidare tr√§ning. Behandlas i kapitel 3.9
‚óè Min batch size, 64: L√§gsta m√§ngden h√§ndelser i minnet f√∂rr√§n vi b√∂rjar tr√§na
modellerna. Detta fungerar ocks√• som storleken av partiet som modellen
tr√§nas p√• en g√•ng.
‚óè Œ± : Alpha, 0,001. Detta √§r inl√§rningshastigheten (eng. learning rate).
‚óè ùú∏ : Gamma, 0.99.

Gamma √§r diskonteringsfaktorn f√∂r algoritmen.

Diskonteringen behandlas noggrannare i kapitel 3.5
‚óè ùõú: Epsilon, 1-0.05. Epsilon √§r f√∂rknippad med utforskning och exploatering,
h√∂gre epsilon leder till en h√∂gre grad an utforskning. Detta behandlas i
kapitel 3.7. Epsilon minskar varje episod fr√•n 1 tills den √§r 0.05
‚óè Optimeringen, Adam. Simpel stokastisk lutnings (eng. gradient decent)
algoritm f√∂r att optimera neurala n√§tverket.

Bilaga 4:
Aktierna som h√∂r till S&P 100 indexet i januari 2020:

Symbol

Namn

Symbol Namn

Symbol

Namn

AAPL

Apple Inc.

MDT

Medtronic plc

NFLX

Netflix

ABBV

AbbVie Inc.

MET

MetLife Inc.

NKE

Nike, Inc.

ABT

Abbott Laboratories

MMM

3M Company

NVDA

NVIDIA Corp.

ACN

Accenture

MO

Altria Group

ORCL

Oracle Corporation

ADBE

Adobe Inc.

MRK

Merck & Co.

OXY

Occidental Petroleum
Corp.

AGN

Allergan

MS

Morgan Stanley

PEP

PepsiCo

AIG

American International
Group

MSFT

Microsoft

PFE

Pfizer Inc

76

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

ALL

Allstate

NEE

NextEra Energy

PG

Procter & Gamble Co

AMGN

Amgen Inc.

EMR

Emerson Electric Co.

PM

Philip Morris International

AMZN

Amazon.com

EXC

Exelon

PYPL

PayPal Holdings

AXP

American Express

F

Ford Motor Company

QCOM

Qualcomm Inc.

BA

Boeing Co.

FB

Facebook, Inc.

RTN

Raytheon Co.

BAC

Bank of America Corp

FDX

FedEx

SBUX

Starbucks Corp.

BIIB

Biogen

GD

General Dynamics

SLB

Schlumberger

BK

The Bank of New York
Mellon

GE

General Electric

SO

Southern Company

BKNG

Booking Holdings

GILD

Gilead Sciences

SPG

Simon Property Group,
Inc.

BLK

BlackRock Inc

GM

General Motors

T

AT&T Inc

BMY

Bristol-Myers Squibb

GOOG Alphabet Inc. (Class C)

TGT

Target Corporation

BRK.B

Berkshire Hathaway

GOOG
L
Alphabet Inc. (Class A)

TMO

Thermo Fisher Scientific

C

Citigroup Inc

GS

Goldman Sachs

TXN

Texas Instruments

CAT

Caterpillar Inc.

HD

Home Depot

UNH

UnitedHealth Group

CHTR

Charter Communications

HON

Honeywell

UNP

Union Pacific Corporation

CL

Colgate-Palmolive

IBM

International Business
Machines

UPS

United Parcel Service

CMCSA

Comcast Corp.

INTC

Intel Corp.

USB

U.S. Bancorp

COF

Capital One Financial
Corp.

JNJ

Johnson & Johnson

UTX

United Technologies

COP

ConocoPhillips

JPM

JPMorgan Chase & Co.

V

Visa Inc.

COST

Costco Wholesale Corp.

KHC

Kraft Heinz

VZ

Verizon Communications

CSCO

Cisco Systems

KMI

Kinder Morgan

WBA

Walgreens Boots Alliance

CVS

CVS Health

KO

The Coca-Cola Company

WFC

Wells Fargo

CVX

Chevron Corporation

LLY

Eli Lilly and Company

WMT

Walmart

DD

DuPont de Nemours Inc

LMT

Lockheed Martin

XOM

Exxon Mobil Corp.

77

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

DHR

Danaher Corporation

LOW

Lowe's

DIS

The Walt Disney Company MA

MasterCard Inc

DOW

Dow Inc.

MCD

McDonald's Corp

DUK

Duke Energy

MDLZ

Mondelƒìz International

Bilaga 3:
Aktier i S&P 500 indexet i januari 2020.

Symbol

Namn

Symbol

Namn

Symbol

Namn

MMM

3M Company

ETR

Entergy Corp.

NWS

News Corp. Class B

ABT

Abbott Laboratories

EOG

EOG Resources

NEE

NextEra Energy

ABBV

AbbVie Inc.

EFX

Equifax Inc.

NLSN

Nielsen Holdings

ABMD

ABIOMED Inc

EQIX

Equinix

NKE

Nike

ACN

Accenture plc

EQR

Equity Residential

NI

NiSource Inc.

ATVI

Activision Blizzard

ESS

Essex Property Trust, Inc. NBL

Noble Energy Inc

ADBE

Adobe Inc.

EL

Est√©e Lauder Companies JWN

Nordstrom

AMD

Advanced Micro Devices
Inc
EVRG

Evergy

NSC

Norfolk Southern Corp.

AAP

Advance Auto Parts

ES

Eversource Energy

NTRS

Northern Trust Corp.

AES

AES Corp

RE

Everest Re Group Ltd.

NOC

Northrop Grumman

AFL

AFLAC Inc

EXC

Exelon Corp.

NLOK

NortonLifeLock

A

Agilent Technologies Inc EXPE

Expedia Group

NCLH

Norwegian Cruise Line
Holdings

APD

Air Products &
Chemicals Inc

Expeditors

NRG

NRG Energy

AKAM

Akamai Technologies Inc EXR

Extra Space Storage

NUE

Nucor Corp.

ALK

Alaska Air Group Inc

XOM

Exxon Mobil Corp.

NVDA

Nvidia Corporation

ALB

Albemarle Corp

FFIV

F5 Networks

NVR

NVR Inc

EXPD

78

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

ARE

Alexandria Real Estate
Equities

ALXN

FB

Facebook, Inc.

ORLY

O'Reilly Automotive

Alexion Pharmaceuticals FAST

Fastenal Co

OXY

Occidental Petroleum

ALGN

Align Technology

FRT

Federal Realty
Investment Trust

ODFL

Old Dominion Freight
Line

ALLE

Allegion

FDX

FedEx Corporation

OMC

Omnicom Group

AGN

Allergan, plc

FIS

Fidelity National
Information Services

OKE

ONEOK

ADS

Alliance Data Systems

FITB

Fifth Third Bancorp

ORCL

Oracle Corp.

LNT

Alliant Energy Corp

FE

FirstEnergy Corp

PCAR

PACCAR Inc.

ALL

Allstate Corp

FRC

First Republic Bank

PKG

Packaging Corporation of
America

GOOGL

Alphabet Inc Class A

FISV

Fiserv Inc

PH

Parker-Hannifin

GOOG

Alphabet Inc Class C

FLT

FleetCor Technologies
Inc

PAYX

Paychex Inc.

MO

Altria Group Inc

FLIR

FLIR Systems

PAYC

Paycom

AMZN

Amazon.com Inc.

FLS

Flowserve Corporation

PYPL

PayPal

AMCR

Amcor plc

FMC

FMC Corporation

PNR

Pentair plc

AEE

Ameren Corp

F

Ford Motor Company

PBCT

People's United Financial

AAL

American Airlines Group FTNT

Fortinet

PEP

PepsiCo Inc.

AEP

American Electric Power FTV

Fortive Corp

PKI

PerkinElmer

AXP

American Express Co

FBHS

Fortune Brands Home &
Security

PRGO

Perrigo

AIG

American International
Group

FOXA

Fox Corporation Class A PFE

Pfizer Inc.

T

AT&T

FOX

Fox Corporation Class B PM

Philip Morris
International

AMT

American Tower Corp.

BEN

Franklin Resources

PSX

Phillips 66

AWK

American Water Works
Company Inc

FCX

Freeport-McMoRan Inc.

PNW

Pinnacle West Capital

AMP

Ameriprise Financial

GPS

Gap Inc.

PXD

Pioneer Natural
Resources

ABC

AmerisourceBergen Corp GRMN

Garmin Ltd.

PNC

PNC Financial Services

AME

AMETEK Inc.

IT

Gartner Inc

PPG

PPG Industries

AMGN

Amgen Inc.

GD

General Dynamics

PPL

PPL Corp.

79

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
APH

Amphenol Corp

GE

General Electric

PFG

Principal Financial Group

ADI

Analog Devices, Inc.

GIS

General Mills

PG

Procter & Gamble

ANSS

ANSYS

GM

General Motors

PGR

Progressive Corp.

ANTM

Anthem

GPC

Genuine Parts

PLD

Prologis

AON

Aon plc

GILD

Gilead Sciences

PRU

Prudential Financial

AOS

A.O. Smith Corp

GL

Globe Life Inc.

PEG

Public Serv. Enterprise
Inc.

APA

Apache Corporation

GPN

Global Payments Inc.

PSA

Public Storage

AIV

Apartment Investment &
Management
GS

Goldman Sachs Group

PHM

PulteGroup

AAPL

Apple Inc.

GWW

Grainger (W.W.) Inc.

PVH

PVH Corp.

AMAT

Applied Materials Inc.

HRB

H&R Block

QRVO

Qorvo

APTV

Aptiv PLC

HAL

Halliburton Co.

PWR

Quanta Services Inc.

ADM

Archer-Daniels-Midland
Co

HBI

Hanesbrands Inc

QCOM

QUALCOMM Inc.

ARNC

Arconic Inc.

HOG

Harley-Davidson

DGX

Quest Diagnostics

ANET

Arista Networks

HIG

Hartford Financial
Svc.Gp.

RL

Ralph Lauren
Corporation

AJG

Arthur J. Gallagher & Co. HAS

Hasbro Inc.

RJF

Raymond James
Financial Inc.

AIZ

Assurant

HCA

HCA Healthcare

RTN

Raytheon Co.

ATO

Atmos Energy Corp

PEAK

Healthpeak Properties

O

Realty Income
Corporation

ADSK

Autodesk Inc.

HP

Helmerich & Payne

REG

Regency Centers
Corporation

ADP

Automatic Data
Processing

HSIC

Henry Schein

REGN

Regeneron
Pharmaceuticals

AZO

AutoZone Inc

HSY

The Hershey Company

RF

Regions Financial Corp.

AVB

AvalonBay Communities,
Inc.
HES

Hess Corporation

RSG

Republic Services Inc

AVY

Avery Dennison Corp

HPE

Hewlett Packard
Enterprise

RMD

ResMed

BKR

Baker Hughes Co

HLT

Hilton Worldwide
Holdings Inc

RHI

Robert Half International

BLL

Ball Corp

HFC

HollyFrontier Corp

ROK

Rockwell Automation
Inc.

BAC

Bank of America Corp

HOLX

Hologic

ROL

Rollins Inc.

BK

The Bank of New York
Mellon Corp.

HD

Home Depot

ROP

Roper Technologies

BAX

Baxter International Inc.

HON

Honeywell Int'l Inc.

ROST

Ross Stores

BDX

Becton Dickinson

HRL

Hormel Foods Corp.

RCL

Royal Caribbean Cruises
Ltd

BRK.B

Berkshire Hathaway

HST

Host Hotels & Resorts

SPGI

S&P Global, Inc.

80

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
BBY

Best Buy Co. Inc.

HPQ

HP Inc.

CRM

Salesforce.com

BIIB

Biogen Inc.

HUM

Humana Inc.

SBAC

SBA Communications

BLK

BlackRock

HBAN

Huntington Bancshares

SLB

Schlumberger Ltd.

BA

Boeing Company

HII

Huntington Ingalls
Industries

STX

Seagate Technology

BKNG

Booking Holdings Inc

IEX

IDEX Corporation

SEE

Sealed Air

BWA

BorgWarner

IDXX

IDEXX Laboratories

SRE

Sempra Energy

BXP

Boston Properties

INFO

IHS Markit Ltd.

NOW

ServiceNow

BSX

Boston Scientific

ITW

Illinois Tool Works

SHW

Sherwin-Williams

BMY

Bristol-Myers Squibb

ILMN

Illumina Inc

SPG

Simon Property Group
Inc

AVGO

Broadcom Inc.

INCY

Incyte

SWKS

Skyworks Solutions

BR

Broadridge Financial
Solutions

IR

Ingersoll Rand

SLG

SL Green Realty

BF.B

Brown-Forman Corp.

INTC

Intel Corp.

SNA

Snap-on

CHRW

C. H. Robinson
Worldwide

ICE

Intercontinental
Exchange

SO

Southern Co.

COG

Cabot Oil & Gas

IBM

International Business
Machines

LUV

Southwest Airlines

CDNS

Cadence Design Systems IP

International Paper

SWK

Stanley Black & Decker

CPB

Campbell Soup

IPG

Interpublic Group

SBUX

Starbucks Corp.

COF

Capital One Financial

IFF

Intl Flavors & Fragrances STT

State Street Corp.

CPRI

Capri Holdings

INTU

Intuit Inc.

STE

STERIS plc

CAH

Cardinal Health Inc.

ISRG

Intuitive Surgical Inc.

SYK

Stryker Corp.

KMX

Carmax Inc

IVZ

Invesco Ltd.

SIVB

SVB Financial

CCL

Carnival Corp.

IPGP

IPG Photonics Corp.

SYF

Synchrony Financial

CAT

Caterpillar Inc.

IQV

IQVIA Holdings Inc.

SNPS

Synopsys Inc.

CBOE

Cboe Global Markets

IRM

Iron Mountain
Incorporated

SYY

Sysco Corp.

CBRE

CBRE Group

JKHY

Jack Henry & Associates TMUS

T-Mobile US

CDW

CDW

J

Jacobs Engineering
Group

TROW

T. Rowe Price Group

CE

Celanese

JBHT

J. B. Hunt Transport
Services

TTWO

Take-Two Interactive

CNC

Centene Corporation

SJM

JM Smucker

TPR

Tapestry, Inc.

CNP

CenterPoint Energy

JNJ

Johnson & Johnson

TGT

Target Corp.

CTL

CenturyLink Inc

JCI

Johnson Controls
International

TEL

TE Connectivity Ltd.

CERN

Cerner

JPM

JPMorgan Chase & Co.

FTI

TechnipFMC

CF

CF Industries Holdings
Inc

JNPR

Juniper Networks

TFX

Teleflex

81

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

SCHW

Charles Schwab
Corporation

CHTR

KSU

Kansas City Southern

TXN

Texas Instruments

Charter Communications K

Kellogg Co.

TXT

Textron Inc.

CVX

Chevron Corp.

KEY

KeyCorp

TMO

Thermo Fisher Scientific

CMG

Chipotle Mexican Grill

KEYS

Keysight Technologies

TIF

Tiffany & Co.

CB

Chubb Limited

KMB

Kimberly-Clark

TJX

TJX Companies Inc.

CHD

Church & Dwight

KIM

Kimco Realty

TSCO

Tractor Supply Company

CI

CIGNA Corp.

KMI

Kinder Morgan

TT

Trane Technologies plc

CINF

Cincinnati Financial

KLAC

KLA Corporation

TDG

TransDigm Group

CTAS

Cintas Corporation

KSS

Kohl's Corp.

TRV

The Travelers Companies
Inc.

CSCO

Cisco Systems

KHC

Kraft Heinz Co

TFC

Truist Financial

C

Citigroup Inc.

KR

Kroger Co.

TWTR

Twitter, Inc.

CFG

Citizens Financial Group LB

L Brands Inc.

TSN

Tyson Foods

CTXS

Citrix Systems

LHX

L3Harris Technologies

UDR

UDR, Inc.

CLX

The Clorox Company

LH

Laboratory Corp. of
America Holding

ULTA

Ulta Beauty

CME

CME Group Inc.

LRCX

Lam Research

USB

U.S. Bancorp

CMS

CMS Energy

LW

Lamb Weston Holdings
Inc

UAA

Under Armour Class A

KO

Coca-Cola Company

LVS

Las Vegas Sands

UA

Under Armour Class C

CTSH

Cognizant Technology
Solutions

LEG

Leggett & Platt

UNP

Union Pacific Corp

CL

Colgate-Palmolive

LDOS

Leidos Holdings

UAL

United Airlines Holdings

CMCSA

Comcast Corp.

LEN

Lennar Corp.

UNH

United Health Group Inc.

CMA

Comerica Inc.

LLY

Lilly (Eli) & Co.

UPS

United Parcel Service

CAG

Conagra Brands

LNC

Lincoln National

URI

United Rentals, Inc.

CXO

Concho Resources

LIN

Linde plc

UTX

United Technologies

COP

ConocoPhillips

LYV

Live Nation
Entertainment

UHS

Universal Health
Services, Inc.

ED

Consolidated Edison

LKQ

LKQ Corporation

UNM

Unum Group

STZ

Constellation Brands

LMT

Lockheed Martin Corp.

VFC

V.F. Corp.

COO

The Cooper Companies

L

Loews Corp.

VLO

Valero Energy

CPRT

Copart Inc

LOW

Lowe's Cos.

VAR

Varian Medical Systems

GLW

Corning Inc.

LYB

LyondellBasell

VTR

Ventas Inc

CTVA

Corteva

MTB

M&T Bank Corp.

VRSN

Verisign Inc.

COST

Costco Wholesale Corp.

M

Macy's Inc.

VRSK

Verisk Analytics

COTY

Coty, Inc

MRO

Marathon Oil Corp.

VZ

Verizon Communications

82

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

CCI

Crown Castle
International Corp.

MPC

Marathon Petroleum

VRTX

Vertex Pharmaceuticals
Inc

CSX

CSX Corp.

MKTX

MarketAxess

VIAC

ViacomCBS

CMI

Cummins Inc.

MAR

Marriott Int'l.

V

Visa Inc.

CVS

CVS Health

MMC

Marsh & McLennan

VNO

Vornado Realty Trust

DHI

D. R. Horton

MLM

Martin Marietta Materials VMC

Vulcan Materials

DHR

Danaher Corp.

MAS

Masco Corp.

WRB

W. R. Berkley
Corporation

DRI

Darden Restaurants

MA

Mastercard Inc.

WAB

Wabtec Corporation

DVA

DaVita Inc.

MKC

McCormick & Co.

WMT

Walmart

DE

Deere & Co.

MXIM

Maxim Integrated
Products Inc

WBA

Walgreens Boots
Alliance

DAL

Delta Air Lines Inc.

MCD

McDonald's Corp.

DIS

The Walt Disney
Company

XRAY

Dentsply Sirona

MCK

McKesson Corp.

WM

Waste Management Inc.

DVN

Devon Energy

MDT

Medtronic plc

WAT

Waters Corporation

FANG

Diamondback Energy

MRK

Merck & Co.

WEC

Wec Energy Group Inc

DLR

Digital Realty Trust Inc

MET

MetLife Inc.

WFC

Wells Fargo

DFS

Discover Financial
Services

MTD

Mettler Toledo

WELL

Welltower Inc.

DISCA

Discovery Inc. Class A

MGM

MGM Resorts
International

WDC

Western Digital

DISCK

Discovery Inc. Class C

MCHP

Microchip Technology

WU

Western Union Co

DISH

Dish Network

MU

Micron Technology

WRK

WestRock

DG

Dollar General

MSFT

Microsoft Corp.

WY

Weyerhaeuser

DLTR

Dollar Tree

MAA

Mid-America Apartments WHR

Whirlpool Corp.

D

Dominion Energy

MHK

Mohawk Industries

WMB

Williams Cos.

DOV

Dover Corp.

TAP

Molson Coors Brewing
Company

WLTW

Willis Towers Watson

DOW

Dow Inc.

MDLZ

Mondelez International

WYNN

Wynn Resorts Ltd

DTE

DTE Energy Co.

MNST

Monster Beverage

XEL

Xcel Energy Inc

DUK

Duke Energy

MCO

Moody's Corp

XRX

Xerox

DRE

Duke Realty Corp

MS

Morgan Stanley

XLNX

Xilinx

DD

DuPont de Nemours Inc

MOS

The Mosaic Company

XYL

Xylem Inc.

DXC

DXC Technology

MSI

Motorola Solutions Inc.

YUM

Yum! Brands Inc

ETFC

E*Trade

MSCI

MSCI Inc

ZBRA

Zebra Technologies

EMN

Eastman Chemical

MYL

Mylan N.V.

ZBH

Zimmer Biomet Holdings

ETN

Eaton Corporation

NDAQ

Nasdaq, Inc.

ZION

Zions Bancorp

EBAY

eBay Inc.

NOV

National Oilwell Varco
Inc.

ZTS

Zoetis

83

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning
ECL

Ecolab Inc.

NTAP

NetApp

EIX

Edison Int'l

NFLX

Netflix Inc.

EW

Edwards Lifesciences

NWL

Newell Brands

EA

Electronic Arts

NEM

Newmont Corporation

EMR

Emerson Electric
Company

NWSA

News Corp. Class A

Bilaga 5:
Graferna visar f√∂rdelningen av skillnaden mellan aktier j√§mf√∂rt med en passiv
investering. B√•da graferna anv√§nder modellen 256x256 MLP med 30 dagar som
indata. Den f√∂rsta √§r testset 2 framtid och den andra √§r testset 2.

84

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

85

M. Nokelainen: Aktiv investering med hj√§lp av f√∂rst√§rkt inl√§rning

86

