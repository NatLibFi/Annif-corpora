Machine learning for
predicting alkali salt induced
high-temperature corrosion of
superheater materials

Master’s Thesis
Åbo Akademi University
Faculty of Science and Engineering
Rasmus Fagerlund
2021

Abstract
Climate change due to human activities has caused a global desire to lower CO2 emissions in
the hope of halting the increasing global temperature. For electricity generation, this means
reducing the CO2 emissions by increasing the fraction of renewable electricity generation. For
example, using fuels derived from biomass instead of fossil fuels in steam boilers can reduce
CO2 emissions. However, burning biomass fuels can be problematic, because the fuels tend to
be comparatively rich in ash-forming elements and other trace elements. The ash-forming
elements are carried with the flue gas and can form corrosive deposits on the superheater, due
to the high temperatures inside steam boilers. This type of corrosion is an example of hightemperature corrosion. To prevent corrosion of the superheater, the steam temperature is kept
at a lower temperature, which decreases the overall electrical efficiency of the stream boiler.
Materials able to withstand the corrosive environment exist, but they are typically more
expensive. Balancing initial material costs with a potentially higher electrical efficiency has
caused a drive to better understand the underlying corrosion mechanics, in order to better match
superheater materials with operational parameters.
This work presents how machine learning can be used to predict corrosion severity using
labeled data and open-source software. The data consists of corrosion experiments carried out
under laboratory conditions, where a superheater material is subject to high-temperature
conditions and synthetic ash for a set duration. The corrosion severity is determined using SEMEDX analysis of the oxide layer formed. The experimental conditions and corrosion severity
are saved in a tabular database.
A subset of the database is selected to be used for machine learning. The subset consists of
experiments done in air for one week using synthetic ash mixtures made from alkali salts. The
data is analyzed in detail to better understand its quality and suitability for machine learning.
The analysis includes value distributions, correlation between features and variation found in
repeated experiments. The corrosion data was found not well suited for machine learning, due
to problems such as small sample size, badly distributed data and skewed features. The variation
found in the output proves to be a limit factor for prediction accuracy.

i

A feed-forward neural network (FFNN), random forest (RF) and gradient boosted decision trees
(GBDT) are the chosen algorithms to model the data subset. The models are trained, tuned and
evaluated on separate portions of the data subset. In addition, synthetic data is generated to
further evaluate the models. The RF and GBDT models performed well when compared to the
data quality and random variation. The FFNN did not achieve as good performance metrics and
the predictions were more scattered when compared with the other models. The models are able
to successfully reproduce generally understood high-temperature corrosion trends, such as
increasing the temperature and ash chlorine content increases the corrosion rate, and increasing
the chromium and nickel content in the alloy decreases the corrosion rate. All data analysis and
machine learning were done in Python using open-source libraries.
Keywords: high-temperature corrosion, superheater, machine learning

ii

Preface
This master’s thesis was written for Johan Gadolin Process Chemistry Centre (PCC) at Åbo
Akademi University as part of the umbrella project CLUE2 (Clean and Efficient Utilization of
Demanding Fuels). The project aims to advance the knowledge of effective and safe utilization
of biomass and waste-derived fuels in large scale electricity generation.
To start off, I would like to thank PCC for giving me the opportunity to work on something as
interesting as machine learning. The utilization of machine learning is, in my opinion, still in
its infancy, and I am beyond thrilled to have been a part of this exploratory research at PCC.
I would like to thank Markus Engblom for being an excellent supervisor, and for guiding me
through the process. I could not have produced this work without his help from the many
meetings and discussions we have had. I would also like to thank all personnel at PCC,
especially Oskar Karlström and Mikko Hupa, for their valuable inputs. Additional thanks to the
people outside of PCC who have been tremendously helpful, such as Daniel Lindberg for help
with FactSage, Hanna Kinnunen for help with categorizing alloys and Henrik Saxén for insight
into machine learning. Lastly, I would like to thank my family, friends and SO for keeping me
sane during a global pandemic.
Rasmus Fagerlund
Åbo, 27.04.2021

iii

Table of contents
Abstract ...................................................................................................................................... i
Preface ...................................................................................................................................... iii
Table of contents...................................................................................................................... iv
1.

2.

Introduction ...................................................................................................................... 1
1.1

High-temperature corrosion in steam boilers .............................................................. 2

1.2

Experimental method for studying corrosion .............................................................. 4

1.3

Machine learning ......................................................................................................... 8

1.4

Objectives .................................................................................................................. 10

Methods ........................................................................................................................... 11
2.1

Programming tools .................................................................................................... 11

2.2

Supervised learning algorithms ................................................................................. 12

2.2.1

Artificial neural network .................................................................................... 12

2.2.2

Ensemble learning methods ............................................................................... 15

3.

Corrosion database and data manipulation................................................................. 18

4.

Hyperparameter tuning ................................................................................................. 21

5.

Results and discussion .................................................................................................... 26
5.1

Data analysis .............................................................................................................. 26

5.1.1

Value distributions ............................................................................................. 26

5.1.2

Feature correlation .............................................................................................. 27

5.1.3

Repeated experiments ........................................................................................ 28

5.2

Model evaluation ....................................................................................................... 31

5.3

Synthetic tests ............................................................................................................ 38

6.

Conclusions and future work ........................................................................................ 40

7.

Svensk sammanfattning ................................................................................................. 42
7.1

Metoder ...................................................................................................................... 43

7.2

Resultat ...................................................................................................................... 45

References ............................................................................................................................... 49
iv

1. Introduction
Climate change due to a rising global average temperature is an imminent threat to our modern
society. The increasing temperature observed since the industrial revolution is mainly caused
by anthropogenic drivers [1]. The main anthropogenic driver of this warming is the emission
of greenhouse gases, with carbon dioxide (CO2) being the largest component [2]. Therefore,
reducing CO2 emissions is crucial in order to limit the global temperature increase, and
hopefully reduce the negative impacts of climate change. Reducing CO2 is of interest for most
nations, as evidenced by the Paris Agreement [3].
The combustion of fossil fuels is the largest contributor to anthropogenic CO2 emissions [2].
By switching to the combustion of renewable fuels, a large fraction of global CO2 emissions
can be heavily reduced, as these fuels generally have much lower net CO2 emissions. Recent
years has seen an increase in electricity generation from renewable fuels, as shown in Figure 1.

700 000
600 000

GWh

500 000
400 000
300 000
200 000
100 000
0
1990

1995

2000

2005
Year

Industrial waste

Primary solid biofuels

Municipal waste

Liquid biofuels

2010

2015

2018

Biogases

Figure 1. Global electricity generation from biofuels and waste. The data were downloaded
from [4].

1

1.1 High-temperature corrosion in steam boilers
Corrosion is often a limiting factor when maximizing the electrical efficiency of steam boilers.
The type of corrosion relevant to this work is high-temperature corrosion of the superheater.
More details on high-temperature corrosion can be found, for example, in [5]–[11], and the
information in this section is mostly based on these references. As the name high-temperature
corrosion suggests, superheater materials can become subject to damaging corrosion with
increasing steam temperatures. How high the temperature can be raised is largely dependent on
the fuel burned and the superheater material.
Steam temperatures and the corresponding electrical efficiency of typical modern power plants
burning different types of fuels is shown in Figure 2. The figure shows that the steam
temperature in modern power plants is highly dependent on the fuel. Power plants burning
higher quality fuels, i.e. more homogeneous and pure fuels, tend to operate at higher steam
temperatures. This is partly due to the higher concentrations of corrosion-promoting elements
in lower quality fuels.

Figure 2. Overview of electrical efficiencies and steam temperatures for modern power plants
burning different fuels. Adapted from [12].

2

In power plants burning biomass, the severe cases of corrosion found on the superheater tubes
are often caused by corrosive ash deposits. The inorganic part of biomass, which consists of
ash-forming elements and other trace elements, travels with the flue gas and sticks to the
superheater tubes at sufficiently high temperatures. Therefore, power plants have been forced
to limit the steam temperature, in order to not damage the superheater and cause unnecessary
maintenance stops.
The ash-forming elements relevant to this work are mainly alkali salts. Certain alkali salts have
been found to greatly impact the corrosiveness of an ash deposit by changing its melting
behavior. The two main elements contributing to the melting behavior are chlorine and
potassium. The temperature at which melting begins is called the first melting point (T0), and
chlorine increases the amount of melt formed at T0, while potassium lowers T0. Chlorine is also
known to induce corrosion at temperatures below T0.
The demand for higher electrical efficiencies has caused a drive to find better materials able to
withstand the more corrosive conditions. However, as better materials are typically more
expensive, a problem arise of being able to accurately determine the necessary material for a
set of operational parameters. In other words, being able to balance the upfront material cost
with future gains from a higher net electrical efficiency. This is important when maximizing
power plant profitability.
The alloys used in superheater tubes contain various amounts of elements that improve
corrosion resistance, of which chromium, nickel and molybdenum are particularly notable.
Chromium is a common alloying element added to iron-based materials. In sufficient quantities,
chromium forms a strong chromium oxide (Cr2O3) layer, which offers good protection in hot
oxidizing environments, but fails when alkali chlorides are present. Nickel is used as an alloying
element together with chromium in austenitic stainless steels and nickel-based alloys. Nickel is
more stable than iron in oxidizing environments containing chlorine and has found use in highly
corrosive environments. Molybdenum has been observed to improve corrosion resistance when
alloyed with nickel and chromium, but the exact underlying mechanisms are not yet fully
understood [13], [14].

3

1.2 Experimental method for studying corrosion
The method developed at Åbo Akademi University studies corrosion severity using SEM-EDX
analysis of the oxide layer formed on a material after exposure to a corrosive environment for
a set duration. SEM-EDX analysis allows for a detailed view of the oxide layer and shows
where different elements are present. The oxide layer is also preserved during measurement,
which can prove useful if there is a need to reanalyze a sample. The method simulates an
idealized and simplified boiler environment found at the superheater. The temperature, gas
atmosphere and ash composition are static during the experiments, whereas in real boilers they
tend to fluctuate. In addition, the effect soot blowers have on corrosion is not simulated. The
method has been deemed reliable and the experiments reproducible. Each step of the method is
briefly introduced below, more detailed explanations can be found in [15], [16]. Note that there
are small differences in the described method between the references. This is due to refinements
made over the years.
The samples are prepared by first cutting them into 20x20x5 mm squares. To achieve a uniform
surface, the samples are ground and polished using silicon carbide grinding paper with ethanol.
The samples are then cleaned with ethanol in an ultrasonic bath (and pre-oxidized in [15]).
A synthetic ash mixture is applied to the sample. These ashes are created in such a way to mimic
ash deposits found on superheaters in steam boilers burning biomass or waste-derived fuels.
This includes selecting, weighing, mixing salt compounds into a desired chemical composition.
The mixture is then melted and filtered to a desired particle size distribution. Then 250 mg of
the synthetic ash is placed on the sample using a cylindrical mold with a diameter of 16 mm.
Lastly, the mold is gently tapped with a glass rod to make the ash more cohesive. A prepared
sample is shown in Figure 3.

4

Figure 3. A prepared sample with synthetic ash.
The experiments have also been done without synthetic ash to establish a baseline corrosion
rate. Pure alkali carbonates and sulfates have sometimes been used as reference to keep the
oxide layer intact during cooling, as they generally do not impact the oxide layer formation.
The prepared samples are placed on a sample holder with a maximum capacity of five samples.
The sample holder also features thermocouples between each sample tray. The temperature is
logged during the experiment to ensure it stayed within margins during the heat treatment. The
sample holder is then inserted into a tube furnace using Al2O3 tubes and sealed. The sample
holder and tube furnace are shown in Figure 4.

Figure 4. Sample holder and tube furnace.

5

The experiments are typically done for a week (168 hours), but any duration is possible. A week
typically produces samples with sufficient oxide formation for detailed analysis, without
destroying the surface. The atmosphere inside the furnace generally consists of regular air, but
it can be controlled by injecting other gases, provided the furnace is properly sealed.
After heat treatment, the samples are left to cool at room temperature and prepared for SEMEDX analysis. They are cast in a cylindrical mold using chlorine-free epoxy and cut vertically
to obtain a cross section of the sample. The newly created discs are then cleaned of any
lubricating oil residue from the cutting process, polished using silicon carbide grinding paper
with kerosene (or ethanol in [15]) and finally washed in a petroleum ether ultrasonic bath. The
mold and cross section discs are shown in Figure 5.

Figure 5. A sample inside the mold (left) and a cross section disc (right).
SEM-EDX analysis provides grayscale images of the surface cross section by firing a high
intensity electron beam across it and detecting backscatter electrons (BSE), in addition to
providing detailed information on elemental composition. Multiple SEM images must be taken
and combined into a panorama, in order to show the whole surface. To calculate the oxide layer
thickness, the ash-covered area is identified and the different layers are colored based on
differences in contrast. An example of a panorama is shown in Figure 6. This process is not
fully automated and human intervention is often necessary, due to the uniqueness of each oxide
layer. A script sums the colored pixels vertically and calculates the mean, median, most
occurring, minimum, and maximum oxide layer thicknesses, out of which the mean, median
and maximum values are stored in the database, see Figure 7 for an example. The script also
creates a graph showing the oxide layer thickness distribution, see Figure 8 for an example.
6

Figure 6. An example of a panorama. The upper image is the original and the bottom is
colored.

Figure 7. A calculated oxide layer thickness across the sample cross section. Only the middle
part was exposed to the ash mixture and is therefore used to calculate the thickness.

Figure 8. An oxide layer thickness distribution.
7

1.3 Machine learning
Machine learning is a branch of artificial intelligence focused on the study of computer
algorithms that learn from data to carry out tasks. The field has seen a recent surge in interest
from both academia (see Figure 9) and industry [17], [18]. Major driving forces include the
availability of data, computing capabilities and other advances in machine learning techniques
[19], [20]. Machine learning has also been made available to a wider audience in recent years
through internet communities and the now established open-source frameworks and tools.
Strong machine learning communities, consisting of amateurs and professionals alike, exist on
websites such as Kaggle, Reddit, GitHub, Quora and Stack Exchange.

Total

Machine learning as percentage of total

100000

50

80000

40

60000

30

40000

20

20000

10

0

Percentage of total [%]

Number of articles

Machine learning

0
2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020
Year

Figure 9. Machine learning research interest from 2010 to 2020 based on articles available on
arXiv [21]. The “Machine learning” bar (blue) is the number of articles in subcategories for
machine learning (cs.LG and stat.ML), while the “Total” is articles in the broader categories
Computer Science and Statistics (cs and stat). Cross-listed articles are included.
Machine learning algorithms are broadly categorized into supervised learning, semi-supervised
learning, unsupervised learning or reinforcement learning algorithms, depending on how they
approach tasks. Algorithms may fit into multiple categories. The different categories of
algorithms are briefly introduced below. More details on algorithms and machine learning in
general can be found, for example, in [22]–[24].

8

Supervised learning algorithms use labeled data, i.e. data that contain both input(s) and
output(s). The algorithms attempt to model the relationship between the input(s) and output(s),
in order to predict the output(s) from new input data. Depending on how the output is given,
the algorithms are split into either classification or regression algorithms. Classification
algorithms are used when an output is restricted to a set of values, while regression algorithms
are used for continuous output values. Unsupervised learning algorithms use unlabeled data,
i.e. data only containing inputs, and attempt to find patterns in that data. Unlabeled data are
often inexpensive and easier to obtain compared to labeled data, hence the interest in these types
of algorithms. Semi-supervised learning algorithms combine the above-mentioned categories
by introducing a small amount of labeled data with unlabeled data. This can have a significant
impact on the overall accuracy of the model and might be considered if the potential gain is
worth the cost of acquiring some labeled data.
Reinforcement learning algorithms differ from the others. These algorithms find which actions
an software agent should take in an environment to maximize a cumulative reward. The agent
explores new possible actions in combination with exploiting accumulated knowledge to find
better solutions. Relating this to the real world, imagine teaching a dog (the agent) to pick up
trash and put it in a bin (the action) to receive a treat (the reward). Fortunately, the dog has prior
knowledge, such as knowing how to use its legs to move around the house. The dog also has
its owner to help the learning process and convey which actions to take. However, a software
agent does not necessarily have prior intelligence or human guidance. Therefore, imagine
instead if the dog first must figure out how to traverse the environment using its legs. Then it
must learn that it is supposed to pick up trash using its mouth and move it to a designated place.
This quickly becomes a difficult problem to solve, and the dog would probably never figure out
the task in its lifetime. However, running many parallel simulations in rapid succession is
possible in a software environment, therefore making the process of trial and error feasible. The
process can also be made less random, for example, by introducing intermediate rewards or
restricting joint movement. These changes could help guide the agent in the right direction. This
is how a software agent might start to learn and display intelligent behavior.

9

1.4 Objectives
The premise of thesis is to explore the possibilities of using machine learning to predict the
corrosion severity of superheater materials subject to high temperatures and synthetic ash in
laboratory conditions. The main objectives are:
•

The development and evaluation of machine learning models. Determine the models’
capabilities and limitations when predicting corrosion severity.

•

Analyze the suitability of the corrosion database for machine learning usage. Provide
some insight into how it could be modified for better results.

10

2. Methods
2.1 Programming tools
The code used in this work is written in Python 3.8. The development environment includes an
IDE (PyCharm Community Edition 2020.2) and version control (Git 2.24) with remote
repository hosting (GitHub). These aid the development process and make remote work with
multiple devices simple. Python combined with various open-source libraries (publicly
accessible collections of programming routines) creates a powerful framework for doing
machine learning tasks, as evidenced by its popularity within the machine learning and data
science communities [25]. Table 1 lists libraries that were crucial to this work.
Table 1. Key libraries used in this work.
Library

Description

NumPy

The backbone of many other libraries used for scientific computing in Python.

[26]

It provides highly optimized numerical computing by introducing a
multidimensional array object (called ndarray) together with a large collection
of mathematical functions to operate on the arrays.

pandas

Provides tools for data analysis and manipulation. It introduces the DataFrame

[27], [28]

object, which is based on the NumPy ndarray, along with a large collection of
methods for manipulating the DataFrame.

Matplotlib

Comprehensive plotting and visualization tools. It works well together with

[29], [30]

the other libraries and provides simple ways of creating static, animated or
interactive plots in Python.

Scikit-learn

General machine learning library built on NumPy, Matplotlib and other

[31], [32]

libraries. It features many ready-to-use machine learning algorithms together
with other frequently used tools, for example, data preprocessing functions.

Keras

High-level object-oriented API intended for deep learning. Prior to version
2.4, it supported multiple backends. Keras was later merged into Tensorflow
and can be found as the module tf.keras.

TensorFlow

Numerical computing library developed by Google. It serves a similar purpose

[33]

as NumPy, but with a focus on artificial neural networks.

11

2.2 Supervised learning algorithms
There are many different algorithms to choose from when doing supervised machine learning,
each with its own strengths, weaknesses and quirks. This section focuses on the supervised
learning algorithms used in this work. More details on supervised learning can be found e.g. in
the textbooks [23], [24]. The following subsections (2.2.1 and 2.2.2) use these books as
references.
As introduced in section 1.3, supervised learning algorithms model the relationships between a
set of input features and the output(s). The goal is to produce accurate predictions of the
output(s) using the input features, i.e. reproduce the output(s) from the input features. An
artificial neural network (ANN) and two ensemble learning methods, random forest (RF) and
gradient boosted decision trees (GBDT), were chosen as the algorithms to implement and study.
Testing multiple models help determine if any one model is poorly implemented and should
indicate which method is better suited to model the given data.

2.2.1 Artificial neural network
ANNs are made up of artificial neurons, called nodes or units. These nodes interact with each
other through connections that are assigned weights. The nodes themselves are assigned an
equation, called the activation function, and a bias. The nodes produce an output by feeding the
sum of the incoming connection weights and the bias through the activation function, as
depicted in Figure 10.

12

Figure 10. A node in an artificial neural network.
The architecture chosen to model the data is a fully connected feed-forward neural network
(FFNN). In an FFNN, the nodes are divided into groups called layers, which are ordered one
after another as shown in Figure 11. Each node in a layer is connected to every node in the
following layer. The first layer is called the input layer and it consists of the input features, with
each feature being represented by one node. The last layer is called the output layer and consists
of the output(s). In this work, the output is a single node, representing the mean oxide layer
thickness. The middle layers, called hidden layers, give the FFNN the ability to approximate
any continuous function to any precision, given enough layers and nodes.

Figure 11. A fully connected feed-forward neural network.

13

Training a neural network is at its core an optimization problem. The algorithm tries to find a
set of weights and biases that give good output predictions for a given set of hyperparameters.
Hyperparameters refer to choices made during model selection and training. How they are
chosen is described in chapter 4. A common method for finding good weights and biases in an
FFNN is using a gradient-based optimization algorithm (optimizer). A gradient-based optimizer
tries to minimize a loss function by iteratively changing the weights and biases towards the
negative gradient of the loss function with respect to each weight and bias. Calculating the
gradient for each weight and bias is done using backpropagation.
Backpropagation works on a per layer basis using the chain rule to calculate the gradient and
makes training complex neural networks feasible. Inputs of a sample are fed through the
network (forward pass) and produce an output. An error is then calculated based on the loss
function from the produced output and the actual output for the sample. The error is fed
backwards (backward pass) through the network and used to adjust each weight and bias. This
is then repeated for each sample and one pass through all samples is called an epoch. How many
epochs are needed depends on the optimizer and training-related hyperparameters. [34]
Algorithms using one sample to update the weights and biases are called stochastic. Using only
one sample can sometimes hinder the algorithm in finding a good set of weights and biases.
Real-world data sets typically contain noise, meaning that gradients calculated from individual
samples might not be representative of the “true” gradients. Finding the global minimum (or a
satisfactory local minimum) can therefore prove difficult if the gradients fluctuate a lot. This is
where using a batch of samples can prove useful. The batch approach accumulates gradients for
all samples in the training data set and then use the averages. This way the calculated gradients
are exact. However, this approach is prone to converge to local minima, as there are no
fluctuations in the gradients. The method is also comparatively slow, requiring going through
all samples for one update of the weights and biases. Furthermore, it might cause problems with
memory usage on very large data sets. The mini-batch approach is the middle ground between
the other versions. It uses the averaged gradients of a mini-batch, which is a specified number
of samples. This allows for more control of the training process. [35]

14

2.2.2 Ensemble learning methods
An ensemble model consists of a collection of simpler models, often referred to as base learners.
Ensembles can be either homogeneous, meaning they use a single algorithm for all base
learners, or heterogeneous, where multiple different algorithms are used as base learners. The
most common ensembles are homogeneous and use decision trees as base learners. A decision
tree consists of if-else statements that determine the output. An example of how a tree
categorizes a set of inputs is shown in Figure 12.

Figure 12. An example function z = f(x,y) depicted as a decision tree.
One reason for decision trees being commonly used as base learners is that their accuracy is
easy to control. Tuning the base learner in such a way as to barely perform better than random
predictions is key in producing a good ensemble model. Therefore, base learners are sometimes
referred to as weak learners. The final ensemble prediction is obtained by averaging the outputs
of the base learners (see Figure 13). If the ensemble is used for classification instead of
regression, then the base learner outputs are typically combined using majority voting instead
of averaging.

15

Figure 13. An ensemble of decision trees.
The RF algorithm builds a collection of decorrelated (without serial correlation) decision trees,
which are trained using a modified version of bagging (bootstrap aggregation). For each tree,
bagging generates a bootstrap sample (random sampling with replacement) from the training
set. The trees are then individually created from their respective sample using the CART
methodology described in [36]. Once the trees are created, they are each able to generate a
prediction. The final output of the ensemble is produced by taking the average of all individual
tree predictions. The RF algorithm uses the same bootstrap samples as bagging, but also
considers a random input feature subsample 𝒎 when doing node splitting, while bagging
considers all features 𝒑. This introduced randomness tends to produce better models. For
regression, a subsample size 𝒎 = 𝒑/𝟑 is a good starting point, but 𝒎 should be considered a
parameter to tune. A subsample size of 𝒎 = 𝒑 would make the RF algorithm identical to
bagging. [31], [37], [38].

16

The GBDT algorithm trains individual trees in an additive and sequential approach, in contrast
to the parallel approach of RFs. The idea is that each new tree is fit in such a way that when
combined with the previous trees, it minimizes a loss function. Gradient boosting can be related
to the common gradient descent algorithm. Gradient descent iteratively changes parameters in
the direction of the negative gradient of the loss function, while gradient boosting introduces
new trees fit on the negative gradient, thus working the ensemble towards a local minimum of
the loss function. [31], [39].

17

3. Corrosion database and data manipulation
Experimental conditions and results from the laboratory experiments are saved in a tabular
format, creating a corrosion database. In the table, the columns represent experimental
conditions (input features) and resulting corrosion (outputs). The corrosion severity is
represented by the mean, median and maximum oxide layer thickness formed during an
experiment, of which the mean oxide layer thickness is used. The database contains 2746
samples in total. The experimental conditions are:
▪

Gas composition [vol %], 14 components

▪

Temperature [°C]

▪

Duration [h]

▪

Material

▪

Salt composition [wt %], 25 components

It was decided that the database contains too few experiments for the algorithms to learn the
effects of gases, time and all 25 elements in the ash. Therefore, a data subset was created from
experiments representing the majority of the data; i.e. experiments using alkali salts
(representing ash found in black liquor recovery boilers), experiments carried out in room
atmosphere for a week, and steels for which the majority of the experiments have been made.
Some of the input features will also be modified to better their representation.

The superheater material is represented by its name in the database. By converting the name
into elemental components, the machine learning algorithms are able to learn how different
elemental compositions relate to the mean oxide layer thickness. Chromium (Cr), nickel (Ni)
and molybdenum (Mo) were chosen to represent the alloys. These elements are important to
the corrosion resistance of superheater materials, as introduced in section 1.1. The component
values, shown in Table 2, were chosen by taking an average of chemical composition ranges
given by online alloy databases and alloy producers.

18

Table 2. Chosen superheater materials, their components and number of samples they
represent.
Material

Cr [wt %]

Ni [wt %]

Mo [wt %]

Samples

TP347H

17.5

11

0

202

10CrMo9-10

2.25

0

1

184

X10CrMoVNb9-1

8.75

0

0.95

92

Sanicro 28

27

31

3.5

80

Inconel 625

21

64

8.5

62

AC 66

27

32

0

45

Esshete 1250

15

9.5

1

44

HR11N

29

42

1

28

TP310H

25

20.5

0

21

TP310HCbN

25

20.5

0

16

11.25

0.55

1

14

2.4

0

1

10

X20CrMoV11-1
7CrMoVTiB10-10

Additional experiments where temperatures are greater than the ash mixture’s first melting
point (T0) are removed, thus leaving experiments subject to sub-T0 conditions. Ash mixtures
generally have a melting range, and the corrosion behavior is different when the mixtures are
partially or fully melted [7]. In addition, experiments done without an ash mixture had to be
removed, in order to calculate T0. The salt representation is also improved, by converting the
salt compounds into three ratios based on mole fractions: K / (Na + K), Cl / (Na + K) and CO3 /
(SO4 + CO3). This better reflects the fact that the ash mixture is melted prior to being applied to

the steel. The original database only gives information on which compounds were used to create
the synthetic ash mixtures. In other words, the compounds in the original corrosion database do
not represent the chemical composition of the ash mixture that is applied to the material
samples.

19

The last modifications include removing a few experiments with a mean oxide layer thickness
of greater than 100 μm. The final data subset that is used in the model contains 7 input features
(listed below) and 670 samples, down from 42 and 2746, respectively.
▪

Temperature [°C]

▪

Cr [wt %]

▪

Ni [wt %]

▪

Mo [wt %]

▪

K / (Na + K)

▪

Cl / (Na + K)

▪

CO3 / (SO4 + CO3)

The last step of preprocessing involves dividing the samples into different parts that will be
used in various steps of the machine learning model creation process. Depending on the
validation process (details in chapter 4), the samples are split into either two parts or three parts
as shown in Figure 14. Note that smaller data sets generally require larger testing (and
validation) fractions to better account for sampling variability. To better account for sampling
variability during tuning for this case, additional cross-validation was performed with multiple
random splits.

Figure 14. Data sample splits for the different algorithms.

20

4. Hyperparameter tuning
Tuning the hyperparameters of a machine learning algorithm is important when optimizing a
model’s predictive accuracy. Tuning is done by balancing two model properties called bias and
variance, often referred to as the bias-variance tradeoff, as they are inversely connected. Badly
tuned algorithms tend to produce models with either high bias (underfitted) or high variance
(overfitted). Underfitting occurs when the algorithm has failed to capture all relevant relations
between inputs and outputs, i.e. the model is not performing as well as it potentially could.
Overfitting is the opposite, where the algorithm has accurately learned the training data,
including random noise. This results in a model that does not generalize well, and the model
would produce bad predictions of new data. To better illustrate the bias-variance tradeoff, an
example showing different curve fitting models can be found in Figure 15.

Figure 15. An example of how bias, variance and prediction error relate to model complexity.
The upper graphs show how varyingly complex models might fit a curve to data (black dots).
The underfitted and overfitted models failed to learn the general trend, which might result in
bad predictions, as illustrated by a new sample (blue cross).
21

Training data cannot be used for tuning a model’s hyperparameters, as that could cause
overfitting. Therefore, the hyperparameters can be tuned based on, for example, performance
on unseen data or from cross-validation of the training data. These two methods are called
holdout and k-fold cross-validation. The holdout method uses a set-aside validation set, on
which the trained model is evaluated. The hyperparameters are then tuned to improve the
model’s performance when predicting the outputs of the validation set. The second method, kfold cross-validation, works by splitting the training set into 𝒌 subsets. One subset is set aside
and used for validation while the model is trained on the remaining subsets. This is repeated 𝒌
times, with each subset being used once for validation. The results from these intermediate
models are then averaged into one prediction. This way of performing validation gives that the
model a larger fraction of the data set to train on, potentially producing a better model, at the
cost of iterating 𝒌 times instead of once.
As our data set is small and the data quality varies, the hyperparameters will be manually chosen
from a set of 10 runs. For each run, the sample order is shuffled randomly before the data set is
split. This methodology can be referred to as Monte-Carlo cross-validation and adds another
layer of cross-validation. This is done to ensure that the model is not overfitted on a particular
random shuffle. The random number generator used in the pandas sample() method allows for
reproducibility by using a seed, in case any run needs to be rerun.
The FFNN will be manually tuned using the holdout method. The process involves training the
neural network for an arbitrary number of epochs until the validation metrics no longer improve.
The more epochs a neural network is trained for, the more complex the model becomes. Prior
to finding the number of epochs, other hyperparameters must be chosen. The combination of
hyperparameters in Table 3 yielded a good model. The activation function ReLU and
optimization algorithm Adam were chosen due to their prevalence and good performance in
neural networks [40], [41]. The loss function MSE was chosen due to it generally being the
default choice for regression tasks. The number of hidden layers, nodes and the learning rate
was settled upon using trial and error. Note that these hyperparameters can be difficult to tune
when the data set is noisy, and that the chosen values are most likely not optimal.

22

Table 3. Chosen hyperparameters for the FFNN.
Hyperparameter

Value

Hidden layers
Nodes per hidden layer
Activation function for hidden layer nodes
Optimization algorithm (optimizer)
Learning rate
Loss function
Batch size

2
32
ReLU
Adam
0.02
MSE
64

The number of epochs to train this FFNN as not to underfit or overfit varied greatly depending
on how the samples were shuffled, as shown in Table 4. The table shows at what epoch the
validation error metrics for the model are generally low. The mean absolute error (MAE) and
mean squared error (MSE) both measure the errors of a prediction model. The error is also
known as the residual. MAE describes the average value of the residuals, while RMSE describes
how spread out the residuals are when compared to the MAE. Lower residuals indicate better
predictions.
Table 4. Good epoch choices for the FFNN.
Run

Low MSE at epoch

Low MAE at epoch

1
2
3
4
5
6
7
8
9
10

200
400
250
30
30
30
30
120
200
120

150
400
300
30
30
50
40
120
200
100

An epoch choice of 150, close to the average of both columns, seems like a good compromise.
Choosing an epoch to stop training at can be done by plotting error metrics for training and
validation data for each epoch, and evaluate where the validation error plateaus or begins to
increase. An example of this methodology is shown in Figure 16. Most runs behaved similarly
to this example. The validation error does not drastically increase after the plateau, instead it
experiences a slow and steady incline. Therefore, any epoch choice between 100 and 200 would
probably yield a similarly performing model.
23

Figure 16. Training and validation MSE per epoch from run 8. An epoch choice of 120 is
roughly where the MSE is the lowest.
The ensemble models train quickly compared to the FFNN. Therefore, an automated tuning
process and an alternative validation process was implemented. Random search is the chosen
optimization algorithm and will be used on lists of potential hyperparameters, listed in Tables
5 and 6. Explanation of each hyperparameter, along with other parameters that were kept at
default, can be found in scikit-learn’s API reference documentation [42]. There are more
sophisticated optimization algorithms available, but random search works well in this case and
is directly implemented together with k-fold cross-validation in scikit-learn. The algorithm
must iterate and retrain the model a few thousand times to work through the nested loops, but
as training only takes a fraction of a second, it is not an issue.
Table 5. Hyperparameters tested for RF.

Table 6. Hyperparameters tested for GBDT.

Hyperparameter
n_estimators

Hyperparameter
n_estimators

min_samples_split
min_samples_leaf
max_features
max_depth

Possible values
[20, 40, 80, 160, 320,
640]
[2, 3, 4, 8]
[1, 2, 4, 8]
['auto', 1/3]
[2, 4, 8, 16, None]

min_samples_split
min_samples_leaf
max_depth
learning_rate

24

Possible values
[100, 200, 400, 600, 800,
1000]
[2, 3, 4, 8]
[1, 2, 4, 8]
[2, 4, 8, 16, None]
[0.02, 0.04]

The best hyperparameters per run for RF and GBDT together with the final choices of
hyperparameters are listed in Table 7. Each run used 300 iterations of random search and was
cross validated using 5 folds. The final choices of hyperparameters were made based on
information from the ten runs together with some subjective bias. In general, both ensemble
methods perform well with most hyperparameter choices, as the validation error metrics swayed
at most a few percent when changing hyperparameters. The RF algorithm performed, on
average, better with lower values of min_samples_leaf and higher values of max_depth. In
addition, the ‘auto’ choice is particularly interesting, as it corresponds to a 1.0, meaning that all
features are considered when doing node splitting. In this case, the RF algorithm performed
better when behaving like a bagging algorithm in all ten runs. The GBDT algorithm is in general
more sensitive to tune, due to it having a learning rate parameter. The learning rate was tested
separately using trial and error to land on the possible values of 0.02 and 0.04. GBDT performed
better with lower values of n_estimators and higher values of min_samples_leaf and
max_depth.
Table 7. Best found hyperparameter combinations after 300 iterations for RF and GBDT

RF

Run
1
2
3
4
5
6
7
8
9
10
Final choice

GBDT

Run
1
2
3
4
5
6
7
8
9
10
Final choice

n_
estimators
320
20
80
640
640
20
160
40
160
80
160
n_
estimators
200
100
200
200
200
100
200
200
200
100
200

min_
samples_split
8
3
4
2
2
4
3
4
4
3
3
min_
samples_split
4
2
3
8
3
2
3
4
8
2
3

25

min_
samples_leaf
1
2
2
2
2
2
2
1
1
2
2
min_
samples_leaf
8
8
8
8
8
8
8
2
8
8
8

max_
features
'auto'
'auto'
'auto'
'auto'
'auto'
'auto'
'auto'
'auto'
'auto'
'auto'
'auto'
max_
depth
8
8
None
None
None
None
None
4
None
8
None

max_
depth
16
None
None
16
16
16
None
None
None
16
None
learning_
rate
0.02
0.04
0.02
0.04
0.02
0.04
0.02
0.04
0.04
0.04
0.02

5. Results and discussion
5.1 Data analysis
The data analysis step in this work intends to provide better understanding of the processed data
subset (prior to splitting) using various graphs and metrics, which will aid in evaluating the
final models. As this data has not been used for machine learning before, this step is especially
important.

5.1.1 Value distributions
Figure 17 presents histograms of model input features and output. Ideally, a data set should
contain enough data to cover all variable ranges. An example of a feature with a good value
distribution is the temperature, where the data mostly cover the whole temperature range and
are centered around the middle (500 °C). The other input features have worse distributed data,
shown as large gaps between the bars. In addition, the output (mean oxide layer thickness, or
“Mean” in the figure) is heavily skewed towards lower values.

Figure 17. Histograms for input features and output.

26

The data distributions reflect the fact that the database was originally created with only
documentation in mind. The data was not generated systematically with the intention of being
used for machine learning. Heavily skewed distributions, such as the output, can be corrected
by performing a logarithmic transformation of the values. This was tested, but the
transformation did not result in more accurate models, possibly due to the many experiments
without any corrosion (a mean oxide layer thickness of zero). Other changes to the data set were
also tested, with the intention of improving the distributions. An example is the removal of
experiments with ash mixtures consisting of single salt compounds, for example 100% NaCl or
100% KCl. The tested changes were not successful in producing better models, possibly due to
most of them requiring the removal of experiments, thus shrinking the database.

5.1.2 Feature correlation
Analyzing how the input features and output relate to each other is useful for understanding
how the experiments were conducted. This can be done using a correlation matrix, as shown in
Figure 18. This figure was created by calculating a Pearson correlation coefficient for every
combination of pairs. The coefficient measures the linear correlation between two sets of data.
A value of 1.0 corresponds to a perfectly positive linear correlation, and -1.0 corresponds to a
perfectly negative linear correlation.

Figure 18. Correlation matrix of the input features and output.

27

The mean oxide layer thickness shows some weak correlations with the input features. The
temperature, potassium ratio and chloride ratio have a weak positive linear correlation, while
chromium, nickel and molybdenum have a weak negative linear correlation with the mean oxide
layer thickness. However, there are unexpected correlations as well. The temperature, element
ratios and material components appear weakly correlated with each other. This stems from how
the laboratory experiments are decided on. Materials that are known to be more corrosion
resistant have in the experiments typically been subject to more corrosive test conditions. For
testing materials, there is little practical interest in conducting a weeklong experiment in, for
example, 400 °C with a highly corrosion-resistant material. The outcome will almost certainly
be that there is no corrosion. However, from a machine learning perspective, it is of interest
because the models have no prior knowledge of the corrosion resistance of different materials.

5.1.3 Repeated experiments
The data set contains repeated experiments, which can provide information on output variation.
There are 428 unique combinations of experimental conditions (input feature combinations) in
the database, which consists of 670 samples. Out of these 428 unique combinations, 163 have
been repeated two or more times. These repeated combinations make up 405 samples, shown
in Figure 19. This is a significant portion of the total database and should represent it to some
degree. From the repeated experiments, it is possible to analyze the variation in the mean oxide
layer thickness and, thus, provide information about the expected accuracy of the models.

28

Figure 19. Repeated experiments sorted by within-combination average of the mean oxide
layer thickness. The error bars show the maximum and minimum mean oxide layer thickness
for each unique combination.
Figure 19 shows that there is considerable variation in the output, and that it increases with the
mean oxide layer thickness. Some of the variation most likely stems from how corrosion
severity is measured, but it is difficult to say how much is naturally occurring variation, and
how much is method-specific variation. As these experiments represent a majority of the data
set, it is not unreasonable to assume that the models, if properly implemented, should produce
predictions with a similar variation on unseen data.
It can be observed that the variation seems to decrease above 50 μm and become almost
nonexistent at 100 μm. One reason for the low variation at 100 μm was identified. There are
five samples above 100 μm in the original data set that were repeated. These five samples are
not represented in the processed data set (and not in the repeated experiments shown in Figure
19), as all samples above 100 μm were removed. The removal of the five samples does not have
a large impact on the accuracy of the machine learning models, but it might mislead observers
to perceive the variation as decreasing to zero when the mean oxide layer thickness approaches
100 μm. To clarify, the variation should not decrease to zero at 100 μm, but it does due to the
removal of all experiments above 100 μm. However, the decrease in variation after 50 μm is
still present and is assumed to be due to randomness.

29

An earlier assessment of the experimental method found some variation between identical
samples run in parallel [15]. This variation differed between the steel alloys, with 10CrMo9-10
having the largest variation in the mean oxide layer thickness: 29.3 ± 8.7 µm. This difference
in variation could be attributed to the steel-specific behavior of corrosion layer growth. The
author stated that the oxide layers formed for 10CrMo9-10 was very porous due to the different
layers of iron oxides formed. It is further stated that “if this porosity is taken into account, then
the results are very good”, referring to the reproducibility of the experiments. In other words, if
the oxide layers were non-porous, there would be little variation between the repeated
experiments.
In addition to having Figure 19 to reference when evaluating the models, the mean absolute
error (MAE), the root mean squared error (RMSE) and coefficient of determination (R2) will
be calculated from the variation found in the repeated experiments. The RMSE is the square
root of MSE, and it is used instead of MSE due to it having the same unit as the dependent
variable and MAE. MAE and MSE are described in chapter 4, and R2 is described in section
5.2.
The error metrics are calculated by setting the error to the largest within-combination deviation
from the within-combination average mean thickness, and setting the actual thickness to the
average within-combination mean thickness. These two are shown together in Figure 20, in a
similar style to how the actual model predictions are later presented (see Figure 23 for
reference). Translating the variation into error metrics make it possible to directly compare the
variation to the final model prediction errors. These numbers should be a rough estimation on
the highest achievable accuracy. The idea is that if the models are able to perfectly model the
data, then the only factor preventing the models from generating perfect predictions is the
present variation. The calculated errors from the repeated experiments are the following:
•

RMSE = 11.2

•

MAE = 6.2

•

R2 = 0.72.

These values do not represent the whole data set, nor the exact variation. They merely serve as
estimates of the observable variation and rough estimates on the highest achievable accuracy.
30

Figure 20. Within-combination average mean thickness and the largest deviation from the
mean. The combinations are sorted average mean thickness.

5.2 Model evaluation
To obtain a general picture of the predictive ability of the models, they are evaluated by
comparing the correct testing data outputs with predictions generated by the models from the
testing data inputs. The evaluation metrics MAE, RMSE and R2 are calculated from a set of 10
runs, shown in Table 8. Using results from multiple runs help reduce the effect of sampling
variation. To keep the sample order shuffling random, but reproducible, these runs use different
random seeds than the ones used during hyperparameter tuning.
The coefficient of determination (R2) measures how much of the variation in the output is
explainable by the model. Generally, R2 is between 0.0 and 1.0, with higher values indicating
better predictions. A 0.0 would indicate that none of the variation is explained by the model,
and 1.0 that all variation is explained. Note that R2 alone cannot indicate how well a model
performs, as it depends on the data quality. Modeling data sets with high variation, like the
corrosion database, can be successful even with a seemingly low R2.

31

Table 8. Evaluation metrics from 10 runs.
FFNN

RF

GBDT

Run

RMSE

MAE

R2

RMSE

MAE

R2

RMSE

MAE

R2

1

13.1

8.5

0.65

12.1

7.9

0.71

12.1

8.0

0.71

2

14.6

9.8

0.47

13.1

8.5

0.57

13.0

8.3

0.58

3

17.9

11.3

0.48

14.7

8.7

0.65

14.9

8.5

0.64

4

16.5

10.4

0.58

14.9

9.6

0.66

14.3

9.2

0.69

5

17.7

11.4

0.48

14.8

9.1

0.64

14.6

9.1

0.65

6

17.3

12.4

0.15

13.2

9.2

0.50

13.6

9.3

0.47

7

17.3

10.7

0.59

15.2

9.9

0.69

15.6

10.3

0.67

8

17.3

11.6

0.36

14.4

9.7

0.56

15.0

10.0

0.52

9

15.1

10.2

0.54

13.5

8.8

0.63

12.6

8.3

0.68

10

19.5

12.2

0.37

15.7

10.1

0.59

15.3

10.0

0.61

Average

16.6

10.8

0.47

14.1

9.1

0.62

14.1

9.1

0.62

The differences between the average values for RMSE and MAE shown in Table 8 indicate that
the residuals are spread out, as was expected given the large variation in the data set. The further
away RMSE is from MAE, the larger the variation. It is worth noting that RMSE has a tendency
to increase in relation to MAE with the number of samples (n), as RMSE is bound by MAE ≤
RMSE ≤ n1/2 · MAE [43]. The ensemble models seem to perform better than the FFNN model,
with smaller and less spread-out residuals. Furthermore, a lower R2 than that of the ensemble
models indicates that less variation can be explained by the FFNN. This might be influenced
by the different validation methodology. In hindsight, all three models should have been
produced using the same process. Lastly, RF and GBDT perform nearly identically when
compared over a set of 10 runs. This supports the decisions made during hyperparameter tuning.
Table 9. Performance metrics of the models and estimated limits.
RMSE

MAE

R2

FFNN

16.6

10.8

0.47

RF

14.1

9.1

0.62

GBDT

14.1

9.1

0.62

Estimated limit due to variation

11.2

6.2

0.72

32

The models are compared in Table 9 with the estimated limits of the metrics calculated from
the variation found in the repeated experiments. If these limits were the highest achievable
accuracy, then our models could be further optimized. However, when taking into account the
small data set, bad value distributions and random variation, the models perform satisfactorily.
To study the model predictions in more detail, predictions from a single run are analyzed. Run
5 was chosen due to having metrics close to the average. Multiple approaches to visualize the
variation is done to provide different perspectives of how the residuals are spread out. Figures
21 and 22 plot the residuals and relative errors of the predictions versus the predictions. These
figures show that larger predictions generally have a larger absolute error, but a lower relative
error. The models increasing absolute error mimics the general trend seen in the repeated
experiments. In other words, the models are able to qualitatively reproduce the error trend.
Figure 23 plots the true values and the predictions for the mean oxide layer thickness, with each
sample ordered according to its true value.
Comparing these three figures with the variation found in repeated experiments (Figure 19)
shows that the model predictions are similarly scattered to the variation, and that the prediction
scatter increases similarly to the variation, i.e. with an increasing mean oxide layer thickness.
In addition, Figure 23 shows that the models tend to underpredict the thickness for thicker oxide
layers, and overpredict the thickness for thinner oxide layers. This behavior is suspected to be
caused by the data having fewer samples with higher values, i.e. skewed output of the data
visible in Figure 17 subsection 5.1.1.

33

Figure 21. Residuals versus predicted mean oxide layer thickness from training data (blue,
left) and testing data (green, right).

34

Figure 22. Relative error versus predicted mean oxide layer thickness from training data (blue,
left) and testing data (green, right). Note that some errors are not shown, due to them being
above 300% on the y-axis.

35

Figure 23. True values and predictions from testing data samples. The samples are sorted
according to the true mean oxide layer thickness.
To obtain an idea of how important each feature is to the models when they generate predictions,
a feature importance score can be calculated using various techniques. This is only done for the
ensemble models and not the FFNN, as no technique is implemented in TensorFlow. There are
packages that address this, one being SHAP [44]. However, the feature importance scores
should be roughly similar to the ensemble models’ scores and ultimately only affect the
interpretability of the model, not the prediction accuracy.

36

The “gini importance” or mean decrease in impurity (MDI) is a common technique to calculate
feature importance for an ensemble of trees and is directly implemented into scikit-learn’s RF
and GBDT model classes. It is worth noting that MDI can sometimes be misleading and should
not be taken as an absolute fact [45]. There are arguably better alternatives to MDI [46], but as
they are not implemented into the model classes, they will not be used.

Figure 24. Feature importance for RF (left) and GBDT (right).
The feature importance graphs in Figure 24 show that most input features have a relatively high
importance when the ensemble models decide their outputs. The importance of these input
features, or experimental parameters, is supported by literature on high-temperature corrosion,
such as [5]–[11]. Examples of generally understood phenomena are that increasing temperature
promotes corrosion and that alkali chlorides are corrosive. These scores do not necessarily
explain which parameter contributes the most to corrosion. They merely show how useful the
parameters are to a model when it generates predictions of the output.
The first reason for temperature being the most important feature is most likely due to it having
a definite effect on corrsion, as evidenced by the literature. Another reason might be that the
value distribution of the temperature is better than the other input features. A good distribution
enables the models to more accurately reproduce how temperature impacts the output. The
carbonate-to-sulfate ratio proved to have little impact on the predictions and could be removed
without much, if any, impact on the predictive ability of the models.

37

5.3 Synthetic tests
As shown in the previous section, the predictions are scattered, but seem to follow the general
variation trend displayed by the repeated experiments. To further study how the predictions
change with different inputs, synthetic input data is generated and fed to the models. The
resulting outputs and synthetic input data are plotted in figures 25 and 26. The synthetic data
include changes to temperature, ash composition and material. From these figures it can be
concluded that the models are able to reproduce some of the basics of high-temperature
corrosion, for example, that corrosion rate increases with temperature.

Figure 25. Predicted mean oxide layer thickness for synthetic data. Changes in temperature and
chlorine content are shown on the graph axes. Three values for K/(Na+K) were tested for each
model, represented as columns. Static values are 2.25% Cr, 0% Ni, 1% Mo (representing the
alloy 10CrMo9-10) and 0.1 CO3/(SO4+CO3).

38

Figure 26. Predicted mean oxide layer thickness for synthetic data. Changes in temperature and
chlorine content are shown on the graph axes. Three materials were tested for each model,
represented as columns (elemental components found in Table 2). Static values are 0.2
K/(Na+K) and 0.1 CO3/(SO4+CO3).
These synthetic tests also give some insight into how the different algorithms work. As
described in subsection 2.2.2, RF and GBDT are ensembles of decision trees. This means that
for each synthetic experiment (combination of inputs), a prediction is chosen based on if-else
statements, resulting in a discrete output. Therefore, the ensemble models are unable to
interpolate between their chosen outputs. This makes the graphs look blocky. The lack of data
for values in some features, as shown in Figure 17 section 5.1.1, contributes to the blockiness
and makes the jumps between predictions larger. The blockiness can be reduced by having more
tightly spaced data, i.e. better distributed data. In contrast, the FFNN graphs are smooth because
NNs can interpolate between data, and thus, produce a continuous output. However, this is not
always advantageous. There is an area around 550 °C where the predicted mean oxide layer
thickness decreases. This phenomenon is most likely caused by bad interpolation where data is
lacking. The oxide layer thickness is also heavily skewed towards lower values, further
incentivizing the models to generate lower predictions, in order to minimize the MSE loss
function. Both the ensemble models’ inability to interpolate and the seemingly bad interpolation
by the FFNN can be corrected by collecting more data.

39

6. Conclusions and future work
This thesis explored how machine learning can be used to predict the corrosion severity of
superheater materials subject to a high-temperature environment and synthetic ash. Three
supervised learning algorithms (a feed-forward neural network, random forest and gradient
boosted decision trees) were successfully trained, tuned and tested on labeled experimental data.
The machine learning models were created in Python using open-source libraries. The models
were able to reproduce some of the basics of high-temperature corrosion, such as temperature
and chlorine content in the ash increasing the corrosion rate, and higher concentrations of
chromium and nickel in the alloy decreasing the corrosion rate. In addition, the models were
shown to have relatively good predictive accuracies, when compared with the random variation
found in the data set. The random forest and gradient boosted decision trees models were on
average more accurate than the feed-forward neural network.
The corrosion database was analyzed in detail to better understand its quality and suitability for
machine learning. The analysis included value distributions, correlation between features and
variation found in repeated experiments. In general, the corrosion data was found not well suited
for machine learning, due to problems such as small sample size, badly distributed data and
skewed features.
Subsection 5.1.1 introduced the value distributions, and the histograms revealed large gaps in
the distribution of values for the experimental parameters. Using a statistical methodology for
planning future experiments could help balance the data set and improve prediction accuracy.
Reporting of the corrosion experiments could be expanded upon. Instead of documenting the
mean, median and maximum values and only using the mean value for machine learning, a
seven-number or five-number summary of the distribution could be used in conjunction with a
multi-output regression model. This model would predict an estimation of the oxide layer
thickness distribution, which is arguably more useful that simply predicting the mean thickness.
However, the best alternative is to store the whole distribution. It could later be converted in
case the need arises, perhaps using better methods than number summaries.

40

The large variation in the mean oxide layer thickness seen for the same input parameters could
be evaluated in detail. The first step could include analyzing whether the oxide layer porosity
is the main reason for the variation present in the mean oxide layer thickness, as discussed in
subsection 5.1.3. If a reliable way of measuring the porosity is found, it could potentially be
used to calculate a non-porous oxide layer thickness equivalent.

41

7. Svensk sammanfattning
Minskning av koldioxidutsläpp är en åtgärd som kan motverka den globala uppvärmningen.
Inom energisektorn kan utsläppen minskas genom att övergå från förbränning av fossila
bränslen till förnybara energikällor. Fokus i denna avhandling ligger på ångpannor och hur
förbränning av biomassa har medfört korrosionsproblem.
Biomassa syftar främst på biologiskt nedbrytbara produkter och avfall som härstammar från
jord- och skogsbruk, men även kommunalt avfall. Biomassa som bränsle är heterogent, d.v.s.
bränslets kemiska sammansättning varierar betydligt, i jämförelse med fossila bränslen.
Dessutom innehåller biomassa i allmänhet mer föroreningar än fossila bränslen. Exempel på
problematiska föroreningar är askformande ämnen och metallföreningar, som tillsammans har
en tendens att vid höga temperaturer bilda korrosiva beläggningar på ångtuber, särskilt
överhettaren. Denna typ av korrosion kallas högtemperaturkorrosion.
För att förhindra skadlig korrosion vid förbränning av biomassa måste rökgastemperaturen
begränsas, vilket leder till en begränsad elektrisk effektivitet. Mer korrosionsresistenta
legeringar har utvecklats för att uppnå högre rökgastemperaturer, men dessa brukar innehålla
dyra legeringsämnen som till exempel krom, nickel och molybden, vilket gör dem mindre
attraktiva ur ett kostnadsperspektiv. Att välja rätt legering i planeringsskedet kan vara mycket
svårt, eftersom det finns många parametrar att beakta vid kostnadsoptimering av ångpannan.

Johan Gadolin processkemiska centret vid Åbo Akademi har i flera år undersökt hur
överhettarmaterial korroderas av olika ämnen, temperaturer och rökgassammansättningar.
Syftet med detta arbete är att undersöka hur maskininlärningstekniker kan användas på denna
korrosionsdatamängd för att skapa modeller som kan förutspå korrosionsgraden av olika
överhettarmaterial, samt att analysera datamängdens användbarhet för maskininlärning.

42

7.1 Metoder
Korrosionsexperimenten har utförts genom att en syntetisk aska placeras ovanpå en polerad
provbit överhettarmaterial. Provbiten placeras sedan i en ugn för värmebehandling, som oftast
varar en vecka. Den värmebehandlade provbiten skärs på mitten och tvärsnittet analyseras med
SEM-EDX. Utifrån de producerade bilderna kan det nybildade oxidskiktet analyseras och
tjockleken mätas. Oxidskiktets tjocklek berättar hur mycket provbiten har korroderat. Medel-,
median- och maximivärdet på tjockleken, samt de experimentella parametrarna sparas sedan i
tabellform.
Korrosionsdatamängden innehåller många olika experiment med unika korrosionsmekanismer.
Ett val gjordes därför att endast fokusera på experiment med liknande korrosionsmekanismer.
Korrosionsdatamängden filtrerades så att kvar fanns endast experiment gjorda i en vecka med
vanlig luft som ugnsatmosfär, samt experiment bestående av typiska överhettarmaterial utsatta
för syntetisk aska bestående av alkalisalter. Den ursprungliga datamängden består av 42
parametrar och 2746 experiment, medan den behandlade datamängden består av 7 parametrar
och

670

experiment.

Den

behandlade

datamängdens

parametrar

är

temperatur,

materialsammansättning (krom, nickeln och molybden) och asksammansättning (tre
ämnesförhållanden: K/(Na+K), Cl/(Na+K) och CO3/(SO4+CO3)).
Eftersom Åbo Akademis korrosionsdata är märkta data, d.v.s. data som innehåller både
information om experimenten samt resultaten, används maskininlärningsalgoritmer med
övervakad inlärning. Övervakade inlärningsalgoritmer lär sig samband mellan indata och utdata
(de experimentella parametrarna respektive resultaten) för att kunna förutspå utdata från nya
indata. Med andra ord så kommer maskininlärningsmodellerna förutspå hur mycket en provbit
överhettarmaterial kommer korrodera ifall den utsätts för en viss temperatur och
asksammansättning i en vecka. Ett artificiellt neuronnät (ANN) och två ensemblemetoder,
random

forest

(RF)

och

gradient

boosted

decision

trees

(GBDT),

är

de

maskininlärningsalgoritmer som används. Neuronnätets arkitektur är ett fullt anslutet
framkopplingsnätverk och ensemblealgoritmerna består av beslutsträd.

43

Algoritmparametrarna för neuronnätet och ensemblemetoderna ställdes in på två olika sätt.
Neuronnätets parametrar ställdes in manuellt med valideringsdata, medan ensemblemetoderna
utnyttjade k-delad korsvalidering och automatisk parameteroptimering. Detta ledde till att
datamängden delades i tre delar (70 % träningsdata, 15 % valideringsdata och 15 % testdata) åt
neuronnätet och i två delar (85 % träningsdata och 15 % testdata) åt ensemblemetoderna. De
valda parametrarna efter inställning och validering hittas Tabell 1, 2 och 3.
Tabell 1. Neuronnätets slutliga parametrar.
Parameter

Värde

Dolda lager
Noder (neuroner) per dolt lager
Aktiveringsfunktion i de dolda lagren
Optimeringsalgoritm
Inlärningshastighet
Förlustfunktion
Satsstorlek
Epoker

2
32
ReLU
Adam
0,02
MSE
64
150

Tabell 2. Slutliga parametrar för RF.
Parameter
n_estimators
min_samples_split
min_samples_leaf
max_features
max_depth

Värde
160
3
2
'auto'
None

Tabell 3. Slutliga parametrar för GBDT.
Parameter
n_estimators
min_samples_split
min_samples_leaf
max_depth
learning_rate

Värde
200
3
8
None
0,02

Programkoden till arbetet skrevs i Python. För databehandling och maskininlärning användes
programbibliotek med öppen källkod, varav de viktigaste är NumPy, pandas, Matplotlib, scikitlearn och Keras/TensorFlow.

44

7.2 Resultat
Kvaliteten på datamängden analyseras med hjälp av olika grafer för att få en referenspunkt åt
resultaten. Figur 1 visar hur datapunkterna är fördelade för alla experimentella parametrar samt
medeltjockleken på oxidskiktet. Det kan konstateras att fördelningen av värdena är generellt
sätt dålig, vilket försvårar algoritmernas inlärningsprocess.

Figur 1. Histogram av de experimentella parametrarna och medeltjockleken (”Medel” i
figuren).
Upprepade experiment kan ge en bild över hur mycket brus datamängden innehåller, som i sin
tur visar hur noggranna prediktionerna kan förväntas vara, ifall algoritmerna är korrekt
inställda. Figur 2 visar alla experiment som har upprepats åtminstone en gång. Upprepade
experiment utgör en stor del av datamängden, 405 experiment av 670, och modellernas
prediktioner kan därför förväntas att ungefär följa figuren. Dessutom uppskattades bästa
möjliga värden på felmåtten (se Tabell 4) genom att uppskatta bruset med hjälp av skillnaderna
mellan alla parameterkombinationers största avvikelse samt medelvärde.

45

Figur 2. Upprepade experiment där parameterkombinationerna är sorterade efter
experimentens medelvärde inom kombinationen. Felstaplarna visar det största och minsta
värdet på medeltjockleken för varje kombination.
Modellerna utvärderas på testdatamängden med hjälp av felmått och grafer. Felmåtten som
används är rotmedelkvadratfelet (RMKF), medelabsolutfelet (MAF) och förklaringsgraden
(R2). Tillsammans berättar dessa att prediktionernas fel är relativt stora och utspridda, samt att
ensemblemetoderna ger i allmänhet bättre prediktioner än neuronnätet.
Tabell 4. Modellernas felmått och förklaringsgrad.
RMKF

MAF

R2

ANN

16,6

10,8

0,47

RF

14,1

9,1

0,62

GBDT

14,1

9,1

0,62

Uppskattad gräns på grund av bruset

11.2

6.2

0.72

För att få något konkret att jämföra resultaten med, så kan modellernas prediktionsförmåga
illustreras (se Figur 3 och 4) och direkt jämföras med bruset som finns i de upprepade
experimenten (se Figur 2). Trots att felmåtten är relativt stora, så är prediktionerna bra, eftersom
spridningen i residualerna följer bruset i de upprepade experimenten. Dessutom är felmåtten
inte långt ifrån de värden som uppskattades från bruset.

46

Figur 3. Korrekta värden samt prediktioner av testdatamängden. Experimenten är sorterade
enligt den korrekta medeltjockleken på oxidskiktets.

Figur 4. Residualen mot prediktionen. De blåa (vänstra) graferna är träningsdatamängden,
medan de gröna (högra) graferna är testdatamängden.
47

Prediktionernas kvalitet kan vidare bekräftas genom att generera prediktioner av syntetiska
data, se Figur 5. Dessa grafer visar tydligt att modellerna har lärt sig grunderna i
högtemperaturskorrosion, som till exempel att fenomenet är mycket temperaturberoende, trots
att korrosionsdatamängden innehåller mycket brus. För att öka modellernas noggrannhet
behövs mera och bättre data.

Figur 5. Syntetiska data och prediktioner. Temperatur och relativt klorinnehåll i askan visas
på grafernas axlar. Tre olika värden på den relativa mängden kalium i askan användes, se
kolumnerna. De övriga parametrarna har fasta värden, 2,25 % Cr, 0 % Ni, 1 % Mo (motsvarar
legeringen 10CrMo9-10) och 0,1 CO3/(SO4+CO3).

48

References
[1] Knutson, T., J.P. Kossin, C. Mears, J. Perlwitz, and M.F. Wehner, “Detection and
attribution of climate change,” in Climate Science Special Report: Fourth National
Climate Assessment, Volume I, Wuebbles, D.J., D.W. Fahey, K.A. Hibbard, D.J. Dokken,
B.C. Stewart and T.K. Maycock, Eds. U.S. Global Change Research Program,
Washington, DC, USA, pp. 114–132.
[2] Blanco G., R. Gerlagh, S. Suh, J. Barrett, H.C. de Coninck, C.F. Diaz Morejon, R. Mathur,
N. Nakicenovic, A. Ofosu Ahenkora, J. Pan, H. Pathak, J. Rice, R. Richels, S.J. Smith,
D.I. Stern, F.L. Toth, and P. Zhou, “Drivers, Trends and Mitigation,” in Climate Change
2014: Mitigation of Climate Change: Working Group III Contribution to the IPCC Fifth
Assessment Report, Edenhofer, O., R. Pichs-Madruga, Y. Sokona, E. Farahani, S. Kadner,
K. Seyboth, A. Adler, I. Baum, S. Brunner, P. Eickemeier, B. Kriemann, J. Savolainen, S.
Schlömer, C. von Stechow, T. Zwickel and J.C. Minx, Eds. Cambridge: Cambridge
University Press, 2015, pp. 351–412.
[3] Conference of the Parties to the United Nations Framework Convention on Climate
Change. President, “Adoption of the Paris Agreement. Proposal by the President. Draft
decision -/CP.21.” Dec. 12, 2015, Accessed: Apr. 29, 2021. [Online]. Available:
https://undocs.org/FCCC/CP/2015/L.9/Rev.1.
[4] International Energy Agency, “Data & Statistics,” IEA. https://www.iea.org/data-andstatistics (accessed Apr. 29, 2021).
[5] G. Sorell, “The role of chlorine in high temperature corrosion in waste-to-energy plants,”
Mater.

High

Temp.,

vol.

14,

no.

3,

pp.

207–220,

Jan.

1997,

doi:

10.1080/09603409.1997.11689546.
[6] M. Spiegel, A. Zahs, and H. J. Grabke, “Fundamental aspects of chlorine induced corrosion
in power plants,” Mater. High Temp., vol. 20, no. 2, pp. 153–159, Jan. 2003, doi:
10.1179/mht.2003.018.
[7] B.-J. Skrifvars, R. Backman, M. Hupa, K. Salmenoja, and E. Vakkilainen, “Corrosion of
superheater steel materials under alkali salt deposits Part 1: The effect of salt deposit
composition and temperature,” Corros. Sci., vol. 50, no. 5, pp. 1274–1282, May 2008, doi:
10.1016/j.corsci.2008.01.010.
[8] P. Viklund, “Superheater corrosion in biomass and waste fired boilers : Characterisation,
causes and prevention of chlorine-induced corrosion,” 2013, Accessed: Jan. 26, 2021.
[Online]. Available: http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-120438.
49

[9] M. Broström, “Aspects of alkali chloride chemistry on deposit formation and high
temperature corrosion in biomass and waste fired boilers,” Doctoral thesis, ETPC Report
10-04, Umeå University, Faculty of Science and Technology, Department of Applied
Physics and Electronics, Energy Technology and Thermal Process Chemistry., Umeå,
2010.
[10] U. Kleinhans, C. Wieland, F. J. Frandsen, and H. Spliethoff, “Ash formation and
deposition in coal and biomass fired combustion systems: Progress and challenges in the
field of ash particle sticking and rebound behavior,” Prog. Energy Combust. Sci., vol. 68,
pp. 65–168, Sep. 2018, doi: 10.1016/j.pecs.2018.02.001.
[11] H. P. Nielsen, F. J. Frandsen, K. Dam-Johansen, and L. L. Baxter, “The implications of
chlorine-associated corrosion on the operation of biomass-fired boilers,” Prog. Energy
Combust. Sci., vol. 26, no. 3, pp. 283–298, Jun. 2000, doi: 10.1016/S0360-1285(00)000034.
[12] K. Braimakis, D. Magiri Skouloudi, D. Grimekis, and S. Karellas, “Εnergy-exergy
analysis of ultra-supercritical biomass-fuelled steam power plants for industrial CHP,
district

heating and

cooling,”

Renew. Energy, vol.

154,

Feb.

2020, doi:

10.1016/j.renene.2020.02.091.
[13] A. Zahs, M. Spiegel, and H. Grabke, “The influence of alloying elements on the chlorineinduced high temperature corrosion of Fe-Cr alloys in oxidizing atmospheres,” Mater.
Corros., vol. 50, no. 10, pp. 561–578, 1999, doi: https://doi.org/10.1002/(SICI)15214176(199910)50:10<561::AID-MACO561>3.0.CO;2-L.
[14] K. Lutton Cwalina, C. R. Demarest, A. Y. Gerard, and J. R. Scully, “Revisiting the effects
of molybdenum and tungsten alloying on corrosion behavior of nickel-chromium alloys in
aqueous corrosion,” Curr. Opin. Solid State Mater. Sci., vol. 23, no. 3, pp. 129–141, Jun.
2019, doi: 10.1016/j.cossms.2019.03.002.
[15] M. Westén-Karlsson, “Assessment of a Laboratory Method for Studying High
Temperature Corrosion Caused by Alkali Salts,” Licentiate thesis, Report 08-03, Åbo
Akademi University, Turku, Finland, 2008.
[16] J. Lehmusto, “The role of potassium in the corrosion of superheater materials in boilers
firing biomass,” Doctoral thesis, Åbo Akademi University, Turku, Finland, 2013.
[17] Algorithmia, “2020 state of enterprise machine learning,” 2020. Accessed: Jan. 28, 2021.
[Online]. Available: https://algorithmia.com/state-of-ml.
[18] Algorithmia, “2021 enterprise trends in machine learning,” 2021. Accessed: Jan. 28, 2021.
[Online]. Available: https://info.algorithmia.com/2021.
50

[19] D.

Amodei

and

D.

Hernandez,

“AI

and

Compute,”

OpenAI,

2018.

https://openai.com/blog/ai-and-compute/ (accessed Jan. 29, 2021).
[20] R. Boutaba et al., “A Comprehensive Survey on Machine Learning for Networking:
Evolution, Applications and Research Opportunities,” J. Internet Serv. Appl., vol. 9, May
2018, doi: 10.1186/s13174-018-0087-2.
[21] “arXiv.org e-Print archive.” https://arxiv.org/ (accessed Feb. 09, 2021).
[22] A. Géron, Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow, Second
Edition. O’Reilly Media, Inc., 2019.
[23] T. M. Mitchell, Machine Learning. New York: McGraw-Hill, 1997.
[24] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data
Mining, Inference, and Prediction, 2nd ed. New York: Springer-Verlag, 2009.
[25] Kaggle, “State of Machine Learning and Data Science 2020,” Executive Summary, 2020.
Accessed: Jan. 28, 2021. [Online]. Available: https://www.kaggle.com/kaggle-survey2020.
[26] C. R. Harris et al., “Array programming with NumPy,” Nature, vol. 585, no. 7825, Art.
no. 7825, Sep. 2020, doi: 10.1038/s41586-020-2649-2.
[27] W. McKinney, “Data Structures for Statistical Computing in Python,” Austin, Texas,
2010, pp. 56–61, doi: 10.25080/Majora-92bf1922-00a.
[28] The pandas development team, pandas-dev/pandas: Pandas 1.0.5. Zenodo, 2020.
[29] J. D. Hunter, “Matplotlib: A 2D Graphics Environment,” Comput. Sci. Eng., vol. 9, no. 3,
pp. 90–95, May 2007, doi: 10.1109/MCSE.2007.55.
[30] Thomas A Caswell et al., matplotlib/matplotlib: REL: v3.2.2. Zenodo, 2020.
[31] F. Pedregosa et al., “Scikit-learn: Machine Learning in Python,” J. Mach. Learn. Res., vol.
12, no. 85, pp. 2825–2830, 2011.
[32] Olivier Grisel et al., scikit-learn/scikit-learn: scikit-learn 0.23.1. Zenodo, 2020.
[33] M. Abadi et al., “TensorFlow: A system for large-scale machine learning,”
ArXiv160508695 Cs, May 2016, Accessed: Jan. 27, 2021. [Online]. Available:
http://arxiv.org/abs/1605.08695.
[34] M. A. Nielsen, “Neural Networks and Deep Learning,” 2015, Accessed: Dec. 09, 2020.
[Online]. Available: http://neuralnetworksanddeeplearning.com.
[35] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.
[36] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen, Classification and Regression
Trees. Taylor & Francis, 1984.

51

[37] L. Breiman, “Bagging Predictors,” Mach. Learn., vol. 24, no. 2, pp. 123–140, Aug. 1996,
doi: 10.1023/A:1018054314350.
[38] L. Breiman, “Random Forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32, Oct. 2001, doi:
10.1023/A:1010933404324.
[39] J. H. Friedman, “Greedy function approximation: A gradient boosting machine.,” Ann.
Stat., vol. 29, no. 5, pp. 1189–1232, Oct. 2001, doi: 10.1214/aos/1013203451.
[40] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for Activation Functions,”
ArXiv171005941 Cs, Oct. 2017, Accessed: Apr. 28, 2021. [Online]. Available:
http://arxiv.org/abs/1710.05941.
[41] S. Sun, Z. Cao, H. Zhu, and J. Zhao, “A Survey of Optimization Methods from a Machine
Learning Perspective,” ArXiv190606821 Cs Math Stat, Oct. 2019, Accessed: Apr. 28,
2021. [Online]. Available: http://arxiv.org/abs/1906.06821.
[42] scikit-learn, “API Reference — scikit-learn 0.24.1 documentation.” https://scikitlearn.org/stable/modules/classes.html (accessed Apr. 25, 2021).
[43] C. Willmott and K. Matsuura, “Advantages of the Mean Absolute Error (MAE) over the
Root Mean Square Error (RMSE) in Assessing Average Model Performance,” Clim. Res.,
vol. 30, p. 79, Dec. 2005, doi: 10.3354/cr030079.
[44] S. M. Lundberg and S.-I. Lee, “A Unified Approach to Interpreting Model Predictions,”
Adv. Neural Inf. Process. Syst., vol. 30, pp. 4765–4774, 2017.
[45] “How are feature_importances in RandomForestClassifier determined?,” Stack Overflow.
https://stackoverflow.com/questions/15810339/how-are-feature-importances-inrandomforestclassifier-determined (accessed Nov. 10, 2020).
[46] M. Loecher, “Unbiased variable importance for random forests,” Commun. Stat. - Theory
Methods, pp. 1–13, May 2020, doi: 10.1080/03610926.2020.1764042.

52

