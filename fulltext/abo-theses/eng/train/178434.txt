AI and Game complexity
Game benchmarking by using reinforcement learning

Olav Lillerovde
Master of Science Thesis
Supervisor: Luigia Petre
Faculty of Science and Engineering
Ã…bo Akademi
2020

Olav Lillerovde

38848

â€œAll we have to do is to decide what to do with the time given to usâ€
- J.R.R Tolkien, The Fellowship of the Ring, 1954

i

Olav Lillerovde

38848

ii

Olav Lillerovde

38848

Abstract

The aim of this thesis is to use different reinforcement learning techniques to produce
models that can play video games. The core idea behind reinforcement learning is that an
AI can learn to play a game only by exploring its options inside the game envir onment
without us explicitly telling it how to play. The AI-model receives rewards when it
performs a good action and penalties when performing a bad action.
Games have been and are often used as benchmarks for AI technologies because they make
it easier to evaluate the performance of the AI. By developing AI that can solve
increasingly more complex games it creates opportunities for other real-world applications
for reinforcement learning such as: computing complex 3D structures of proteins, creating
personalised medication treatment plans for patients and predicting early onset retinal
disease.
The algorithms we use are Advanced Actor(A2C) Critic, Proximal Policy Gradient (PPO2)
and Neuroevolutionary of Augmenting Topologies (NEAT). We use these algorithms to
produce AI models for two different games Mortal Kombat and Super Castlevania 4. Both
games were released on the Super Nintendo Entertainment System. The games are loaded
into a Python environment known as gym-retro which simulates a running Super Nintendo
Entertainment System. A Python script hosting a machine learning model is used to interact
with the gym-retro environment and read the situation in game and predict the best course
of action.
The rewards obtained from training are analysed and compared to each other. Some general
statistics are extracted, and a visual evaluation of each modelâ€™s performance and game
strategy is done.
Keywords: Reinforcement learning, A2C, PPO2, NEAT

iii

Olav Lillerovde

38848

iv

Olav Lillerovde

38848

Acknowledgement
I want to thank everyone that listened to me talk about my thesis idea and encouraged me
to write about what I was passionate about. I want to thank my supervisor, Luigia Petre for
all the help and guidance throughout this whole process. I also want to thank, Amalia
Skrifvars for listening to my long ramblings regarding this topic and encouraging me to
continue when the writing process became difficult. In addition, a huge thanks to Amalia
for helping me proofread, I do not think it would have been done in time if not for your
help. My sincerest thanks to everyone who listened, without you, there would be much
fewer games in my thesis.

v

Olav Lillerovde

38848

List of contents
1. Introduction ............................................................................................................................ 1
2. Background ............................................................................................................................ 4
2.1 Deep blue.......................................................................................................................... 4
2.2 AlphaGo and AlphaGo ZERO ......................................................................................... 5
2.3 AlphaStar .......................................................................................................................... 6
3. Resources ............................................................................................................................... 8
3.1 Stable baselines ................................................................................................................ 8
3.2 Gym-retro ......................................................................................................................... 8
4. Machine Learning .................................................................................................................. 9
4.1 The steps of Machine Learning ...................................................................................... 10
4.1.1 Data Collection ........................................................................................................ 10
4.1.2 Data Preparation ...................................................................................................... 11
4.1.3 Choosing a model .................................................................................................... 12
4.1.4 Train the model ....................................................................................................... 12
4.1.5 Evaluate the model .................................................................................................. 14
4.1.6 Parameter tuning ..................................................................................................... 16
4.1.7 Make predictions ..................................................................................................... 16
4.2 Supervised Learning ....................................................................................................... 17
4.2.1 Linear regression ..................................................................................................... 18
4.2.2 Logistic regression .................................................................................................. 22
4.3 Artificial Neural Networks ............................................................................................. 24
4.4 Reinforcement Learning ................................................................................................. 27
4.4.1 NeuroEvolution of Augmenting Topologies (NEAT) ............................................ 29
4.4.2 Advantage Actor Critic (A2C) ................................................................................ 32
4.4.3 Proximal Policy Optimization PPO2....................................................................... 33
5. Implementation and Results ................................................................................................. 36
vi

Olav Lillerovde

38848

5.1 Setting up the gym-retro platform .................................................................................. 36
5.2 Super Castlevania 4 ........................................................................................................ 41
5.3 Mortal Kombat ............................................................................................................... 51
6. Conclusion and future work ................................................................................................. 61
7. Svensk Sammanfattning ....................................................................................................... 63
8. Sources ................................................................................................................................. 70

vii

1. Introduction
Many people play games and know games, but it can be difficult to properly define what a game
is. A game can be seen as an activity or sport with a set of constraints or rules often with a goal
condition that determines when it is over. Sometimes there is even a scoring system that rank
the performance of participating players. Because of this, games can be seen as a set of
constraints set upon an environment with a scoring system based upon the participantsâ€™
interaction within the environment given the constraints and finally with a predetermined done
condition that will mark the end of the session. This might be a very reductionist definition and
even somewhat vague. The point is to illustrate that most activities can be viewed as a game if
we properly define the conditions, scoring, constraints, and environment. In this way, saving
money can be a form of game: a game session is a month, the environment is real life, a player
is the person wanting to save money, there are no constraints, but scoring is done by viewing
how much less money was spent in comparison to a regular month. This approach is called
gamification, in which other activities, for instance, education receive game aspects and the
participants are more motivated to achieve a better result because of the way it was presented
to them. It is therefore a growing thought that if we can develop an artificial intelligence (AI)
that can perform well in games then we can create an AI that can perform well in other areas as
well. AI is the over encompassing concept to create a machine that can simulate human
intelligence and behaviour. Machine learning is a branch in computer science focusing on
developing systems that can improve from experience without being explicitly programmed.
By exploring how well an AI can tackle a complex game we can use the same approaches to
develop AI that can tackle other complex problems. We can do this by gamifying the complex
problem into a â€œgameâ€ that the AI can become proficient at.
It can be difficult to evaluate the performance of a trained reinforcement learning model, but
games are usually structured with clearly defined goals and scoring systems. This makes them
excellent for benchmarking AI performance because, in theory, a higher score will indicate that
the game is played better. In addition to this, many games have a winner and loser dynamic,
which essentially means that the game will end when one player has outperformed the other
players. This provides an easy method for evaluating the performance of two performing AImodels, or if an AI can beat a human player.

1

Olav Lillerovde

38848

There are many factors to determine when assessing the complexity of a game. The state-space
complexity of a game is the number of legal game positions reachable from the starting position
in the game (1). Game trees are the total number of possible games that can be played from the
starting position. Another way to think about it is to think the total number of leaves in a game
tree with root position being the initial position. Upon making moves in each game we diverge
from the initial root position and enter subtrees of the game tree. The leaves of this subtree have
labels which indicate â€œWINâ€, â€œLOSSâ€ or â€œDRAWâ€ (and more). Therefore, we can establish
that some paths are more likely to win than others, however, in a multiplayer game we can only
choose the decision for our own action and therefore the path will dynamically change based
on both playersâ€™ decisions. A minmax search can be executed to determine the value of being
in a given situation based on the end leaf points. This establishes the notion that some paths
will inevitably lead to failure and some will lead to victory. The problem is that there are games
that are so complex that searching the entire game tree is increasingly becoming a hardware
and computational problem. Another way of solving this problem is to introduce artificial
intelligence that can learn to play the game in such a way that the choices made by the AI leads
it onto a subtree that incidentally corresponds to the most rewarding subtrees of the game tree.
If we can develop artificial intelligence that can solve incredibly complex games, we can use
those same techniques and approaches to solve complex real world problems.
General purpose artificial intelligence (GPAI) is the ability of an AI to perform a task in the
same manner a human would perform it. Developing GPAI means that when the AI is training,
it should have the same or similar constraints as humans do. AI can perform tasks that humans
cannot at a rate that is unprecedented, humans can in some cases perform better with scarcity
of data or language-related tasks. Furthermore, the data the AI trains on is highly filtered in
contrast to the environmental surroundings that affect a human. An AI playing a game can only
see the game, but a human will hear different sounds from the environment, smell perhaps, and
other factors that impact the general nerve system. In that sense, AI becomes highly focused on
one task with little or no regard to the context or affected by noise. Researching and developing
GPAI is important, because it enables the AI to intelligently perform its task based on context.
The thesis begins with a brief description of game complexity striving for general purpose AI,
and the benefits provided by this endeavour.
In the background chapter, we discuss major AI vs professional human players in three different
games. We briefly discuss the development and impact of each event and touch upon the human
vs. machine element present within this topic.
2

Olav Lillerovde

38848

The Machine Learning chapter entails a basic overview of different machine learning
approaches. We explain the essential steps taken in machine learning training. Next, we
initialize the basis of pattern searching and idea in machine learning with an in-depth look at
linear regression and logistic regression. We then discuss the development and structure of
artificial neural networks and how they tie together with reinforcement learning. Finally, we
take a deep look into the three main machine learning techniques used in the experiments done
in this thesis: A2C, PPO2 and the NEAT algorithm.
The chapter Implementation and results briefly discusses setting up the environment for the
games and then details all the steps needed to carry out the experiments: finding memory values,
setting up a training environment, initializing reward functions and game states. We look at two
different games: â€œMortal Kombatâ€ and â€œSuper Castlevania 4â€ both for the Super Nintendo
Entertainment System. An introduction to both games is given and an overview of the gamesâ€™
goals and controls are given. We discuss the design choices behind the reward functions and
then discuss the results. The results are coupled with graphs detailing the development of
performance in the three different machine-learning approaches previously mentioned. A
detailed visual analysis is discussed in relation to the graph and some game strategy analysis is
done to explain the nature and meaning behind the actions of the AI.

3

Olav Lillerovde

38848

2. Background
In this chapter, we discuss some of the major milestones in software programs playing highly
difficult and complex games at a professional level. We discuss how it was developed, as well
as what game was played and the impact of the result from the event itself. We also briefly
discuss the human element in games and what it means in the context of human vs. machine.

2.1 Deep blue

Deep blue is the name of a chess-game computer developed by IBM. Most notably Deep Blue
is known for defeating the world champion (Garry Kasparov) in chess in 1997 in a best of sixgame match. The games resulted in two wins for Deep Blue, one win for Kasparov and three
draws. The event itself received much media attention and discussed the narrative of man vs.
machine. However, Deep Blue was also an important milestone and push for the capabilities of
computers. Although there had been chess-playing computers in the past, they had not been as
successful as Deep Blue (2).
Deep Blue used customised â€œchess chipsâ€ to execute an alpha-beta search algorithm in parallel.
This was a brute force search approach and one of the developers also stated that it was not
artificial intelligence, instead that it was â€œartificial intelligenceâ€ by super-human calculating
speed and brute force (3) capable of calculating 200 million moves per second, or 50 billion
positions in three minutes. Deep Blue did not use intelligent behaviour to win, instead it had
the processing power to quickly calculate the best move.
This development leads to a question: is there a reason for humans to play these games
competitively if a computer can do play it better? Kasparov mentioned in a TED-talk that the
world of chess still wants a human champion, and even though there are mobile apps with chesscomputers that are stronger than Deep Blue, people are still playing chess. He proclaims that
machines have instructions and objectivity, but humans have passion and purpose (4).

4

Olav Lillerovde

38848

2.2 AlphaGo and AlphaGo ZERO
In 2016, Google Deep Mind developed an AI called AlphaGo. The AI was developed to play
the ancient complex Chinese boardgame of Go. In Go, two players take turns, using either black
or white stones, to strategically create spaces of territory. Once all possible moves have been
played, the player with the most captured area wins. There are other rules as well, but the gist
of the rules is that it is an easy-to-learn game but also an incredibly deep and complex game.
The game has 10170 different possible board configurations, which makes it more complex than
chess.
Deep Mind created a program called AlphaGo that combined advanced search trees with deep
neural networks. AlphaGo was first introduced to Go by showing it amateur Go replays and
teaching it with a technique called â€œimitation learningâ€. After learning the basics of how
humans played Go, AlphaGo trained against itself for thousands of games. AlphaGo would
improve itself upon each game it played against itself. This process increased the proficiency
of AlphaGo until it defeated the greatest Go player at the time, Lee Sedol. AlphaGo received a
professional rank of 9th dan (the highest certification) for this match. Some moves made by
AlphaGo in the matches were considered so new and surprising that they perplexed expert GO
players.
After the match with Lee Sedol, Deep Mind expanded upon AlphaGo, and created AlphaGo
Zero. AlphaGo Zero would learn Go from scratch without learning the basics from imitation
learning. It began with zero experience and knowledge about the game, it instead learned to
play by exploring the game and playing against itself, it then moved on to learning to play
against AlphaGo. AlphaGo Zero trained for approximately 40 days and played 29 million
games during this time. In an inside evaluation performed by Deep Mind, AlphaGo Zero played
100 games against AlphaGo (slightly improved version from AlphaGo Lee Sedol), and won
89â€“11 (5) (6).
AlphaGoâ€™s victory was a major milestone in the AI research community. Go was for many
considered to be the next challenge after Chess.

5

Olav Lillerovde

38848

â€œI have grown through this experience; I will make something out of it with
the lessons I have learned. I feel thankful and feel like I have found the reason
I play Go. I realize it was a really good choice to play Go. It has been an
unforgettable experienceâ€
â€“ Lee Sedol in AlphaGo documentary 1:25:33-1:26.04 (7)

Before the summit, Lee Sedol was confident he would win most of the games. In the end, the
final score was 4â€“1 in favour of AlphaGo. The experience humbled him, as read in the quote
above, and like Kasparov, Sedol seems confident and optimistic about the future of Go.
However, on 19th November 2019, Lee Sedol announced his retirement from professional play.
He stated that he could not become the top overall player due to there being a new entity that
could not be defeated referring to emerging AIs playing Go. He also stated that his career has
been long, spanning over 24 years and that he thought it would be difficult for him to compete
with the younger generations of Go players (8) (9).

2.3 AlphaStar
After the success of AlphaGo, Google Deep Mind decided that their next challenge would be
the seemingly impossibly complex game StarCraft 2 (10). StarCraft 2 is a science fictionthemed real-time strategy game developed by Blizzard Entertainment (11). Since its release,
the game has had two expansion packs released with additional content. In the game players
must manage high-level economic decisions while simultaneously managing up to 200
individual units. There are many different strategies in StarCraft 2, each with different counter
strategies. StarCraft 2 also offers players three unique factions to play as, each with its own
advantages and disadvantages. In combination, they create an incredibly complex strategy game
with many different playstyles, which in return yields a highly competitive e-sports scene.
There is also imperfect information on the game state and each player needs to explore and
gather information about his opponent to determine the optimal strategy against his strategy. At
every time step, AlphaStar will receive an observation that includes all the observable units and
their attributes. This observation is imperfect and will only include the observable space of

6

Olav Lillerovde

38848

which AlphaStar can â€œseeâ€. From the observation AlphaStar had an approximation of available
actions equal to 1026, comparing it to Go in which the available actions are 361 (12).
Previous games mentioned before were turn-based strategy games but in StarCraft 2 decisions,
strategy and tactics need to be executed in real time. The game state dynamically changes, and
players need to react to these changes with a limited supply of attention. Actions per minute
(APM) is a representation of how many clicks and actions that a player performs on average
per minute throughout a game. The best players can achieve a high APM and have each of those
actions be meaningful actions, but they are still limited by how much information from the
game they can meaningfully take in, decode, and respond to. Because of the vast number of
factors to consider in each game, it becomes an art form for a professional player to correctly
play efficiently despite of his or her limitations. Deep Mind wanted to mimic the limitations of
humans in the implementation of AlphaStar to maintain the nature of the game. AlphaStar has
the following constraints: delays caused by network latency, computation time, limited APM
(peak APM lower than humans) (12).
AlphaStar was trained by using imitation learning to establish a baseline on how to play. It was
then trained using a multi-agent training league in which it competed against different versions
of itself. To have the AI remain flexible and adaptable, Deep Mind introduced exploiter agents.
These agents develop strategies that try to exploit the weak points of the main agentsâ€™ strategy.
AlphaStarâ€™s preliminary version (without any limitations) played and won a show match with
five games against two professional StarCraft 2 players, â€œTLOâ€ and â€œMaNaâ€. Deep Mind then
put three versions of AlphaStar on the European server to obtain a match-making rating (MMR)
to evaluate it against humans. AlphaStar Final (final version) achieved an MMR of 6336 and
reached the rank of â€œGrand Masterâ€ on the league placement at the time, which is the highest
rank. This MMR score puts AlphaStar higher than 99.98% of all the players that played in this
league (about 90,000 people). As a reference, the Finnish professional player â€œSerralâ€ (number
1 ranked player in Europe) currently has an MMR of 7438 (13).

7

Olav Lillerovde

38848

3. Resources
In this chapter, we briefly discuss some very important resources we use when we set up the
training environment for the experiments.

3.1 Stable baselines

Stable baselines (14) is a Python package based on the OpenAI baselines. The package is a set
of improved implementations primarily focused on the reinforcement learning algorithms found
in OpenAI baselines. The package is very intuitive to use and offers many reinforcement
learning algorithms with minimal setup. Stable baselines take a vectorised version of the game
state provided by Gym-retro and applies the reinforcement learning on it. Stable baselines is
very quick and easy to set up, which makes it easier to test different training solutions. The
package also offers a built-in call function that enables saving important data at regular
intervals.

3.2 Gym-retro

Gym retro is a platform for reinforcement learning research on games (15). Gym-retro is a
Python package that sets up a game environment in which we can interact, using Python code.
Gym-retro has an impressive number of older games supported with premade customisation
files; it also allows for adding new games to the platform and documentation on how to do so.
An environment is set up using the game-files, a start state for the game, and a scenario
configuration. We will discuss this in depth in the implementation chapter. There are several
advantages to using Gym-retro, as it allows for easy implementation and takes care of setting
up the game environment. It also makes it easy for us to use existing Python libraries such as
stable-baselines and NEAT to perform reinforcement learning algorithms.

8

Olav Lillerovde

38848

4. Machine Learning
Machine learning (ML) is a subfield of Artificial Intelligence focusing on algorithms and
methods that make computers â€œlearnâ€. We can define learning as the ability to improve a on a
task or skill through experiences. Unlocking the learning potential in computers enables new
levels of competence and performance in a vast number of fields.

Figure 1: The relationship between AI, ML, RL, DL and DRL (56)

Computers can learn from patterns in medical data and suggest and provide more personalized
treatment options for a patient. Computing personally targeted treatment plans will increase the
quality of care for each patient and reduce the dosage errors when prescribing medicine.
Computers can learn to simulate and predict complex protein 3D structures in order to aid
scientific discovery in structural biology (16). This would provide a more thorough
understanding of the molecules that make up the human body. This understanding provides
methods for scientists to develop medicines that work with the unique shape of any proteins.
Computers can learn to analyse texts and translate them into a given language. This opens huge
communication barriers and would provide high-quality reading material in a given language.
Computers can learn to play highly complex and competitive games and there are already
examples of such computers defeating seasoned professional players. This means that a
9

Olav Lillerovde

38848

computer can achieve significant performance within a closed environment with highly
complicated rules and variables to consider. The complexity of these games provides a
benchmark on the level of complexity the computer can handle while having similar player
capacities as humans. ML can benefit a diverse range of applications and could significantly
improve quality of life for everyone.
If the performance of a program is increased by some experiment or iteration, it is said to have
increased its capabilities and learned from this experiment. It is therefore critical to properly
define how to measure performance, what the task is, and what kind of experiment the program
is learning from. These topics will be illustrated in more detail later in this chapter.

4.1 The steps of Machine Learning
There are certain steps that are common to most problems within Machine Learning as well as
Data Science projects. Although the methods can vary widely among themselves, the essence
remains the same.
4.1.1 Data Collection
This step refers to the collection of data that will be used for analysis. There are several ways
to collect data such as measuring, gathering, using polls and other tools. The information that
was gathered is then put into a new dataset detailing the measurement. Using an existing dataset
could be considered, because it saves time for the analyst and development can begin earlier
but this is not traditionally referred to as data collection. There can be several obstacles for a
private team to collect data, such as the scope of the project, how many people a private team
can reach, or if the data is personal. The same information could have already been collected
by a government organisation and might have been recorded for a long time. Several
organisations and governments release statistics and data on previously recorded events or
country measurements. Different international organisations also regularly release datasets that
can be downloaded free of charge, and community-based data science websites Kaggle (18)
provide a hub for data scientists to analyse and discuss their findings online. Several
organisations also provide APIs that enable users to request a subset of the organisationâ€™s data.
For instance, Spotify allows users to request a dataset containing information about different
bands. Such a dataset contains each song, album, popularity among users, and several other
features for hobby data scientists to enjoy. The drawback of using an existing dataset is that
there might be missing values and it might not contain the relevant data for analystsâ€™
experiment.
10

Olav Lillerovde

38848

Creating a dataset and recording the values can be done to acquire relevant information for a
specific project. This approach is good because it allows the analyst to acquire a dataset that is
relevant specifically for an experiment. There are several ways to record new data, whether by
physically measuring with a stopwatch or with a ruler and writing down the results. Another
way is to collect data from reputable sources and combine it, provided the data can be combined.
It is also possible to create surveys and distribute the survey to collect information about a
relevant topic. There is also the option to use software to record information on the computer,
such as images, frames or sounds. However, personally recording the data does not make it any
less prone to errors or missing values. There will always be some discrepancy of information;
it is therefore important to consider the collected data and evaluate if it is representative and
relevant for the â€œbigger pictureâ€.
4.1.2 Data Preparation
The data that is collected during the data collection step is seldom complete. There could be
data missing in the dataset or the data could have the wrong layout or format. Missing values
means that one or more entries for some row are not specified, possibly due to data not being
released, or even because no measurement was done. It is important to note that, unless the
reason is given, it is difficult to determine why a particular label is missing: it might be an error,
or it might be unreleased information.
There are two main possibilities to deal with missing data. The first one is to replace the missing
values (sometimes called â€œnullâ€ values) with other values that are deemed suitable, for instance
the average of the existing values in other rows, or certain agreed upon baseline values, such as
the number zero. The second one is to exclude from the analysis all rows containing missing
values. If the number of missing values rows is not significant with respect to the dimension of
the dataset, then this is a reasonable solution; if, for instance, the rows with missing data cover
half the dataset, then this is not a good solution. For example, an inspection of a dataset
containing CO2 emissions over time for different countries reveals that numerous missing
values have been included in the timespan 1960-1975. Some countries in the dataset include
data during this time, but numerous other countries do not show any data during the same
duration. Instead of the missing value for CO2, we could add the number 0 or the average of
the other existing measurements. In both cases, we skew certain statistical approaches in the
wrong direction [3,4]. We might also only analyse the data for the countries that have complete
CO2 entries; this might be the optimal strategy, except that, if half the countries are removed
because of missing values, our analysis is only representative of the remaining countries.
11

Olav Lillerovde

38848

Sometimes the dataset provided can be in the â€œwrongâ€ format. If we use the example in the
previous paragraph, we can imagine different ways of listing the years. The years can be listed
in a column called â€œyearsâ€ and have all years listed in the rows beneath. Alternatively, each
unique year could have its own column and the respective data could be put in the appropriate
row beneath. The years could also be separate from the columns, which could make them nondefining properties of the dataset. If the dataset is in the wrong format, it can lead to difficulties
when performing an analysis. In this case, the dataset needs to be transformed to fit our criteria.
Transforming the data means that we transpose or rearrange columns and rows into a format
that fits our criteria. For instance, what if instead of labels, countries are listed as features instead
of labels, then the analysis will be difficult to perform. In this case, the data will need to be
transformed. Transforming the data means transposing or rearranging the data into a format that
is more useful for analysis.
4.1.3 Choosing a model
The machine learning model can be defined as the artifact that is created by the training process.
The training process uses a machine learning algorithm to train on training data.
Choosing a machine learning model has much to do with understanding the data and
understanding the problem at hand. Different machine learning approaches solve different
problems. Therefore, understanding what the problem is, what the data represents, and what
kind of solution is expected is key to choosing a model. There are several broad categories
within ML that have their own techniques and methods that suit a specific type of problem.
Some of these categories are supervised learning, unsupervised learning and reinforcement
learning. We will discuss these categories in more detail in later chapters. Each category uses
different approaches and assumptions about the data and it is therefore important to choose the
correct category and the correct model.
4.1.4 Train the model
Typically, the dataset is split into two parts, a training set and a test set. The training set is used
to train the model and the test set is used to evaluate the performance after training. Sometimes
a validation set is also included, which is used to estimate prediction error model selection (57).
As a significant amount of data is needed to train a model, the training set is larger than the test
set. The ratio is usually between 75 and 80% training and the remainder as the test set. The test
set will remain unseen by the model until after the training is complete.
We will now establish some notation for future chapters. The label xi denotes the input variables
and yi denotes the target value to be predicted. A pair (xi , yi) is called a training example, and
12

Olav Lillerovde

38848

a training set consists of m training examples. The training set is defined formally as {(xi , yi) ;
i = 1,â€¦m }. The i notation is used to describe the index of the current value. Let X be the entire
space of input values, and let Y be the entire space of output values. The hypothesis h is used
to describe the function that predicts the target value y with a given input x. During the training
process, we also have some variable denoted Î¸, which is a hidden variable that will be calibrated
when the model is trained. These parameters are sometimes referred to as weights and serve as
calibrators in the hypothesis function mapping.
A cost function is used to determine the difference between the label returned by the training
algorithm and the label in the dataset. A parameter is a numerical or another measurable factor
that sets the conditions of its operator. In the context of the cost function, it is a variable whose
ideal value is not yet known. By trying different values for the parameter, we can calibrate it to
find the variable that best determines the correct label. Another term sometimes used for
calibrating is tweaking, which means to lower or increase the value of the variable. By tweaking
the variable and checking the results, we can determine which direction provides a lower overall
cost. The cost function will yield a numerical (the cost) that is proportional to the parameterâ€™s
ideal calibration. If the cost function yields a high number, then the cost of the current parameter
is high, and it means the calibration is not ideal. The intuition is that tweaking the parameters
to obtain a lower cost will result in the best calibration of the model.
Different models will have different methods for tweaking the parameters. There are several
challenges when deciding how to tweak the parameters, such as if the tweak should be negative
or positive, and how large the tweak should be. Typically, a model will have some mathematical
function that includes the cost function and iteratively looping a step that calibrates the
parameters. This loop will continue either to a convergence point or for a set number of
iterations. At this point, the model is trained.
There are different definitions of a convergence point for different machine learning techniques.
A useful technique to apply during this stage is to plot the cost function over how many
iterations the loop performs. Ideally, the graph should begin high and be decreasing over time
as the cost drops, then at some point the cost function will be reduced by very small amounts
as it closes in on the local minimum. If the graph is increasing over time, then it would signify
that the algorithm is overshooting the local minimum when taking a â€œstepâ€ towards the local
minimum. We will discuss these phenomena in later chapters. Regardless of the approach, using
this plotting technique is a good method to check if the algorithm is converging towards its
goal.
13

Olav Lillerovde

38848

4.1.5 Evaluate the model
When the model is trained, it is time to evaluate the performance of it. The model will be given
the input of the xi values in the test set and asked to predict the outcome value for a given xi.
The performance can be measured in terms of how good the model is at predicting the correct
values in respect to the labels yi.
Actual
Prediction

Positive

Negative

Positive

True positive

False positive

Negative

False negative

True negative

Table 1 Evaluation table. Showing the predicted value and the actual value in a comparison. A true positive is when the
prediction is equal to the actual value, and that value is positive. If a prediction is positive but the actual value is negative, that
would be a false negative. A similar approach is done with the other values.

The table above illustrates the table of outcomes possible when making predictions for the test
set. If the predicted label with regard to the input xi is positive, and the respective label yi is
positive, it is then considered a true positive. If the respective label yi is negative, it is a false
positive. The same kind of logic applies for the negative values (19). The following metrics are
described on binary data, but the same techniques can be used on regression models with some
changes, for instance, declaring a range which is considered true positive (error margin).
Some metrics can be calculated from Table 1 Evaluation table such as:
ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =

ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘  + ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ 
ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘  + ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘  + ğ¹ğ‘ğ‘™ğ‘ ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘  + ğ¹ğ‘ğ‘™ğ‘ ğ‘’ ğ‘ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ 

The equation above (Accuracy) refers to the accuracy of the model. It is the probability that the
model will predict a correct label, either positive or negative, on a given input xi. However,
although this is the most intuitive approach it can give a false sense of high performance. Given
a dataset with skewed data, meaning that there is not a distribution of positives and negatives
in the training and test set, this can create instances in which the accuracy is wrongly set to
being very large. Consider a case in which the majority of the cases were negative, and the
model successfully predicted the majority of true negative values. Due to the inflated value
from the true negatives, it would be difficult to determine if the model learned to successfully
predict a true negative, or if it learned to predict a negative in most cases. Therefore, it is
important to use other metrics as well when evaluating the performance of a model.

14

Olav Lillerovde

38848

ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =

ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ 
ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘  + ğ¹ğ‘ğ‘™ğ‘ ğ‘’ ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ 

The equation above (recall) is also referred to as the true positive rate. It will describe the
probability that the model will predict a correct positive label. By replacing the â€œpositivesâ€ with
â€œnegativesâ€ in the table above, the true negative rate can be calculated. The equation will give
the fraction of the cases in which the prediction was correct.
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =

ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ 
ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘  + ğ¹ğ‘ğ‘™ğ‘ ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ 

The equation above (precision) refers to, given all positives (predicted and true), what fraction
are true positives. In other words, both recall and precision provide more information about
what errors the model makes. It is therefore ideal to aim for a high score in both precision and
recall. However, even these metrics can be misinterpreted due to skewed values.
There is also a metric named F1 score which refers to an equation that yields the harmonic mean
between precision and recall. It takes precision and recall as parameters and outputs a balanced
mean between these values.
ğ¹1 = (

2
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› âˆ— ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
)
=
2
âˆ—
ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™âˆ’1 + ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›âˆ’1
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

Using the F1 score for a metric gives a better intuition of the modelsâ€™ performance. Although
the F1 score is considered a good metric for evaluating both precision and recall, one critique
is that the formula gives equal importance to both precision and recall. It is perceived as a
conceptual weakness that there is no parameter detailing the weight of precision (20).
When assessing the performance of these metric scores, a higher score will indicate a better
model. The range of the metrics score is between zero and one, where a model with a score of
one would be a perfect model. However, if a model were to receive a score of 1, it would signify
that the model has successfully predicted all the labels in the test set. It is important to check
for errors during this stage, or an explanation to why the model was perfect. One error that can
occur is that the model is given a test set that is equal to the training set. This would make the
model make perfect predictions for the given test set, however, when faced with new values
from new data the performance would likely drop. Similarly, if the metric score is zero, then
the model might have trained on data that was not representative of the test set, or it might not
have had enough data to learn how to make successful predictions. Usually a good model will

15

Olav Lillerovde

38848

have a score of 75% or above. A score that is close to a 100% is possible, but it is important to
check for errors and understand how and why it retrieved these results.
4.1.6 Parameter tuning
During the step, the model is trained and the results of the metrics point towards a good or a
bad model. If the score is not satisfactory, then some parameters need to be tuned. There are
several different machine learning approaches that use different amounts of parameters and the
knowledge on how to tune the parameters for a given machine learning method comes with
experience. The general intuition is that there are parameters given by the analyst that will affect
the machine learning training process, and it is difficult to know what the correct number for a
given parameter is at the beginning of the experiment. The parameters can be tuned and changed
in order to achieve a better result. Depending on the problem there can be considerable fine
tuning in order to achieve better results.
4.1.7 Make predictions
When the other steps are completed, the model will be trained and fine-tuned. The model is
then ready to perform predictions for new data that falls within the same category as the training
data. The model can be implemented in different software and be given user input and make a
prediction on the given data. For instance, if an image classifier model is trained to classify a
picture as a certain kind of animal, then the user could input a picture of an animal, and the
classifier will make a prediction on what kind of animal it is.

16

Olav Lillerovde

38848

4.2 Supervised Learning
The most widespread type of ML is called supervised learning (SL). The most common
intuition illustrating SL is to think that the model is learning from labelled data. Labelled data
is a collection of data points with associated values that are known beforehand.
For supervised learning, we divide the features into input and output features. A more formal
explanation follows:
Let X be the space of input values and Y be the space of output values. Let x âˆˆ X and y âˆˆ Y.
The value x(j)i and y(j)i represents the iâ€™th value in the jâ€™th column. A pair (x(j)i, y(j)i) is then a
training example. Then the goal is to learn a hypothesis h : Xâ†’ Y (21), that when given a new
unlabelled datapoint (xn) will predict a label yn. In practice, this means that the features in x will
be used to produce a value y. The features in X will be the basis on how to train the hypothesis
h to produce a certain value y. Usually, the data X and Y are split into two (or more) parts: a
training set and a test set. The training set is used to train the hypothesis h and the test set is
used to measure the accuracy of the trained hypothesis. The training set can be formulated as
the training example (xi,yi) with a list of m entries {(xi, yi); i = 1,â€¦,m}.
The reason for the split is that both the test set and the training set need to include representative
amounts and labels for the relevant dataset. If the trained hypothesis predicts values y that are
like the original case of x, then the hypothesis is considered to be good.
Depending on the type of machine learning problem the output y will be different. For instance,
if the hypothesis h should predict a continuous value, then we have a regression problem. For
instance, the hypothesis h is supposed to predict the number of umbrellas sold in a given month
depending on the month and amount of rainfall (mm) that month. If the predicted value y is a
discrete value, then we have a classification problem. For instance, if the hypothesis h should
predict if there was thunder or not, it would be a discrete value of either 1 or 0, and therefore
the value y âˆˆ{0,1}.

17

Olav Lillerovde

38848

Month

Rainfall (mm)

Umbrellas Sold

Thunder

January

112

15

Yes

February

100

13

No

March

150

20

Yes

â€¦

â€¦

â€¦

â€¦

Table 2: Example dataset with features month, rainfall and umbrellas sold

The table above is an example of features and values. The features can be considered as
categories or a measurable property. In this case, the features are month, rainfall, thunder, and
umbrellas sold. These features contain several measured variables that are located below the
features. The measured variables are known as labels. As an example, January is a value in
â€œmonthâ€, 112 is a value in â€œrainfall (mm)â€, 15 is a value in â€œumbrellas soldâ€ and Yes is a value
in thunder (since there was a reported thunder in the timeframe). All rows below the features
are entries in the dataset and represent an instance of recorded data.
4.2.1 Linear regression
Linear regression (LR) is a statistical approach to model the relationship between the features
y and their associated labels. The approach produces a line or (hyper)plane that best
approximates the label, given the features. The function provided by this approach will, given
an input x, produce an output y. The point provided will be an estimation of what the value of
y would be, given x.

18

Olav Lillerovde

38848

Figure 2: Linear regression graph example. Datapoints as blue dots and the fitted line in red

Figure 1 details the relationship between x and y with a fitted line (red) to a series of datapoints
(blue). Visually, the linear function h is the red line and the datapoints are the blue dots. The
line shows the relationship between all datapoints and forms a line detailing the growth. Tracing
the line upwards, it is apparent that the line has a positive growth and the interception point (the
value of y when x=0) is five. Picking a random point along the horizontal axis gives a match
on the red line, i.e. the predicted y for the given x.
Given x âˆˆ X and y âˆˆ Y, an approximated linear function for h would for one column be:
â„Î¸ (ğ‘¥) = ğœƒ0 + ğœƒ1 âˆ— ğ‘¥1 + ğœƒ2 âˆ— ğ‘¥2 + â€¦ + ğœƒğ‘› âˆ— ğ‘¥ğ‘›
The Greek symbol Î¸ (theta) models a hidden variable that will be determined by training the
model, and thus the hypothesis h will be learned through training. The subscript number from
0 up to n marks the index of a given label. The variable xi then refers to the iâ€™th label in X. The
x0 in the formula above is set to be 1 in order to simplify notation. Therefore, by simplifying
the math and including the column number, the expression can be written as:
ğ‘š

ğ‘›

â„ğœƒ (x) = âˆ‘ âˆ‘ Î¸(ğ‘—) ğ‘– âˆ— x (ğ‘—) ğ‘– = ğœƒ ğ‘‡ ğ‘¥
ğ‘—=0 ğ‘–=0

19

Olav Lillerovde

38848

Another way to approach this problem is to view Î¸ and x as vectors, with n being the number
of input variables. Treating the values in Î¸ and x as two vectors, they would look something
like this:
ğ‘¥ = {ğ‘¥0

ğ‘¥1 â‹¯

ğ‘¥ğ‘› } ğ‘ğ‘›ğ‘‘ ğœƒ = {ğœƒ0

ğœƒ1 â‹¯

ğœƒğ‘› }

The Î¸ with a superscript T means that the values in Î¸ are treated as a transposed vector.
Multiplying the transposed vector with the vector x yields cross product Î¸Tx.
ğœƒ0
{ğœƒ1 } âˆ— {ğ‘¥0
â‹®

ğ‘¥1 â‹¯

ğ‘¥ğ‘› } = {ğ‘¥0 ğœƒ0 + ğ‘¥1 ğœƒ1 + â‹¯ + ğ‘¥ğ‘› ğœƒğ‘› }

ğœƒğ‘›
The machine learning part in this problem is tied to how the line is optimised to best fit the
datapoints. The line is fitted with a cost function often denoted as J(ğœƒ). There are different cost
functions that are better suited depending on the situation. The intuition of a cost function is to
make h(x) close to y. The cost function primarily associated with linear regression is the Least
Squares cost function. This function will measure how close hÎ¸(xi) is to the corresponding yi.
The function is defined as follows:
ğ‘š

1
ğ½(ğœƒ) = âˆ‘(â„ğœƒ (ğ‘¥ğ‘– ) âˆ’ ğ‘¦ğ‘– )2
2
ğ‘–=1

The formula is proportional to the length between the predicted value and the actual value. To
obtain positive values, the formula is squared. The cost function will give a value (cost) for the
current line depicting the relationship between the data points. The next issue to solve is how
to fit this line to the most optimised location. The smaller the value given by the cost function,
the better the fit. The method is to calculate the cost function J(Î¸), compare it to the value yi
and modify the parameters Î¸. Thereafter the whole process needs to be repeated, until some
termination criterion is met, for instance, that the optimum has been reached and J(Î¸) could not
be any smaller. There is an algorithm which, for each step, tries to find a local optimum in
which J(Î¸) will be the lowest. This algorithm is called gradient descent and is formalised in the
following way:
Î¸ğ‘— â‰” Î¸ğ‘— âˆ’ ğ›¼

ğœ•
ğ½(Î¸)
ğœ•Î¸ğ‘—

The formula shows how Î¸j is updated in an iteration. The update is the original value Î¸j
subtracted by a real number Î±, which in turn is multiplied with a partial derivative of Î¸j. The
learning rate Î± refers to the base step length taken in each iteration. The learning rate is
20

Olav Lillerovde

38848

multiplied with a partial derivative of the current location of Î¸j. The partial derivate yields a
slope which is either positive or negative, depending on the sign of the derivative. In a local
minimum the slope is 0.

Figure 3: Gradient descent example. Each circle represents a point in time and how it moves closer to the minimum. The
black slope on the right only represents the slope of the first circle. The slope would actually move until becoming a
horizontal line at the local minimum.

Plotting a function represents a cost function J(Î¸). The black slope represents the derivative of
the chosen point, which changes depending on the current location. Arrows indicate the step
length done with repeated iterations. Red graph is J(Î¸).
The function portrayed on the figure above describes the curve of function J(Î¸). The lowest
point of the graph is referred to as the local minimum in this function. The gradient descent
algorithm will stepwise choose a new position that will yield a lower cost. The slope of the
derivative can be positive, negative or 0. If the slope is positive, it moves upwards from left to
right. If the slope is negative, it moves downwards from left to right. By visually inspecting the
graph, we can determine that a negative slope only exists on the left side of the local minimum,
and a positive slope only exists on the right side of the local minimum. When Î¸j is updated, the
partial derivative of Î¸j yields a new slope. As Î¸j converges on the local minimum, the slope will
decrease, and when the local minimum is reached the slope will be entirely horizontal (it will
be 0). Once the local minimum has been reached, the value of Î¸j will determine the best fit for
the hÎ¸(x).
21

Olav Lillerovde

38848

4.2.2 Logistic regression
With linear regression, the model can make a prediction of a real number for a given problem.
The model provides a prediction y for a given input x. However, in some cases the objective of
the problem might change from predicting a real number, to predicting a classification for a
given input. This approach is referred to as a classification problem. If there are a discrete
number of elements available in the label section such as a binary distribution {0,1}, or a
discrete number of elements such as cat, dog and mouse, then the problem can be treated as
classification problem. The model will on a given input x predict a discrete element y.
The sigmoid function is used in the logistic regression approach, because of its values ranging
between 0 and 1. The function is constrained horizontally, as x tends towards infinity. The
sigmoid function is formalised as follows:
ğ‘”(ğ‘§) =

1
1 + ğ‘’ âˆ’ğ‘§

Equation 1 Sigmoid function

The function is plotted in the following way:

Figure 4: Sigmoid function graph. Visual representation of the sigmoid function asymptotically placing it between 0 and 1.

The function g(z) tends towards 1 when z is approaching infinity, and towards 0 when z is
approaching negative infinity. Therefore, the function is always bound between 0 and 1. There
are similar functions, but the sigmoid function has certain neat qualities and remains the most
popular.

22

Olav Lillerovde

38848

The sigmoid function is fitted into a hypothesis hÎ¸(x) in the following way:
â„Î¸ (ğ‘¥) = ğ‘”(Î¸ğ‘‡ ğ‘¥) =

1
1 + ğ‘’ âˆ’Î¸

ğ‘‡ğ‘¥

In this formula x is a vector and Î¸ is a transposed vector. Together they form a cross product.
This cross product Î¸Tx is another way of writing hÎ¸(x). Some probabilities can be drawn from
the function.
ğ‘ƒ(ğ‘¦ = 1|ğ‘¥; ğœƒ = â„ğœƒ (ğ‘¥) ğ‘ğ‘›ğ‘‘ ğ‘ƒ(ğ‘¦ = 0|ğ‘¥; ğœƒ) = 1 âˆ’ â„ğœƒ (ğ‘¥)
These functions provide the probability of y = 1, when given x with the parameter of ğœƒ to be
equal to â„Î¸ (ğ‘¥). From this equation we can also deduce that the probability of y = 0 with the
same parameters is 1-â„Î¸ (ğ‘¥). Merging these two equations, it forms the following function:
1âˆ’ğ‘¦

ğ‘ƒ(ğ‘¦|ğ‘¥; ğœƒ) = (â„ğœƒ (ğ‘¥))ğ‘¦ (1 âˆ’ â„ğœƒ (ğ‘¥))

Depending on the y, this function will provide different results. When y tends towards 0, the
first expression will become infinitely small or become 0, which would make the expression
equal to 1. In this case, the remaining expression would multiply to the power of 1â€“y, where y
was equal to 0. Therefore, depending on the direction of y, the expression goes in either
direction. This solves the problem of choosing a function that fits for both a value that is 1 or
0.
Since the values vary between being either 0 or 1, the cost function should have a high cost
when the value is wrong. In order to penalise this properly, a function provides a low cost on
the correct value, and an infinitely high cost to the wrong value.
ğ‘–ğ‘“ ğ‘¦ = 1 â†’ ğ‘ğ‘œğ‘ ğ‘¡(â„ğœƒ (ğ‘¥), ğ‘¦) = âˆ’ ğ‘™ğ‘œğ‘”(â„ğœƒ (ğ‘¥))
The above equation shows an example where y is equal to 1. The cost function when given the
input x into the hypothesis h, while predicting y, is equal to the negative logarithm of hypothesis
h given an input x.

23

Olav Lillerovde

38848

4.3 Artificial Neural Networks

Artificial Neural Networks are often referred to as simply neural networks. A neural network is
a computational model, inspired by the biological neural network found in the human brain. A
neural network consists of artificial neurons, which are connected to each other through
synaptic weights. The artificial neurons include three sets of rules: multiplication, summary and
activation.

The input for neural networks is typically a set of numbers. These numbers are combined to
form a vector. Each number in the vector will receive its own input node. The inputs can be
binary representations or real numbers, depending on what they represent. If the data contain
strings, such as animal type = {â€œDogâ€, â€œCatâ€}, then that data can be transformed into either a
numerical representation ( 0=â€Dogâ€ and 1=â€Catâ€) or the entire variable can also be changed to
a binary one named isCat, which should serve the same function. Another, more advanced
example is that of how to convert input of a video game into a vectorised representation of the
screen. In a video game, we consider the screen to be the feedback of what is happening in the
environment of the game. Therefore, we would have to input the game screen into the neural
network. This is done by treating each pixel in the game screen as a numerical RGB unit. By
adding all RGB numerical values for each pixel into one variable, we can treat it as a vector.
The Super Nintendo Entertainment System has a resolution of 256x224 which equals 57,344
pixels. All these pixels need their own input node in a neural network. However, this would
already be incredibly difficult computationally. Therefore, certain dimensionality reduction
techniques are used to group pixels together and decrease the number of input nodes needed. It
is also possible to turn the game into grayscale to reduce the number of inputs needed.
The output for neural networks can be in different formats, depending on the problem we are
trying to solve. In some cases, the number of output nodes can represent the number of
categories to choose from. For instance, if we are trying to determine if an image is of a dog or
a cat, then there would be a node for cat and dog. One of the nodes would activate depending
on the input. The output nodes usually symbolise the controller of the task. A great example to
illustrate this point is video games, in which each output node is linked to a button on the
controller. Based on some situation depicted on the screen, that information will travel through
the neural network and be outputted as the right combination of key presses on the controller.
24

Olav Lillerovde

38848

Figure 5: Artificial Neuron (22). Shows a single neuron, the input nodes shown as â€œlabelsâ€ on the left all connect into the
neuron in the middle. The input nodes are multiplied with a weight and is summarized in the neuron. The output is determined
by passing the sum through a transfer function.

As seen above, the incoming inputs that feed into the artificial neuron, are multiplied with an
individual weight. This weight represents the â€œimportanceâ€ of this pathway, or how often this
combination is useful. The middle section summarises all weighted inputs and biases. After
summarisation, the sum passes through a transfer function to determine if the neuron
â€œactivatesâ€. The transfer function is also referred to as the activation function. One example of
an activation function is the sigmoid function. If the sum of the incoming weighted inputs is
higher than a certain threshold, the neuron will activate and send a value determined by the
transfer function to the next layer of neurons. This value can be binary or a number between
zero and one.
Combining and interconnecting these artificial neurons into larger and more complex networks
is what makes up the artificial neural network. Larger interconnected networks can solve very
complex problems.

25

Olav Lillerovde

38848

Figure 6: Neural Network. Node layers are connected forward from left to right, the first set of neurons form the input layer,
the last node on the right forms the output layer, and the series of layers of nodes in the middle form the hidden layer.

Each neuron (sometimes called a node) has its own input, either from other neurons or the
environment. The first layer, called the input layer, receives information from the environment.
The last layer is referred to as the output layer because, at this point, the neural network has
processed the information given to it and has arrived at some conclusion about what to do with
the input. A network with only an input layer and an output layer is called single layer
perceptron. The layers in between the output layer and the input layer are called hidden layer.
A network with one or more hidden layers is referred to as a multi-layer perceptron (MLP).
The hidden layer consists of several groups of neurons that communicate forwards. Each neuron
is often connected to every neuron in the following layer, but partially connected and fully
disconnected networks also exist. In a fully forward connected network, the nodes will be
connected to all nodes in the following layer until the output layer. This is referred to as
feedforward network. The strength of the connections between the neurons is determined by a
transfer function and the data. This dynamic enables a learning process, in which the
connections can determine patterns in data (23).
Neural network models have weights similar to the unknown parameter ï± in linear regression.
These weights are multiplied with the inputs in the multiplication stage. Calibrating the strength
of these weights is how the neural network learns to make judgements based on the data given
to it. Depending on the problem, we will use different cost functions. For regression-based
tasks, we use sum-of-squared-errors and for logistic regression squared error or cross-entropy.
Backpropagation (59) is used to compute the gradient in the weight space in a feedforward
neural network. This method is used for updating the weights in the model. Due to the
compositional form of the model, a chain rule can be used for differentiation (24) (25). The
26

Olav Lillerovde

38848

chain rule enables us to determine the local gradient for each neuron. Backpropagation is used
to perform a forward and backward sweep of the network, going through each neuron and its
connections to determine which connections are the most fitting for the situation.

4.4 Reinforcement Learning
When we think about learning, we often associate it with interaction and exploration. When we
begin to learn a new skill, we will often seek to explore which options we can choose and how
we can make them. For instance, when learning how to draw, we hold the pen differently and
try out different ways of drawing. Depending on the result, we might change certain methods
or adopt new approaches that allow for a better result. This process creates a feedback loop
from the cause (drawing) to the effect (result of what we drew). By changing how we draw in
order to improve the skill, we are essentially maximising our development based on our
previous actions. In this chapter, we will formalise a computational approach to learning by
interacting and exploring. This approach is referred to as reinforcement learning.

Figure 7: Visual representation of reinforcement learning and the different parts making up the process (60)

The learner (the agent) is not given instructions on which actions to take. Instead, the agent has
to interact with the environment and discover which actions yield the highest reward. In some
cases, the connection between the action and the reward can be difficult to determine. In this
case, there could be combinations of actions over a longer period of time that in the end give a
high reward. This problem is referred to as delayed reward. For instance, imagine that the agent
is trying to learn the best way to travel across the country. If the agent starts to walk, it will
27

Olav Lillerovde

38848

immediately come closer for each step it takes. It will take a long time to complete the journey,
but it will also receive a steady input, showing it is approaching the goal. If the agent instead
booked a flight ticket, it would have to travel to the airport and wait for the flight to depart In
the end, flying across the country would be the quickest and the best option and the agent would
be given its reward upon landing.
The agent is required to sense the environment it is acting in. The environment has a state in
any given moment. The state contains a great deal of information about the current condition
of the environment. The agent performs actions that have the ability to change the conditions
of the environment. The goal in this scenario is a set of conditions that we are trying to reach
in a given state. When comparing the previous state to the current state, we can look at the
action we performed in the previous state and see if the conditions improved in favour of the
goal. When the agent discovers an action that reliably gives it a reward, it will begin to use this
action more frequently because it is â€œrewardingâ€. However, it must also explore the
environment further in order to discover other potentially â€œrewardingâ€ actions. Some actions
might only be applicable in certain instances. This means that with certain conditions in the
environment some actions might be more rewarding (26).
The policy of the model is the mapping from the perceived states of the environment to the
available actions to the model. The policy defines the set of rules that govern the behaviour of
the model. Training the model includes determining what action to perform for a given
combination of input. As an example, we can imagine our input is all the pixels on the screen
displaying an old videogame and the output section is each button on the controller. The pixels
change depending on the state of the game. The intuition is that each time the pixels move, the
conditions of the game will change. Therefore, there will be at least one action that is the most
optimal action to take.
The reward signal defines the goal of a reinforcement learning problem. At each time step, a
set of conditions are checked and depending on the number of conditions that are met (or not
met), the model receives a number, which is called the reward for a given time step. The
objective of the model is to maximise this number. The reward is one of the most important
factors that determine the shape and development of the policy. If the model performs an action
that receives a high reward, the pathway will be enhanced. This makes the model more likely
to perform this action again.

28

Olav Lillerovde

38848

4.4.1 NeuroEvolution of Augmenting Topologies (NEAT)
The NEAT algorithm is an evolutionary algorithm, which uses artificial neural networks
coupled with genetic algorithms to solve problems. The algorithm achieves this by letting the
models evolve into a version of themselves that can solve the problem. Evolutionary algorithms
are inspired by biological evolution and use mechanisms such as reproduction, mutation,
recombination, selection and extinction to evolve models that can solve a given problem (27,
28, 58).
Genetic algorithms create a population of solutions for a problem. The individuals of the
population have different names but will be referred to as genomes throughout this thesis.
Traditionally genomes are treated as binary strings of 0s and 1s. When the genomes are created
the binary strings are randomised. The randomisation creates â€œbiodiversityâ€ in the population,
and similar to DNA strings, some regions with a specific combination of 1s and 0s will have
some associated phenotype. In the case of using neural networks for computation, this translates
to a genome being a â€œuniqueâ€ configuration of pathways between the input and output layer.
There are different versions of genetic algorithms, that include their own version of the
selection, fitness, crossover and mutation steps. Genetic algorithms use a fitness function to
measure the performance of a genome. The fitness function is similar to a reward function in
reinforcement learning, but they are implemented differently. A fitness function will determine
the fitness of each genome. The fitness level is proportional to a genomes chance to be used in
the next repopulation stage. Like natural selection, the genomes with a higher fitness score will
be more likely to survive and reproduce for the next generation. The repopulation happens by
selecting a pair of parent genomes, selection chance is proportional to the fitness score. The
probability rate Pc describes the pairs chance to trigger a crossover. The crossover makes the
two genomes switch a predetermined portion of their genetic code. If the crossover does not
trigger, the pair will produce two children that are identical to the parents. There is also a
mutation step in which each locus (node in the genetic code) in the pair has a chance to mutate
with a probability rate of Pm. Mutation happens by switching a locusâ€™ number to the opposite,
in the case of binary representation this would switch a 0 into a 1 (and vice versa). These steps
make up the process that continues until the population is filled up with n genomes. (27)

29

Olav Lillerovde

38848

Figure 8: A genotype to phenotype example. NEAT algorithm (28). Table containing information about each node and its
connections. And a visual representation of the table below.

Figure 6 shows a network in development by the NEAT algorithm and its associated attributes.
The attributes are characteristics that further explain or detail the network. The first row shows
all the nodes in the network, these are also sometimes referred to as genes. This row contains
information about what function each node has in the network. Nodes 1,2, and 3 are sensor
nodes as well as input nodes, this means that these nodes receive input from the training
environment. Node number 4 is a hidden node in this network and it receives its input from
nodes 1,2. The hidden notation means that this node is a part of the hidden layer as well as that
the output from this node is not defined at beginning of training, we do not know exactly what
information and function this particular node serve in the complete network. The last node nr 5
is a hidden output node and receives input from nodes 3,4 and outputs an action to perform.
The next row details the connections between the nodes, the weight of the connection, if the
connection is enabled in the phenotype, and the innovation number. The innovation number
identify the original historical ancestor of each gene. The weight of a connection is the
relevancy of the connection. The last row depicts a visual representation of the network.
Figure 6 is a good representation of how a genome in the NEAT algorithm is structured. The
genome is a set of connections and nodes. Something that is different from a traditional neural
30

Olav Lillerovde

38848

network is that it does not have to be fully connected to the next layer of nodes. Upon mutation,
a new node can be added or a new connection between nodes can be made. In order to perform
crossover in a network we have to know which genes match between different genomes. This
is done by recording the historical origin for each gene. Even if the weights differ the genes
with the same historical origin share some structural similarity. A global innovation counter is
used to assign a new value to each new gene created by mutation. This creates a trackable
chronological order within a genomes network. If these genes are used during crossover then
they will keep the same innovation number in the â€œnewâ€ genome. Genes that do not match up
with any other genes are considered either excess or disjointed and will therefore not be used
in crossover. The more disjointed two genomes are, the more incompatible they will be. If one
genome has a high level of innovation and is paired with a genome with a low level of
innovation, then they will have a fewer compatible crossover sections (28). We can measure
this compatibility level between genomes with the following formula:
ğ›¿=

ğ‘1 ğ¸ ğ‘2 ğ·
+
+ ğ‘3 âˆ— ğ’²
ğ‘
ğ‘

The formula contains the following notation the number of excess (E) and disjoint (D) genes,
the average weight differences of matching genes W, number of genes (N) in the genome and
the coefficients c1, c2, c3 adjusting the importance of the three factors. The distance measure ï¤
enables us to create a compatibility threshold for different genomes. The compatibility between
different genomes is used to put genomes into specie specific categories. Each genome is only
member of one specie group. The NEAT algorithm uses explicit fitness sharing, in which
species must share the fitness of their niche. Niching refers to the formation of groups (species)
of genomes in a population. This also means that comparing genomes in different species might
prove to be very different. Niching is useful because it encourages population diversity which
makes the algorithm explore different strategies to achieve a higher fitness score. Fitness
sharing creates a shared fitness value that represents the overall fitness of a specie (29). Species
will then grow or shrink depending on the average adjusted fitness compared to the populations
average.
After the fitness evaluation, crossover and mutation steps are completed for all genomes, a new
generation of genomes will be ready, and the process will begin anew. This generational loop
continues for a pre-specified set of generations, or until a certain fitness threshold has been
reached. During training the NEAT model will be exploring the environment by performing
different actions, some actions will lead to more favourable states. Thus, the model will receive
31

Olav Lillerovde

38848

an increased fitness score based on its performance. The accumulated fitness score will be
assigned as the total fitness score for that genome. Genomes that solve the given problems well
will receive higher fitness scores and they will be more likely to be used in the repopulation
stage. Through the usage of crossover and mutation of networks, the best genomes will be
combined. The newer generations will be descendants of previous genomes that did well facing
the same problem, thus, there is a sense of evolution in the genomes ability to solve the problem.
As the generations progress, certain species will inevitably gain a higher adjusted fitness score.
Because of this progression the size of those species will grow over time, eventually replacing
smaller species. In the end the algorithm is meant to converge on the solution with the highest
fitness score (30).
4.4.2 Advantage Actor Critic (A2C)
At each timestep during the training process, the model has a choice on which action to perform.
The actions will have expected value returns associated with them. The model wants to select
the action that yields the highest expected value return for a given state. In a value-based model
the action value can be approximated by a neural network. Some of the methods that
approximate this value have certain issues that are reflected in that the reward only affects the
state pair that led to this decision. This slows down the learning process because the neural
network does not learn the full context of the state and the reward. We can imagine that there
might be similar states that would benefit from performing this action but instead of learning
from the context, the neural network must learn and enhance those pathways separately. There
are also policy-based methods which updates the parameters typically using gradient descent
(31).
Actor critics combines the two approaches and develops a value function as well as a learning
a policy. The actor critic dynamic is represented by the actor (the policy) and the critic (the
value function). The advantage part of the advantage action critic algorithm refers to the
advantage function being used to determine how good an action is compared to others. A2C
generate two outputs when it receives input. The first one is estimation of how high reward it
can receive going forward and the second is the recommendation of what action to take. The
A2C model will reflect upon its progress regularly while it is training during a game session
(instead of at the end of a game session) and will reflect upon its current state location and what
course of action to choose next. This reflection tunes the critic to see if the development from
the action performed in the previous state has been rewarding. The critic will then use the
knowledge it gained to critique the choices of the actor and nudging the calibration in the correct
32

Olav Lillerovde

38848

direction. However, if the actor makes a choice that is better than the estimation of the critic
the actor will tweak the critics parameters.
A2C has another variant called Asynchronous Advantage Actor Critic(A3C). Asynchronous is
referring to using multiple CPU threads on a single machine to run several workers that explore
the game environment in parallel. Running them in parallel increases the chance of diversifying
the exploration of the content and we end up with exploring different regions of the neural
network quicker. These agents explore in parallel and updates a shared model. A2C on the
hand, has only one worker running, essentially doing the same work with one worker. A2C
proved to perform better than the asynchronous implementation according to a blogpost made
by OpenAI (32). The extra noise added by the A3C did not have performance boosting qualities
and A2C proved to be more cost-efficient.
4.4.3 Proximal Policy Optimisation PPO2
Policy gradient methods computes an estimator of the policy gradient and plugs it into a
stochastic gradient ascent algorithm. This means that these methods will compute an estimator
that will be used as input into a gradient ascent algorithm, so instead of traversing to a local
minimum, it will try to ascend to a local maximum. Proximal policy optimisation was developed
by OpenAI (33). The PPO2 algorithm is mostly the same as a regular PPO, the difference being
that the PPO2 version is made for GPU and multiprocessing. It also uses a vectorized
environment, and slight modifications made by stable baselines.
The algorithm uses an advantage-based approach so that in each state, the model will evaluate
what the predicted outcome of each action will be, and upon performing that action it will
subtract the actual reward outcome.
ğ´ğ‘¡ = ğ·ğ‘–ğ‘ ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘’ğ‘‘ ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘  âˆ’ ğ‘ğ‘ğ‘ ğ‘’ğ‘™ğ‘–ğ‘›ğ‘’ ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘’
The discounted rewards refer to the weighted sum of each reward the agent received in each
timestep during the current session. The baseline estimate computes an estimate of the sum of
discounted rewards and how high it will be at the end of the session from the given state
forward.

33

Olav Lillerovde

38848

By computing this number, we can determine if our policy evaluated the action and the state
correctly. Coupled with this, we take the logarithmic probabilities from the output from the
policy network and choose an action.
ğ‘™ğ‘œğ‘”ğœ‹ğœƒ(ğ‘ğ‘¡ | â€¦ |ğ‘ ğ‘¡ )
The Î¸*Ï€ refers to the policy network. Combining the formulas, we determine the policy gradient
loss function (34):
ğ¿ğ‘ƒğº (ğœƒ) = ğ¸ğ‘¡ [ğ‘™ğ‘œğ‘” ğœ‹ğœƒ(ğ‘ğ‘¡ | â€¦ |ğ‘ ğ‘¡ )ğ´ğ‘¡ ]
For each iteration, the formula will learn to associate positive advantages with more favourable
actions, and negative disadvantages with low reward yielding actions. The output of the policy
network will, however, be noisy, and the advantage function will therefore produce noisy
advantages. This will in turn lead to the policy being overfitted onto noisy output. To counteract
this, the algorithm has trust regions. The trust regions make sure that the new policy does not
differ too much from the old policy. Therefore, the policy will be updated more safely and
within trusted regions (35).
ğ‘

ğ‘€ğ‘ğ‘¥ğ‘–ğ‘šğ‘–ğ‘ ğ‘’ âˆ‘
ğ‘›=1

Ï€Î¸ğ‘›ğ‘’ğ‘¤ (ğ‘ğ‘› | â€¦ |ğ‘ ğ‘› )
ğ´
ğœ‹ğœƒğ‘œğ‘™ğ‘‘ (ğ‘ğ‘› | â€¦ |ğ‘ ğ‘› ) ğ‘›

ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ: ğ¾ğ¿ğœ‹ğœƒğ‘œğ‘™ğ‘‘ (ğœ‹ğœƒ) â‰¤ ğ›¿
The Kullback-Leibler (KL) constraint keeps the policy from changing too much from the old
policy. This, however, poses some problems as well. It adds extra cost to the optimisation
process and therefore may lead to undesirable behaviour in the model. Proximal Policy
Optimisation seeks to combine these two methods into a single formula. The main objective
function in PPO looks like this:
ğ¿ğ¶ğ¿ğ¼ğ‘ƒ (ğœƒ) = ğ¸ğ‘¡ [(min (ğ‘Ÿğ‘¡ (ğœƒ)ğ´ğ‘¡ , ğ‘ğ‘™ğ‘–ğ‘(ğ‘Ÿğ‘¡ (ğœƒ), 1 âˆ’ ğœ–, 1 + ğœ–)ğ´ğ‘¡ ]
The E refers to expectation of the timestep t. Rt(Î¸)At is the default objective for normal policy
gradients. It seeks to find actions that perform better than the baseline estimate. The second
term is a clipped version of the normal policy gradient objective. The symbol ğœ– is a
hyperparameter, and its purpose is to remove the incentive for moving rt outside the interval [1ğœ– , 1+ ğœ– ]. We take the minimum of the clipped and unclipped objective, so the final objective
is a lower bound on the unclipped objective. The intuition is that in certain cases in which the
policy changes drastically, the policy change will be â€œclippedâ€, instead of changing the policy
34

Olav Lillerovde

38848

dramatically because of one strange episode. Clipping refers to that if the changes to the policy
is too large. then the change will have a max limit, clipping flatlines the growth at a certain
point which makes the policy stay within the trust region.

35

Olav Lillerovde

38848

5. Implementation and Results
5.1 Setting up the gym-retro platform

The Super Nintendo Entertainment System (shortened SNES) is a videogame console first
released in Japan in 1990, 1991 in North America, and 1992 in Europe. I chose to primarily use
games available on this machine due to the lower resolution and lower action-space than on
newer consoles.
Older consoles can be difficult and expensive to acquire, as these consoles are increasingly
becoming collectorâ€™s items. Due to the collector status on these consoles, the market value for
them and subsequently the games are increasing. Therefore, it is more ideal to use a digital
version of older consoles. Furthermore, it would also be difficult to use a modern programming
language with an older videogame console. I chose to use an SNES emulator for my
experiments. An emulator is hardware or software that allows the host-computer system to
behave like another computer system (sometimes called guest). An SNES emulator allows
SNES games to be played on a personal computer. In order to emulate SNES games, the SNES
core files are needed. Gym-retro uses the Snes9x (36) emulator. Emulators can be used to load
ROM files. A ROM file is a computer file which contains a copy from a read-only memory
chip. In the context of games, it is a copy of the files from the video game cartridge. The ROM
files can be saved in different formats, but the gym-retro library only works with super famicom
(.sfc ) files for SNES games. Incidentally, this file type is commonly used in the Snes9x
emulator.
OpenAIâ€™s gym-retro platform (15) comes with an integration tool and the gym-retro library.
The integration tool can be used to load game files and allow the user to inspect the game state
via the virtual memory addresses. These addresses contain all the variables that are used in the
game. A frame in a videogame refers to a moment in time and the associated image on the
screen, the frequency of frames, is called frame rate and it refers to the movement between these
images creating the illusion of a moving â€œuniverseâ€. During each of these frames, the variables
in the virtual memory can change, either referring to internal variables or describing something
on the screen. However, the integration tool did not function properly on my computer.
Therefore, I used a multi-system emulator known as Bizhawk (37) instead, which offers similar
functionality. Alhough Bizhawk runs on a different core, the virtual addresses remain the same.
36

Olav Lillerovde

38848

Bizhawk has a tool called RAM search, which offers different options to search for values
during the emulation of the game. The tool displays a list of all the virtual memory addresses
in use by the game. A radio button allows for switching between â€œvalue equalâ€ or â€œvalue not
equal toâ€. When searching, the tool will compare the last compared state to the current state and
remove the addresses that did not satisfy the condition. I used this approach to systematically
check which addresses changed when the game condition changed, and I was able to isolate
several memory addresses that were linked to important game features such as player health,
player x-position, player y-position, score, etc. We will discuss the implementation of these
variables later in this chapter.
The gym-retro library has a built-in library of game configuration files. These files are the
necessary files required to run the game on the gym-retro platform. If a game exists on this list,
it can be imported using a built-in import function. If it is not on the list, they can be manually
added and configured. None of my games were on this list. The necessary files are a data.json,
scenario.json, metadata.json, a state file, and the actual ROM file in the appropriate format.
These files can be manually configured and put into a new directory named after the relevant
game. The data.json file defines the list of game-related variables that Python can see based on
their memory addresses in the game. The scenario file defines the reward function and done
condition using the variables defined in data.json. The metadata.json defines the default starting
state. The state file defines the entire state of a game and this file allows the program to
repeatedly load this state and train from that point forward.
The data.json file contains information about game variables and memory addresses. I found
the memory addresses by using the Bizhawk RAM-search tool.

37

Olav Lillerovde

38848

Figure 9: RAM-Search in Mortal Kombat SNES, main player fighter on left and enemy fighter on the right, on the left side
contains realtime saved virtual memory addresses and their values. The right top box contains the tool and shows information
regarding virtual addresses and their values.

The picture above shows the RAM-watch on the right and the values displayed on screen. The
RAM watch is another tool in Bizhawk that enables virtual memory address observation. The
virtual addresses in Bizhawk are written as hexadecimals; however, gym-retro prefers them in
decimals. I converted all values I found into decimals and added the Snes9x rambase value.
Then I could add the values as type â€œ<u2â€ (little endian two-byte unsigned value) into the
data.json file.

38

Olav Lillerovde

38848

Figure 10: Values from data.json Mortal Kombat. Data file contains the following information: given Variable name, the
virtual memory address, and the type of address

The variables defined in the data.json file refer to the address and that they should be read as
the given type. These variables can be used in the scenario file to create conditions and reward
functions.

39

Olav Lillerovde

38848

Figure 11: Rewards and Penalties used for training in Mortal Kombat

The scenario file consists of two main components, the done condition and the rewards. Both
components use the variables defined in the data.json file. The done condition is a conditional
statement that will reset the platform to the default state. The intuition is that when this condition
is met the game is over or the AI should stop investigating new possibilities. The progression
between the start state and the done condition is the cycle of a playthrough. The rewards are
conditional statements that check game variables and provide rewards or penalties based on the
desired AIâ€™s behaviour. For instance, in the fighting game Mortal Kombat, the AI should stay
alive while simultaneously defeating the opponent. Therefore, if the player (AI) wins the round,
it will receive a reward. If the player loses a round, it will receive a penalty. We will discuss
the choice of rewards, penalties and done conditions more in depth later.
The metadata.json file is very straightforward and remains the same for most files across the
entire gym-retro game library. It tells gym-retro where to find the default state file.
The state file defines the entire state of the game and enables loading from a specific point with
specific settings. A state file can be created by loading the game, then navigating to the relevant
part with the appropriate settings and then using the save state function of the emulator.
Bizhawk save state option did not work for me, due to it using a different core. I therefore used
40

Olav Lillerovde

38848

Retroarch (38), a frontend for emulators, game engines and media players. Retroarch enables
the user to load a specific core and then the appropriate ROM file. (39) (40).

5.2 Super Castlevania 4

Super Castlevania 4 (SC4) is a platform game developed and published by Konami and was
first released on the SNES in Japan in 1991. A platform game is a subgenre of action games.
Action games focus on physical challenges for the player. The player must use hand-eye
coordination and reaction time to solve a challenge. A platform game focuses on the player
having to jump or climb between suspended platforms and avoiding obstacles. Therefore, there
is usually a puzzle in terms of how to navigate the level effectively, while avoiding enemies.
SC4 has the added complexity of including patrolling enemies and having to perform specific
movements to progress in the level.
In SC4, the player controls the character â€œSimonâ€ and guides him through progressively more
difficult levels. The player begins with a certain amount of health and the health decreases every
time Simon takes damage. There are enemies in the levels who will attack Simon upon sight.
Taking damage via attacks or touching an enemy will decrease Simonâ€™s health. Health can be
restored via items that are hidden in each level. If Simonâ€™s health fully depletes, he will be
knocked out and lose a life. Upon losing all lives the game is over and the player has lost. The
game is won by completing 11 unique levels. The player can always see a section of the level,
and as Simon moves forward, the camera will follow Simon and reveal more of the level. This
phenomenon is referred to as side-scrolling. This means that there is perfect information
concerning the current section of the level. There are, however, hidden items behind destructible
obstacles and secrets hidden in plain sight. Simon has a chain weapon that he can attack enemies
with. Destroying enemies will reward the player with points to a total score. Sometimes the
enemies will also drop a powerup that can be picked up and will enhance the abilities of Simon.
Simonâ€™s chain weapon can also be upgraded twice. Upon upgrading the chain, it will provide
higher damage and higher range. Simon can attack with this chain in eight different directions,
which is as many directions as the SNES controller can manage (41) (42).

41

Olav Lillerovde

38848

Figure 12: Super Castlevania 4, Simon attacking an enemy with his chain, simon on left, enemy skeleton on right

The picture above is a screen capture from an emulated version of the game on my computer.
Simon is attacking with his chain weapon towards an enemy skeleton. Information regarding
the state of the game and the playersâ€™ progress can be seen at the top of the picture.
After properly setting up the training environment, I began working on determining the training
variables. There are two scenarios which cause Simon to â€œdieâ€: either his health depletes, or the
timer reaches zero. However, upon investigation I realised that when the timer reaches zero,
Simonâ€™s health will also deplete, causing his health counter to instantly change to zero. I was
trying to cut down the training time as much as possible and, therefore I defined the â€œdoneâ€
variable to be when Simonâ€™s health reaches zero. There is also a scenario that I encountered
during training, where the AI does not move beyond the starting location. This results in
considerable training time being allocated low rewarding training experiences. During these
low rewarding training exercises the AI does not encounter any enemies and the training session
is therefore more likely to end because of a timeout. A timeout occurs after a set amount of time
is spent on a level without completing it. Therefore, this scenario requires more training time
and yields less value to the training. I tried to solve this problem by implementing rewards that
incentivises movement and exploration in the level.
The goal of the game is to defeat the final boss in the last level. To reach the last level, the
player needs to complete the other unique levels. Therefore, one of my goals was to determine
how to translate completing a level into a reward function. This is the long-term goal of the
training process. However, I realised early that clearing specific sections of one level would be
a more realistic challenge to overcome. My intuition was to incentivise the AI to maximise the
score, move forward, and stay healthy, all while exploring the level. I developed three reward
42

Olav Lillerovde

38848

functions based on this: 1) whenever the score changes, 2) Simonâ€™s X-position is increased and
3) whenever Simonâ€™s weapon level would increase. Whenever the score changes, the AI is
rewarded, receiving a higher score indicates that more of a levelâ€™s content has been explored.
When a level is completed, the remaining time will be multiplied with a certain amount. The
sum is then added to the playerâ€™s total score. Defeating enemies also provides an increase in
total score. The second reward function whenever Simonâ€™s x-position changed, provides the AI
with a small incentive to move forward, instead of standing still. The final reward function
provides a reward if Simonâ€™s weapon level increases. My intuition was that this would make
the AI explore the level more, since the weapon upgrades are usually found behind destructible
environment. One penalty function was also added: whenever Simonâ€™s health decreased a
negative reward would be given. This would incentivise the AI to stay safe, avoid danger, or
attack the enemy from afar.
I trained three different models on the same level, with the same or very similar reward
functions. The reward functions are the same, but I adjusted the scores provided during the
training in the NEAT algorithm. The training sessions lasted around 10-12 hours and included
a million timesteps for the A2C algorithm and the PPO2 algorithm, and 30 generations with a
population of 100 genomes. The A2C and PPO2 algorithms were set to record reward, length,
and time into a CSV document every 1000 timesteps, but unfortunately it did not record as often
as specified. The NEAT algorithm has a built-in function, which collects data and writes a
detailed description of the training process once a certain threshold has been met. Usually, this
will be when a certain fitness has been reached. Unfortunately, this threshold was never met.
Instead, I recorded the summary between each generation. This summary provides information
about the fitness of a species, adjusted fitness, size of the species, the ID, age and stagnation. I
saved all findings in separate documents and transformed the data into a more usable format.
The data was then analysed using jupyter notebook (43) and a Python library called plotly (44).

43

Olav Lillerovde

38848

Figure 13: Rewards over entry number, A2C 1 million timesteps Super Castlevania 4

The picture above depicts the recorded development of the rewards over every recorded game
played by A2C. The Y axis represents the reward achieved in that session, and the X axis
represents the entry number. I set it to record every thousand timeframes, but only around 50
were recorded. The numbers appear quite high, but they are also a product of me testing with
different reward values. The graph remains stable around the 3800 mark, but towards the end a
spike in reward is suddenly seen before reverting to the previous baseline. Upon running the
trained model and rendering the results to inspect what the AI was doing, I discovered that the
AI had learned some notable behaviours. The AI learned that slinging the chain weapon in
random directions at all times increases the chances of it hitting an enemy or destructible
environment. The AI did not appear to do these moves consequently and with direction, though.
If an item appeared, the AI would not immediately pick it up, but rather jump back and forth
until it picked it up or the item disappeared. The items disappear after a certain amount of time,
if they are not picked up. There is a spike in reward around 45-47, during which the AI
performed extremely well according to the graph.

44

Olav Lillerovde

38848

Figure 14: AI stuck under staircase Super Castlevania 4. Main character on the left with whip, and skeleton enemy on the left

When I run the model and render the game visually simultaneously, I make the program print
out the total reward gained when the done condition is met. The picture above yielded similar
results to the spike in reward seen earlier in figure 7. In this iteration, the AI slings the chain
around until falling beneath the stairs and progressing forward until reaching a wall. The room
can only be escaped by going left and jumping by the stairs, then going right again. The stairs
can be traversed by walking normally across it. Jumping on the stairs makes Simon fall through.
There is a destructible object beneath the stairs that provides a powerup, either health or weapon
level upgrade. Due to the random attacks, the AI obtains this powerup and is then stuck by the
wall trying to jump through it. The AI continues this process until the timer runs out and
Simonâ€™s health is depleted.

45

Olav Lillerovde

38848

Figure 15: Rewards over entry number, PPO2 1million timesteps Castlevania 4

Similarly, to the A2C algorithm, I trained another model using the PPO2 algorithm for 1 million
timesteps. The above graph details the development of the model. This model recorded about
60 entries with a high variance, peaking at 11 000. This spike outperformed the A2Cs highest
point by about 1000 points. Judging by the data, it seems that the PPO2 takes more risks than
the A2C algorithm during training. These risks translate into very low scores, but also scores
that are slightly higher than the baseline established by the A2C. I ran the program for a visual
reference to see what the AI was doing, but it appeared to be performing worse than the A2C
algorithm. The model of the PPO2 model would spin rapidly in circles until the timer went out,
not having progressed much further than the initial start position. Judging from the graph, the
AI managed to progress to similar lengths as the A2C model. Some notable behaviour changes
are that the PPO2 model seems more concerned with moving left and right, and rarely if ever
jumps or attacks.

46

Olav Lillerovde

38848

Figure 16: Comparison Rewards over entry number, A2C and PPO2 1million timesteps Super Castlevania 4

When plotting the data together it reveals that the A2C algorithm performs consistently better
than the PPO2. Visually the highs points of the PPO2 seem to be higher than A2C, but on
average A2C performs better.
Table 3: Comparison over some general statistics from the datasets for A2C and PPO2 from training in Super Castlevania 4

A2C

PPO2

Mean

3856

3789

Std

1414

1842

Min

2481

690

Max

10644

11676

Recorded Entries

51

62

The table above confirms the assumptions about the graphs in figure 10. On average the A2C
algorithm perform slightly better than PPO2. There is a higher variance in PPO2, depicted by
the higher standard deviation. The minimum value from the PPO2 recordings is also
considerably smaller than the one from A2C. The max is however at similar ranges.
Before we discuss the NEAT algorithm and the results obtained, we will discuss some â€œerrorsâ€
that were present in the training of the mentioned algorithms. Reinforcement learning will try
47

Olav Lillerovde

38848

to maximise the reward function in any way it can and given enough iterations, the model will
find the best strategy that maximises rewards. Although the algorithms used the same reward
functions, they performed very differently in the game. The PPO2 algorithm discovered a way
to exploit the reward function concerning the x-position. The reward function reacts to
whenever the x-position is increased. Although the purpose of the function was to incentivise
the AI to go right, it also rewarded the AI for going back and forth. By continuously going left
and right until the timer reached zero, the AI maximised this reward. In comparison, the A2C
discovered that attacking in random directions would sometimes provide it a more diverse set
of rewards. However, I speculate that the AI getting stuck beneath the stairs might be the result
of the reward function incentivising it to go right when it should have gone left. We will discuss
these principles further in the end of the chapter. Due to these implications, and the NEAT
implementation being a bit different, I made a few changes to the reward functions when
translating them into fitness functions.
The reward function concerning the x-position was changed in the NEAT setup, to instead
check if the x-position was higher than the previous x-position. Rewarding the AI for
progressing beyond the previous maximum x-position achieved. Furthermore, another
condition was added to the done condition. I added a counter that starts along with the game,
and upon receiving a change in fitness score, the counter will reset. If no change in fitness
occurs, the counter will reach, in this case, 250 and the done condition will be triggered.

48

Olav Lillerovde

38848

Figure 17: NEAT algorithm training 30 generations Super Castlevania 4

The fitness scores are different and appear lower than the A2C and PPO2, because of their
scales being different. The NEAT algorithm performed overall better than both A2C and PPO2.
Unlike the previous approaches, a clear growth in fitness can be observed through the 30
generations. I set the stagnation limit to 10 during the training of this model, which means that
the species had no increase in fitness for 10 generations. The species plateaued around 125 and
began stagnating. Upon inspecting the AIâ€™s performance visually, I noticed that the AI moves
Simon with determination in a single direction and ascended the stairs with no problem, but it
did not jump or attack needlessly. The AI also discovered something referred to as â€œinvincibility
framesâ€, during which the player character takes damage and the hitbox for the player is
removed for several frames, thus allowing the player character to move through enemies
unharmed. The AI walks into the first enemy and then proceeds to walk past him during the
invincibility frames. It is unclear whether the AI learned about these frames or simply chose to
walk forward using a brute force approach. One more interesting observation was the AI
reactions on the next small section, in which there is an enemy on a suspended platform walking
back and forth. Instead of jumping on the platform and using its previous approach or attacking
the enemy, the AI chooses to crouch down under the platform. Going under the platform can
only be done with a crouched movement, which requires some patience because it is slower. It
successfully moves passed this platform while crouched and then proceeds to jump off a cliff.
49

Olav Lillerovde

38848

There is an â€œimpossible jumpâ€ section here which is meant as a misdirection. The real way
forward is to access a door close to the jump. In order to enter this door, the player needs to
stand still and press the â€œUPâ€ key. The AI did not figure out how to solve this problem during
training.

Figure 18: Adjusted Fitness NEAT algorithm Super Castlevania 4

The picture above depicts the adjusted fitness for the species during each generation. This graph
reveals that there is growth until around 15-20 generations, at which point the AI could not
move beyond the section and its progress had stagnated. The stagnation in turn makes the
species go extinct and the progress is â€œlostâ€. While the initial first 15 generations have a steady
growth rate, the other half of the graph indicates a decrease in fitness.
Each machine learning approach produced an AI, which developed a unique in-game behaviour.
The NEAT algorithm produced the best performing AI for SC4 and developed a determined
forward moving AI. The NEAT algorithm AI also had a clear growth curve during training.
The A2C algorithm produced the AI with the most offensive playstyle, swinging its chain
everywhere. The training data also reveals that it is very consistent in its performance. The
PPO2 algorithm produced the AI with the most unorthodox playstyle, with which it quickly
50

Olav Lillerovde

38848

started to exploit reward functions to perform on par with the A2C agent. However, the training
data reveals that the PPO2 is much more inconsistent during training and was much more likely
to stand in the same place doing nothing until the timer depleted.
The training of these models was done with limited resources (my own personal computer) and,
because of this, the models were not as developed as they could have been. The obstacles that
my models faced and were unable to overcome could and would likely change if given more
time to train. The next hypothetical steps in this process would be to increase training
capabilities by upgrading the hardware or by using a cloud-based solution to remotely train the
models. The next step would be to tweak the reward functions and experiment with new ones.
Next, will be to increase the number of timesteps to 10 million and witness the change in
performance. The NEAT algorithms configuration files could be adjusted to increase the
stagnation limit. This would allow the struggling species more opportunities to progress and
also allow more species to survive longer, which would halt the extinction process, as seen in
figure 12.

5.3 Mortal Kombat
Mortal Kombat (MK) is an arcade fighting game developed by Midway Games and released in
1992. Midway Games filed for bankruptcy in 2009, which led to most of the assets of the
company being purchased by Warner Brothers (45). The game was released on several different
game consoles, one of which was the SNES. Fighting games are a subgenre of the action games
genre. Fighting games are two-player games, in which a player fights another player or a
computer. Each player controls a character and each character usually has some advantages and
drawbacks associated with them. The goal of the game is defeating the other â€œplayerâ€ by
depleting his characterâ€™s health to zero. Characters lose health by taking damage from attacks.
These attacks are high kick, low kick, high punch, and low punch. The characters are also able
to move in each direction depicted on the SNES controller. The â€œUPâ€ and diagonal â€œLEFTâ€
and â€œRIGHTâ€ will invoke a jump. The characters are also able to block by pressing the â€œRâ€
(right top) button. These moves can be combined into combo moves if they performed in a
certain order. Combo moves offer situationally better and more damaging moves. Playing the
game well requires a good balance between defensive moves and offensive moves. The player
is also required to have a good reaction time and knowing how to respond to certain offensive
moves.
The single-player version of the game is played over 12 different levels. During each level, the
player is faced with an opponent and is required to win a best-of-three rounds to progress to the
51

Olav Lillerovde

38848

next. If the player loses, a â€œcontinueâ€ section will appear and the player can press the start
button to try the same level again. There are seven different characters available for the player
to play as. The fighter called â€œSub-Zeroâ€ was chosen due to personal preference, but also
because he is an iconic mascot of the game.

Figure 19:snapshot from the game Mortal Kombat, AI playing as the fighter on the left, Sub-Zero

The picture above is taken from the beginning of the game. Sub-zero, controlled by the AI, is
fighting the computer-controlled character â€œLiu Kangâ€. Both characters begin the game with
the same amount of health and are positioned on either the right or left side of the screen. A
timer will start, which limits the round to 60 seconds. If the timer reaches zero, the character
with the most amount of health left will be declared the winner of the round. In case the amount
of health is equal or both charactersâ€™ health reaches zero at the same time, the round will end
with a draw, which means no points are awarded.
In comparison to the other game, the reward functions and done conditions were easy to define.
The game is done if the player loses twice, which is defined by enemy round win equals 2. I
defined three different reward functions: 1) whenever the player wins a round, 2) whenever the
enemyâ€™s health decreases and 3) when the opponents health is decreased, and the AIâ€™s health is
greater than the opponentâ€™s. The first reward function incentivises the AI to win as many rounds
52

Olav Lillerovde

38848

as possible which is the real goal of the game. It is difficult to provide accurate feedback to the
AI on exactly why it wins the round, but upon enough iterations, it should learn some strategy
to optimise a round winning strategy. The second reward function is to incentivise making
attacks that connect with the opponent and to develop an aggressive playstyle. The third reward
function is meant to incentivise keeping an advantage throughout the fight. There are also three
penalty functions that are essentially the opposite of the reward functions. The penalty functions
are as follows: 1) Whenever a round is lost, 2) Whenever the playerâ€™s health is decreased and
3) whenever the playerâ€™s health is decreased, and the opponentâ€™s health is greater. The first and
second penalty function is meant to incentivise the AI to take as little damage as possible. If the
player takes too much damage it will lead to a defeat. The last penalty function is similar to the
third reward function, it incentivises keeping an advantage but in this case, it is meant to punish
the AI if it yields the advantage to the opponent.
The same three methods were chosen for the training, A2C, PPO2, and NEAT. The A2C and
PPO2 were trained for a million timesteps and the NEAT algorithm ran for 10 generations with
about 20 genomes with a stagnation limit of 20. The training process lasted about 10-12 hours
for each model.

Figure 20:Reward over entry number, A2C 1Million timesteps Mortal Kombat

53

Olav Lillerovde

38848

The previous figure is the data recorded during the training of the A2C model. The Y axis is
the reward achieved during the corresponding session and the X axis is the entry number of the
recorded session. In total 310 entries were recorded over the course of 1 million timesteps. It is
quite consistent in its training and two outlying high points can be seen. Upon rendering the
game while the AI played, I could see some notable developments in playstyle. The AI used a
combination of two states. A defensive state in which the AI continuously went in and out of
the block stance. I speculate that because of the nature of how the AI operates, it cannot hold
down the â€œRâ€ key. Instead it repeatedly presses the â€œRâ€ key. This is interpreted by the game as
the character going in and out of the block stance. The second state I observed was an offensive
style, in which the AI continuously used the punch move. While there are different techniques
that negates this tactic, the punching tactic can be very effective against inexperienced players.
When one punch connects, it will initiate a chained punch combo. This move deals a lot of
damage and it can be difficult to escape it. The AI was very limited in its movement, mostly
playing defensively and waiting for the opponent to approach. When the opponent approaches,
it initiates the block stance, before going into the offensive state and successfully beating the
opponent. The AI managed to win against the first opponent regularly but would often lose on
the second. The AI does not appear to be determined in its playstyle and would often block or
punch when the opponent was not near the AIs character.

54

Olav Lillerovde

38848

Figure 21:Rewards over length A2C Mortal Kombat

This figure shows the development of the A2C model. It is very conservatively increasing its
rewards while progressing, until it reaches about 3.5K length. Here it starts to fluctuate
somewhat in its performance.

55

Olav Lillerovde

38848

Figure 22:PPO2 1million timesteps Mortal Kombat

The figure above shows the development of the PPO2 algorithms model. The numbers are in
general larger than the A2Cs number featured in figure 14. The graph also shows some very
low points with barely any rewards but overall, the mean value of PPO2â€™s rewards are higher
than the A2C. The PPO2 algorithm regularly manages to score above 300, and even above 400.
Upon visually inspecting the AIâ€™s performance, I noticed several improvements in comparison
to the A2C model. The model had all the previous states I described for A2C but used them
smarter and with â€œdeterminationâ€. It also figured out how to perform a special move called
â€œSlideâ€ in which Sub-Zero slides forward while crouched and delivers a low kick. This move
is performed by using the following moves at the same time, BACK, BLOCK, LOW PUNCH,
and LOW KICK. This move is good for closing the distance between fighters, also called a gap
closing move, and attacking at the same time. The AI used this move regularly and in specific
situations. Therefore, I believe that it developed a strategy on how to win. The AI uses the slide
attack to move closer to the opponent and once there, it initiates the same punching combo as
the A2C model did. If the strategy is unsuccessful, it enters a defensive state and performs the
slide attack again. The AI manged to defeat 3 fighters with this strategy, with a best of three
rounds against each fighter. The fourth opponent is called â€œRaidenâ€ and specialises in long

56

Olav Lillerovde

38848

range attacks and gap closing moves. This character performs the AIs strategy better than Subzero.

Figure 23: Rewards over length PPO2 Mortal Kombat

The PPO2 has significantly more fluctuation in its performance. The performance varies greatly
when length is above 5k.

57

Olav Lillerovde

38848

Figure 24:Comparison Reward over Time, A2C and PPO2 Mortal Kombat

It is evident that the PPO2 algorithm outperforms the A2C, in this problem. The A2C algorithm
develops more stable and conservatively, but the explorative and fluctuating performance of
the PPO2 manages to achieve a higher mean reward value.
Table 4: A2C PPO2 value comparison Mortal Kombat

A2C

PPO2

NEAT

Mean

113,69

172,12

26,64

Std

83,60

157,11

104,98

Min

0

0

-31

Max

515

700

460

In the table above we see the values from the training sessions of all the different approaches I
tested for the Mortal Kombat game. The only difference is that the NEAT mean is the collection
of all fitness scores across all the different generations. We will discuss the NEAT results in
depth later in this chapter. It is evident that the PPO2 algorithm performed the best overall. The
A2C and NEAT are slightly similar but with a slight edge to A2C.

58

Olav Lillerovde

38848

Figure 25: Comparison reward over length Mortal Kombat

The figure above compares the rewards over length between A2C and PPO. The spread of the
rewards achieved by the PPO2 algorithm are more diverse than the A2Cs.

59

Olav Lillerovde

38848

Figure 26: Neat Algorithm Fitness over generations Mortal Kombat

The figure above shows the development of fitness in the species during the NEAT training
sessions. I have included this graph at the second generation because none of the species
progressed any further after the second generation. The size of the species and fitness levels
remained unchanged for 11 future generations. At this point, I concluded the training. Overall,
the fitness is low compared to A2C and PPO2. Upon visually inspecting the AIs performance,
I became aware that it had not developed any strategy. The AI moved in all directions seemingly
at random and it attacked or blocked seemingly at random. One key difference between the
NEAT model and the other models was that it used the JUMP move far more frequently. The
model mostly jumped straight up rather than using a forward or backward jump, which are
considered more strategic jumps.
Each machine learning method produced different performing fighters. The PPO2s model
outperformed the other models and played with better strategy overall. I speculated in the Super
Castlevania 4 chapter that the PPO2 model seemed to try to â€œexploitâ€ the reward functions the
best it could. However, in the fighting game community the nature of using the same move
repeatedly in all situations is an exploitative move. This type of strategy is often adopted by
beginner players that press all buttons randomly to perform powerful combos by chance. New
players also tend to repeatedly use the same move.
60

Olav Lillerovde

38848

6. Conclusion and future work
When we investigate AI playing games on a competitive level against real human players, it
becomes apparent that there is a difference between humans playing games and AIs playing
games. However, it can be difficult to distinguish whether it is a human or an AI playing if we
are looking at a replay of a game. There is a passion and a dedication that is not seen in the AI.
The training process for an AI is simply numbers and processes running for several days and
tracking numbers. Because we are humans, our learning operations are limited and we cannot
train consecutively all day, humans spend multiple years dedicating themselves to a game to
excel at it. People are not programmed to play a certain game, but instead choose to dedicate
themselves and hone their skills to become as good as possible.

Therefore, there is a

philosophical question whether it is worth playing these games competitively. When an AI,
such as AlphaGo, can defeat leading champions, like Lee Sedol, after training for less than a
fraction of the real time the champions spent training, is it worth it? We can also look at it from
a different perspective: because AlphaGo was able to train asynchronously, it could effectively
play multiple games at the same time. By learning from each experience, AlphaGo was able to
accumulate training hours much more quickly than a human could and without taking any
breaks. Thus, even if AlphaGo only trained for 40 real days, it trained for thousands of years
when counting all the games it played. Similarly, if a human could play the same game for
thousands of years in one sitting, we would without a doubt expect a high level of skill.
However, the fact that humans are limited in time and energy is perhaps precisely what makes
the human achievement so great and endearing to behold. When Lee Sedol played against
AlphaGo, he commented that he would sometimes look up to visually check the mental state of
his opponent, except that the person sitting in the chair was not his real opponent, he was only
moving the pieces for AlphaGo. Sedol could not assess the mental state of his opponent, which
indicates that there might be other factors beyond just the game that impact how we play against
other humans.
However, as Kasparov said, people still want to have a human chess champion, and people still
play chess. These games are like sports, and video games have for the last 20 years had an
emerging e-sports scene. People still play games and people still want to see humans play these
games. Instead of replacing our players, AI can show us new ways to play these games. There
is also the point that these advances also directly improve our understanding of AI, and help us
61

Olav Lillerovde

38848

develop newer and more sophisticated AI that will play a big role in solving the problems of
today and the future.
The results I obtained from these experiments showed a diverse set of characteristics. The PPO2
algorithm was the approach that obtained the best result in Mortal Kombat and the trained agent
was the most successful out of all the trained agents. However, the other two algorithms also
managed to develop their own strategies. Mortal Kombat had the overall best visual result and
proved to be the game the algorithms developed the best solutions for. Mortal Kombat is a
classic 1v1 and the model has access to several offensive actions that will yield a reward. It is
therefore more likely that it is easier to put the AI on a learning trail, because the inherent point
of the game is quite simple: knock out the opponent by punching him. The game Super
Castlevania 4 proved to be a difficult game for the AI to learn given the limited performance
power. The game is a non-trivial platforming game, in which the way forward is not always
intuitive. Stairways can be put together, so the AI must climb right then left, or enter a door in
a specific location. Therefore, the AI hit certain local maximums and could not progress further.
It is, however, possible that it would have escaped these if given enough time.
The next steps in experimenting with AI and these games would be to first upgrade the hardware
used. One of the main limitations in the experiments I performed was that they were limited by
hardware. Training was slow and 10 hours only yielded about one million timesteps. The
algorithms are already optimised quite well by OpenAI and NEAT and, therefore, the logical
improvement would be to add computational power. We can improve our AI training
capabilities either by replacing the current rig with a better performing rig, or switching to a
cloud-based solution. The increased computational power enables asynchronous training and
more time efficient training. The increased time steps and time used for training will allow us
to explore the possibility if the AI eventually escapes a local maximum, or if it plateaus. Beyond
the computational power problem, we can also add new rewards and change existing ones to
try to optimise the learning process.

62

Olav Lillerovde

38848

7. Svensk Sammanfattning
JÃ¤mfÃ¶relse av prestanda genom fÃ¶rstÃ¤rkningsinlÃ¤rningsmetoder i spel

Denna texten Ã¤r en svensk sammanfattning av min pro graduavhandling som originellt skrevs
pÃ¥ engelska med titeln â€AI and Gamecomplexity â€“ Game benchmarking by using reinforcement
learningâ€. Avhandlingen behandlar hur datorer kan lÃ¤ra sig utfÃ¶ra uppgifter pÃ¥ en jÃ¤mfÃ¶rbar
eller

bÃ¤ttre

nivÃ¥

Ã¤n

maskininlÃ¤rningsmetoder

mÃ¤nniskor.
fÃ¶r

att

FÃ¶r
skapa

att

utveckla

artificiell

denna

intelligens.

programtyp
Det

finns

anvÃ¤nder
mÃ¥nga

anvÃ¤ndningsomrÃ¥den fÃ¶r artificiell intelligens, till exempel Ã¶versÃ¤ttning av sprÃ¥k, eller
kalkylering och simulering av komplexa 3D strukturer av protein. Det finns mÃ¥nga olika
metoder fÃ¶r maskininlÃ¤rning, och sÃ¤tt att justera inlÃ¤rningen likasÃ¥. Det kan dock vara svÃ¥rt att
mÃ¤ta hur effektiva inlÃ¤rningsmetoderna Ã¤r eftersom kÃ¤llor fÃ¶r feedback som hjÃ¤lper datorn
skilja bra och dÃ¥liga handlingar saknas.

Ett mycket effektivt sÃ¤tt att mÃ¤ta prestanda pÃ¥ inlÃ¤rningsmetoderna Ã¤r att trÃ¤na AI till att spela
datorspel. Datorspel innehÃ¥ller poÃ¤ngsystem eller tÃ¤vlingar som gÃ¶r det lÃ¤tt ge feedback till
datorn. Dessa feedbackkÃ¤llor gÃ¶r det lÃ¤tt att skapa fÃ¶rbÃ¤ttringsmÃ¥l fÃ¶r datorn. I mina experiment
utnyttjades tre olika inlÃ¤rningsmetoder fÃ¶r att lÃ¤ra datorn spela tvÃ¥ olika spel.
MaskininlÃ¤rningen gjordes genom att fÃ¶rst ladda upp spelen digitalt i en programmeringsmiljÃ¶.
InlÃ¤rningsmetoderna skapar en modell som utfÃ¶r handlingar i programmeringsmiljÃ¶n. Modellen
fÃ¥r dÃ¤refter feedback baserat pÃ¥ sina handlingar i miljÃ¶n. Om modellen nÃ¥r en punkt dÃ¤r spelet
inte kan fortsÃ¤tta, till exempel om spelkaraktÃ¤ren dÃ¶r, sÃ¥ mÃ¥ste spelet bÃ¶rja om frÃ¥n
startpunkten. Varje gÃ¥ng modellen fÃ¥r feedback sÃ¥ ackumulerar den positiva eller negativa
poÃ¤ng. Innan ett spel bÃ¶rjar om, sparas alla poÃ¤ng frÃ¥n sessionen. I fÃ¶ljande spel lÃ¤r modellen
sig att det finns situationer med mera optimala handlingar fÃ¶r att maximera poÃ¤ngavkastningen.
Genom att upprepa denna process lÃ¤r modellen sig optimera poÃ¤ngavkastningen.

De flesta har spelat nÃ¥gon form av spel, men Ã¤ven om vi kÃ¤nner till spel sÃ¥ kan det Ã¤ndÃ¥ vara
svÃ¥rt att definiera vad ett spel Ã¤r. Det kan vara en aktivitet eller sport med definierade regler
och en slutpunkt som konstaterar nÃ¤r spelet Ã¤r slut. Ibland kan spel ha nÃ¥gon form av
63

Olav Lillerovde

38848

poÃ¤ngsystem, som underlÃ¤ttar att avgÃ¶ra vilken deltagare som hade stÃ¶rst framgÃ¥ng. Spel kan
ocksÃ¥ finnas i andra medier som till exempel virtuella medier, brÃ¤dspel eller innanfÃ¶r ett
geografiskt omrÃ¥de. En definition pÃ¥ ett spel kan dÃ¤rfÃ¶r vara: en mÃ¤ngd begrÃ¤nsningar i en miljÃ¶
med ett poÃ¤ngsystem och ett tidigare definierat slutvillkor som markerar att spelet Ã¤r Ã¶ver.
Denna definition kan vara lite kryptisk och vag, men poÃ¤ngen Ã¤r att illustrera att de flesta
aktiviteter kan anses vara ett spel. Det kan jÃ¤mfÃ¶ras med aktiviteten att spara pengar:
begrÃ¤nsningarna kan vara att vi har en Ã¤ndlig mÃ¤ngd pengar i bÃ¶rjan av mÃ¥naden och att vi inte
kan tillfÃ¶ra nya pengar fÃ¶re slutet av spelet. MiljÃ¶n Ã¤r den verkliga vÃ¤rlden och man fÃ¥r poÃ¤ng
beroende pÃ¥ hur mycket pengar man har kvar i slutet av mÃ¥naden. Slutvillkoret i detta skede Ã¤r
efter en eller flera mÃ¥nader. I detta sparandespel kan man mÃ¤ta hur bra spararen gjorde under
ett visst antal mÃ¥nader och om hen klarade av att spara mera Ã¶ver tid. Detta kallas spelifiering,
vilket innebÃ¤r att man anvÃ¤nder spelelement inom aktiviteter och verksamheter som traditionellt
tillhÃ¶r spel. Spelifiering kan gÃ¶ra interaktionen med aktiviteten roligare och Ã¶ka engagemanget
fÃ¶r materialet. Det Ã¤r en omtalad idÃ©: Om vi kan utveckla artificiell intelligens som kan spela
komplexa datorspel, sÃ¥ skulle vi ocksÃ¥ kunna anvÃ¤nda samma tekniker till att utveckla artificiell
intelligens som kan interagera med komplexa spelifierade aktiviteter. Det kan vara svÃ¥rt att
bedÃ¶ma prestanda hos artificiell intelligens men i spel kan vi lÃ¤tt se hur bra maskinen gÃ¶r det
genom att iaktta poÃ¤ngtavlan.

Ã…r 1997 blev den dÃ¥varande schackstormÃ¤staren Garry Kasparov besegrad av schackmaskinen
Deep Blue, som utvecklades av IBM. HÃ¤ndelsen fick mycket uppmÃ¤rksamhet i pressen och
kallades en historisk strid mellan mÃ¤nniska och maskin. Deep Blue anvÃ¤nde kraftfulla
schackchippar fÃ¶r att snabbt kalkylera och vÃ¤lja det bÃ¤sta steget fÃ¶r att fÃ¶rbÃ¤ttra chansen att
vinna. Denna utveckling ledde till att mÃ¥nga undrade om det fanns en mening fÃ¶r mÃ¤nniskor att
spela spel som schack pÃ¥ en professionell nivÃ¥, om en maskin Ã¤ndÃ¥ kan gÃ¶ra det bÃ¤ttre. Garry
Kasparov sa i en TED-talk Ã¥r 2017 att fastÃ¤n vi har maskiner som kan spela schack bÃ¤ttre Ã¤n
oss, sÃ¥ vill vi fortfarande ha en mÃ¤nniska som stormÃ¤stare (och folk vill fortfarande spela
schack). Han menade att maskinerna har objektivitet och instruktioner, men mÃ¤nniskan har glÃ¶d
och mening. Deep Blue var trots allt egentligen inte smart, utan jÃ¤ttesnabb pÃ¥ att kalkylera det
bÃ¤sta steget i en given situation. NÃ¤sta milstolpe vi diskuterar handlar om AlphaGo, som genom
fÃ¶rstÃ¤rkningsinlÃ¤rning utvecklade ett artificiellt neuralt nÃ¤tverk (ANN) som besegrade den
dÃ¥varande bÃ¤sta spelaren i vÃ¤rlden i det traditionella spelet Go. BrÃ¤dspelet Go har tusentals fler
mÃ¶jliga tillstÃ¥ndskonfigurationer Ã¤n schack, nÃ¥got som gÃ¶r det svÃ¥rare fÃ¶r en maskin att lÃ¤ra sig
64

Olav Lillerovde

38848

spelet. FÃ¶retaget Deep Mind utvecklade AI:n AlphaGo som lÃ¤rde sig spela Go. Den uppnÃ¥dde
det genom att imitera professionella spelare fÃ¶r att fÃ¥ baskunskap om hur man spelar. DÃ¤refter
trÃ¤nade AlphaGo genom att spela mot sig sjÃ¤lv. Resultatet var att den besegrade den dÃ¥varande
stormÃ¤staren i Go, Lee Sedol. Efter evenemanget bestÃ¤mde Deep Mind sig fÃ¶r fÃ¶rsÃ¶ka utveckla
en maskin som spelar Go, utan tidigare instruktioner om hur maskinen skulle spela. Maskinen
fick belÃ¶ning om den gjorde ett bra steg och straff om den gjorde ett dÃ¥ligt. Den nya maskinen
heter AlphaZero, och efter 40 dagars trÃ¤ning mot endast sig sjÃ¤lv hade den ackumulerat Ã¶ver
29 miljoner spel.

Ett artificiellt neuralt nÃ¤tverk (ANN) Ã¤r en berÃ¤kningsmodell som Ã¤r inspirerad av biologiska
neuroner som uppgÃ¶r de neurologiska fÃ¶rbindelserna i hjÃ¤rnan. Ett ANN bestÃ¥r av noder och
anslutningar mellan noderna. Dessa noder fÃ¶rekommer i flera lager och varje nod i ett lager Ã¤r
ansluten till alla noder i nÃ¤sta lager (men det Ã¤r inte ett krav). Anslutningarna mellan noderna
har dolda parametrar som ocksÃ¥ kallas vikter som bestÃ¤mmer denna anslutnings relevans. Ett
ANN tar input i form av en tillfÃ¤llig situation i miljÃ¶n, till exempel brÃ¤dspelskonfigurationen i
schack med sina pjÃ¤spositioner och Ã¶ppna rutor. Denna input gÃ¥r genom nÃ¤tverket och ger
output i form av vilket steg som ger stÃ¶rst chans fÃ¶r att vinna utifrÃ¥n den dÃ¥varande situationen
enligt maskinen. DÃ¥ maskinen har gjort ett nytt steg vill den fÃ¥ en ny input efter att
brÃ¤dspelskonfigurationen Ã¤ndrade sig nÃ¤r varje spelare gjorde ett nytt steg. Maskinen kan lÃ¤ras
maskinen att kÃ¤nna igen bra steg genom att definiera om ett steg leder till en bÃ¤ttre konfiguration
av brÃ¤det, det vill sÃ¤ga en konfiguration som ger oss en stÃ¶rre chans att vinna. En central tanke
i fÃ¶rstÃ¤rkningsinlÃ¤rning Ã¤r att vi vill ge maskinen belÃ¶ningar dÃ¥ den gÃ¶r bra steg och ge den
straff dÃ¥ den gÃ¶r dÃ¥liga steg. BelÃ¶ningarna ges i form av plus- och minuspoÃ¤ng. Dessa poÃ¤ng
formar vikterna i nÃ¤tverket. Genom en fÃ¶rstÃ¤rkningsinlÃ¤rningsalgoritm fÃ¶rsÃ¶ker vi maximera
belÃ¶ningar och pÃ¥ detta sÃ¤tt justeras nÃ¤tverket. NÃ¤tverket trÃ¤nas genom att upprepa spel mÃ¥nga
hundra tusentals gÃ¥nger och pÃ¥ detta sÃ¤tt kommer det utforska olika strategier och metoder. Det
kommer Ã¤ven att fÃ¶rbÃ¤ttras Ã¶ver tid genom att upptÃ¤cka nya strategier som Ã¤r bÃ¤ttre Ã¤n de
tidigare.

Tre typer av fÃ¶rstÃ¤rkningsinlÃ¤rningsalgoritmer diskuteras: NEAT-algoritmen, PPO2 och A2C.
Den fÃ¶rsta algoritmen, NEAT, Ã¤r en evolutionÃ¤r algoritm som anvÃ¤nder neurala nÃ¤tverk och
genetiska algoritmer till att lÃ¥ta digital evolution utveckla en lÃ¤mplig lÃ¶sning pÃ¥ problemet. En
65

Olav Lillerovde

38848

evolutionÃ¤r algoritm Ã¤r en algoritm som Ã¤r inspirerad av evolutionsteorin och fÃ¶rsÃ¶ker spegla
den genom att generera en population dÃ¤r varje individ har olika konfigurationer. DÃ¤refter
evalueras varje individ i en population i en given situation fÃ¶r att avgÃ¶ra vilken individ som
anpassar sig till en viss uppgift bÃ¤st. De bÃ¤sta individerna har en stÃ¶rre chans att fortplanta sig
inom populationen. Genom att upprepa denna process fÃ¶rsÃ¶ker vi att fÃ¥ en population med
stegvis bÃ¤ttre konfigurationer. Genetiska algoritmer Ã¤r en evolutionÃ¤r algoritm som tar
inspiration frÃ¥n genetiken och genernas genotyper och fenotyper. NÃ¤r en population blir
genererad fÃ¥r varje individ tillfÃ¤lligt en konfiguration av nÃ¤tverkets noder och anslutningar
(genotypen) som vill pÃ¥verka hur nÃ¤tverket hanterar spelmiljÃ¶n (fenotypen). NÃ¤r en individ
evalueras kontrolleras hur stor belÃ¶ning den klarar att uppnÃ¥ i en given spelmiljÃ¶. Denna process
ger varje individ ett fitnessvÃ¤rde som Ã¤r en reflektion av dess prestanda. NÃ¤sta steg Ã¤r
Ã¶verkorsning dÃ¤r tvÃ¥ gener vÃ¤ljs med stÃ¶d av fitnessvÃ¤rdet. DÃ¤refter fÃ¶rekommer ett av tre
alternativ: det fÃ¶rsta Ã¤r att ingen Ã¶verkorsning sker och att exakt samma individer blir Ã¶verfÃ¶rda
till nÃ¤sta generation. Det andra alternativet Ã¤r Ã¶verkorsning dÃ¤r en viss del av nÃ¤tverket byts ut
med en annan del. Det sista alternativet som kan ske Ã¤r en mutation i nÃ¤tverket. DÃ¥ kan det
fÃ¶rekomma en ny nod eller en ny anslutning mellan noderna som kan medfÃ¶ra en helt ny
fenotyp. Genom att upprepa denna evolutionÃ¤ra process vill vi stegvis utveckla individer som
har genotyper och fenotyper som Ã¤r bÃ¤ttre lÃ¤mpade att lÃ¶sa det presenterade problemet.
Proximal policyoptimering (PPO2) Ã¤r en fÃ¶rstÃ¤rkningsinlÃ¤rningsalgoritm som evaluerar ett
givet tillstÃ¥nd och fÃ¶rsÃ¶ker fÃ¶rutse vilken handling som Ã¤r den lÃ¤mpligaste. Efter att ha utfÃ¶rt
en handling analyserar den hur lÃ¶nsam handlingen faktiskt var. LÃ¶nsamheten beror pÃ¥ hur
korrekt evalueringen var. PÃ¥ detta sÃ¤tt kan maskinen lÃ¤ra sig vilka handlingar som Ã¤r de mest
optimala i vilka situationer. Genom att upprepa denna process tusentals gÃ¥nger sÃ¥ kan maskinen
bli duktig pÃ¥ att evaluera situationer. PPO2 implementerar ocksÃ¥ en fÃ¶rtroenderegion (eng.
Trust Region) som gÃ¶r att maskinen inte kan Ã¤ndra sig fÃ¶r drastiskt. Detta Ã¤r till fÃ¶r att motverka
tillfÃ¤llen dÃ¤r dramatiska inlÃ¤rningar kan gÃ¶ra att maskinen felkalibreras, det vill sÃ¤ga att givet
ett tillstÃ¥nd skulle evalueringen av situationen vara dramatiskt fel och dÃ¤rfÃ¶r skulle Ã¤ven
maskinen som reaktion pÃ¥ hur den evaluerar situationen dramatiskt Ã¤ndra. Om detta sker flera
gÃ¥nger sÃ¥ skulle maskinen anpassa sig i fel situationer och fÃ¶rstÃ¶ra den tidigare kalibreringen.
Med hjÃ¤lp av fÃ¶rtroenderegionen kan maskinen fÃ¶rhindras Ã¤ndra sig fÃ¶r snabbt och sÃ¥ att
kalibreringen hÃ¥lls innanfÃ¶r regionen.

66

Olav Lillerovde

38848

AktÃ¶r- och kritikerfÃ¶rdelsalgoritmen (A2C) innehÃ¥ller tvÃ¥ huvudkomponenter, en som vÃ¤ljer
vilken handling maskinen utfÃ¶r (aktÃ¶ren) och en som evaluerar handlingen (kritikern). AktÃ¶ren
fÃ¶rsÃ¶ker hitta den handling som ger stÃ¶rst framgÃ¥ng och kritikern gÃ¶r en evaluering fÃ¶re och
efter en handling och evaluerar aktÃ¶ren om den valde riktig handling. Det finns ocksÃ¥ tillfÃ¤llen
dÃ¤r kritikern ger en rekommendation pÃ¥ en handling och det visar sig att rekommendationen Ã¤r
fel. Kanske var handlingen bÃ¤ttre eller sÃ¤mre Ã¤n vad kritikern estimerade och i detta skede mÃ¥ste
kritikern Ã¤ndra pÃ¥ sina kalibreringar. Genom att upprepa denna process sÃ¥ vill bÃ¥de aktÃ¶ren och
kritikern utveckla en fÃ¶rmÃ¥ga till att med god sannolikhet kunna estimera vilken handling som
Ã¤r den bÃ¤sta och en fÃ¶rmÃ¥ga att bÃ¤ttre evaluera situationer.

Avhandlingen behandlar hur de tre ovannÃ¤mnda fÃ¶rstÃ¤rkningsinlÃ¤rningsalgoritmerna anvÃ¤nds
fÃ¶r att trÃ¤na modeller att spela tvÃ¥ olika spel. Detta gÃ¶rs genom en virtuell emulering av
programvaran till Super Nintendo Entertainment System. Sedan matas virtuella versioner av
spelen in i den emulerade konsolen. Som tur har maskininlÃ¤rningsfÃ¶retaget OpenAI utvecklat
ett mjukvarubibliotek till programmeringssprÃ¥ket Python fÃ¶r just sÃ¥dana situationer. Innan vi
kan bÃ¶rja experimentera mÃ¥ste vi definiera ett starttillstÃ¥nd fÃ¶r spelet och skapa vÃ¤rdefunktioner
fÃ¶r algoritmerna. Ett starttillstÃ¥nd definieras fÃ¶r att lÃ¤ra modellen att spela ett spel och inte
istÃ¤llet bara navigera menyn i spelet. Det Ã¤r lÃ¤mpligt att ha ett startlÃ¤ge att Ã¥tergÃ¥ till om
modellen till exempel hamnar i en loop eller karaktÃ¤ren i spelet dÃ¶r. Vi Ã¥tergÃ¥r till startlÃ¤get fÃ¶r
varje iteration i inlÃ¤rningsprocessen. VÃ¤rdefunktionerna skapas genom att definiera vilka
tillstÃ¥nd vi vill belÃ¶na i inlÃ¤rningen, till exempel om modellen fÃ¥r fler poÃ¤ng eller om modellen
nÃ¥r slutet av en nivÃ¥. Samtidigt vill vi ocksÃ¥ definiera tillstÃ¥nd som vi inte vill belÃ¶na utan
dÃ¤remot straffa, till exempel om karaktÃ¤ren i spelet tar skada eller dÃ¶r.

Det fÃ¶rsta spelet Ã¤r ett plattformactionspel som heter Super Castlevania 4 (SC4). Spelet handlar
om att spelaren mÃ¥ste klara sig genom en nivÃ¥ fylld av fiender och fysiska hinder, nyckeln Ã¤r
alltsÃ¥ att kunna balansera kamp och akrobatik. VÃ¤rdefunktionerna fÃ¶r spelet Ã¤r: varje gÃ¥ng
poÃ¤ngen Ã¶kar, varje gÃ¥ng karaktÃ¤ren kommer nÃ¤rmare slutet av nivÃ¥n, och varje gÃ¥ng
karaktÃ¤rens vapennivÃ¥ blir hÃ¶gre genom en â€power upâ€. Modellen fÃ¥r straffar varje gÃ¥ng
karaktÃ¤ren fÃ¶rlorar hÃ¤lsopoÃ¤ng. Varje modell trÃ¤nades i 10â€“12 timar och lÃ¤rde sig spela spelet
pÃ¥ olika sÃ¤tt. A2C-algoritmen lÃ¤rde sig att svÃ¤nga med vapnet pÃ¥ ett kreativt sÃ¤tt. Den lÃ¤rde sig
snabbt att svÃ¤nga vapnet i alla riktningar sÃ¥ att ifall den lyckades trÃ¤ffa nÃ¥got fÃ¶rstÃ¶rbart dÃ¶k en
67

Olav Lillerovde

38848

â€power upâ€ upp eller nÃ¥got liknande som ger poÃ¤ng. Det Ã¤r till synes slumpmÃ¤ssigt var den
kastar vapnet och Ã¤ven om det dyker upp en â€power upâ€ sÃ¥ gÃ¥r inte modellen genast fÃ¶r att
plocka upp den. Efter detta spelade modellen fast sig under en trappa och kom inte lÃ¤ngre pÃ¥
nivÃ¥n. Den fÃ¶ljande algoritm PPO2 betedde sig annorlunda, men ocksÃ¥ mycket sÃ¤mre Ã¤n A2Cmodellen. PPO2-modellen snurrade i cirklar fram och tillbaka istÃ¤llet fÃ¶r att gÃ¶ra nÃ¥got av
vÃ¤rde. Den sista algoritmen, NEAT, var mera intresserad av att gÃ¥ framÃ¥t. Den klarade sig fÃ¶rbi
trappan som A2C blev fast under, och â€lÃ¤rdeâ€ sig gÃ¥ genom en fiende. NÃ¤r man fÃ¥r en skada sÃ¥
kan man inte fÃ¥ en ny skada pÃ¥ nÃ¥gra sekunder, sÃ¥ modellen utnyttjade denna funktion genom
att gÃ¥ genom fienden, i stÃ¤llet fÃ¶r att faktiskt slÃ¥ss mot den. Efter detta undvek modellen nÃ¤sta
fiende genom att krypa under en plattform med en fiende pÃ¥ toppen. Det borde poÃ¤ngteras att
krypa under plattformen Ã¤r svÃ¥rare Ã¤n att bara dÃ¶da fienden. Efter denna del kommer det ett
omÃ¶jligt hopp dÃ¤r man i stÃ¤llet ska gÃ¥ genom en â€doldâ€ dÃ¶rr. Modellen lyckades inte lÃ¤ra sig
var denna dÃ¶rr var och fÃ¶rsÃ¶kte istÃ¤llet hoppa Ã¶ver det omÃ¶jliga hoppet.

Det andra spelet Ã¤r Mortal Kombat, ett fightingspel dÃ¤r man kontrollerar en kÃ¤mpe som slÃ¥ss
mot en annan kÃ¤mpe. Den som klarar att slÃ¥ eller sparka den andra till noll hÃ¤lsopoÃ¤ng fÃ¶rst
vinner. Spelaren mÃ¥ste vinna tvÃ¥ matcher mot en annan kÃ¤mpe fÃ¶r att gÃ¥ vidare till nÃ¤sta kÃ¤mpe
och spelet fortsÃ¤tter tills den sista kÃ¤mpen Ã¤r besegrad eller tills spelaren besegras genom att
fÃ¶rlora tvÃ¥ rundor mot samma kÃ¤mpe. VÃ¤rdefunktionerna fÃ¶r detta spel Ã¤r belÃ¶ning varje gÃ¥ng
fienden tar skada och varje gÃ¥ng en runda vinns. Straffar ges varje gÃ¥ng spelaren skadas och
om spelaren fÃ¶rlorar en runda. Varje modell trÃ¤nades mellan 10 och 12 timmar och lÃ¤rde sig
spela pÃ¥ olika sÃ¤tt. Den fÃ¶rsta algoritmen A2C utvecklade en modell som lÃ¤rde sig byta mellan
fÃ¶rsvar och offensiv strategi. Modellen bytte mellan att slÃ¥ vilt framfÃ¶r sig och att blockera allt
den kunde. Denna strategi gjorde att den i nÃ¥gra fall klarade sig till fÃ¶ljande kamp men oftast
fÃ¶rlorade den i den fÃ¶rsta. PPO2-algoritmen utvecklade en modell som gjorde samma som A2C
men lÃ¤rde sig ocksÃ¥ gÃ¶ra specialrÃ¶relser genom att trycka en viss kombination med knappar pÃ¥
kontrollen. Den lÃ¤rde sig att gÃ¶ra en glidande spark, med vilken den kunde korta ner distansen
till motstÃ¥ndaren. Den hade ocksÃ¥ andra specialrÃ¶relser som den lÃ¤rde sig, men dessa anvÃ¤ndes
inte tillrÃ¤ckligt ofta fÃ¶r att bli en del av modellens inlÃ¤rda strategi. Modellen klarade sig vÃ¤ldigt
bra med denna strategi och besegrade mestadels av tre motstÃ¥ndare och fÃ¶rlorade emot en
kÃ¤mpe som specialiserat sig pÃ¥ lÃ¥ngrÃ¤ckviddsattacker. Den sista algoritmen, NEAT, utvecklade
ingen speciell strategi, men testade i stÃ¤llet slumpmÃ¤ssigt alla knapparna. Ã–verlag var resultaten

68

Olav Lillerovde

38848

fÃ¶r NEAT sÃ¤mre Ã¤n hos A2C- och PPO2-modellerna. Det enda som var mÃ¤rkvÃ¤rdigt med
NEAT-algoritmens modell var att den anvÃ¤nde mycket mera hopp Ã¤n de andra.

Resultaten var varierade och ingen slutsats kan dras om vilken algoritm som utvecklade den
bÃ¤sta modellen fÃ¶r bÃ¥da spelen. I Mortal Kombat hade PPO2 den bÃ¤ttre modellen och i SC4
hade NEAT den bÃ¤ttre modellen. Mortal Kombat Ã¤r ett spel dÃ¤r det var lÃ¤tt att definiera enkla
vÃ¤rdefunktioner som blev tagna i bruk snabbt eftersom spelet handlar om att slÃ¥ss. Det drÃ¶jer
inte lÃ¤nge fÃ¶r en modell i trÃ¤ning att slumpmÃ¤ssigt slÃ¥ motstÃ¥ndaren, vilket resulterar i en
belÃ¶ning. Om man jÃ¤mfÃ¶r detta med SC4 sÃ¥ Ã¤r belÃ¶ningarna icke-triviala. Modellen hinner
gÃ¶ra mÃ¥nga handlingar innan den hittar en bra handling, vilket gÃ¶r att trÃ¤ningsprocessen blir
lÃ¤ngre och det Ã¤r svÃ¥rare fÃ¶r modellen att veta vad den borde fokusera pÃ¥. De fÃ¶ljande stegen i
trÃ¤ningsprocessen skulle vara att uppgradera hÃ¥rdvaran och ge mera trÃ¤ningstid fÃ¶r samtliga
modeller. Med bÃ¤ttre hÃ¥rdvara skulle dessutom trÃ¤ningen vara effektivare. Dessutom kunde
man Ã¤ndra pÃ¥ vÃ¤rdefunktionerna, startlÃ¤get och leka lite med parametrar fÃ¶r att hitta en
kombination som ger bÃ¤ttre resultat. Det Ã¤r mÃ¶jligt att modellerna skulle klara sig bÃ¤ttre om de
bara gavs mera trÃ¤ningstid.

69

Olav Lillerovde

38848

8. Sources
1. Victor, Allis. Searching for Solutions in Games and Artificial Intelligence. 1994.
2.

IBM.

IBM100

Icons

of

progress.

[Online]

[Cited:

19

Mai

2020.]

https://www.ibm.com/ibm/history/ibm100/us/en/icons/deepblue/.
3.

Press,

Gil.

Forbes.

[Online]

[Cited:

19

Mai

2020.]

https://www.forbes.com/sites/gilpress/2018/02/07/the-brute-force-of-deep-blue-and-deeplearning/#53c514eb49e3.
4. Kasparov, Garry. Don't fear intelligent machines. Work with them. [Online] TED,
2017.

[Cited:

2020

Mai

21.]

https://www.ted.com/talks/garry_kasparov_don_t_fear_intelligent_machines_work_wit
h_them/transcript.
5. Mastering the game of Go without human knowledge. Silver, David, et al. 550, 2017,
Nature, Vol. 2017, pp. 354-356.
6. Mastering the game of Go with deep neural networks and tree search. Silver, David, et al.
529, 2016, Nature, Vol. 2016, pp. 484-488.
7. Kohs, Greg. AlphaGo - The Movie. Google Deepmind, 2017.
8. BBC News. Go master quits because AI 'cannot be defeated. [Online] 27 November
2019. [Cited: 21 Mai 2020.] https://www.bbc.com/news/technology-50573071.
9. Yonhap News Agency. (Yonhap Interview) Go master Lee says he quits unable to win
over AI Go players. [Online] 27 November 2017. [Cited: 21 Mai 2020.]
https://en.yna.co.kr/view/AEN20191127004800315.
10. Blizzard Entertainment. Starcraft 2 . [Online] Blizzard Entertainment, 27 Juli 2010.
[Cited: 21 Mai 2020.] https://starcraft2.com/en-us/.
11. Battle.net. [Online] Blizzard Entertainment, 1994. [Cited: 21 Mai 2020.]
https://www.blizzard.com/en-us/?ref=battle.net.
12. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Vinyals,
Oriol, et al. 575, 2019, Nature, Vol. 2019, pp. 350-354.
13. Battle.net. StarCraft 2 Ladder Europe. [Online] [Cited: 28 Mai 2020.]
https://starcraft2.com/en-us/ladder/grandmaster/2.
70

Olav Lillerovde

38848

14. Hill, Ashley, et al. Stable baselines documentation. [Online] Github, 2018.
https://stable-baselines.readthedocs.io/en/master/.
15.

OpenAI.

Blog

Gym

Retro.

[Online]

[Cited:

17

April

2020.]

https://openai.com/blog/gym-retro/.
16. Improved protein structure prediction using potentials from deep learning. Senior,
Andrew, et al. 577, s.l. : Nature, 15 January 2020, Nature, pp. 706-710.
17. Mitchell, Thomas. Machine-Learning. s.l. : McGraw Hill Higher Education , 1997.
18. Kaggle. Kaggle. [Online] [Cited: 17 April 2020.] https://www.kaggle.com/.
19. Introduction to ROC analysis. Tom, Fawcett. 27, 2006, Pattern recognition letters, pp.
861-874.
20. A note on using the F-measure for evaluating data linkage. David, Hand and Peter,
Christen. 28, 2017, Stat Comput, pp. 539-547.
21. Ng, Andrew. CS229 Lecture Notes. 2018.
22. Suzuki, Kenji. Artificial Neural Networks. s.l. : InTech, 2011.
23. Grossi, Enzo and Buscema, Massimo. Introduction to artificial neural networks.
European Journal of Gastroenterology & Hepatology. January 2008, pp. 1046-1054.
24. Tibshirani, Robert and Hastie, Trevor. The Elements of Statistical Learning. s.l. :
Springer, 2001.
25. Slesnick, William E and Crowell, Richard H. Academia. [Online] 5 Januar 2008.
[Cited:

15

Mai

2020.]

https://www.academia.edu/40755893/Crowell_and_Slesnicks_Calculus_with_Analytic_G
eometry_The_Dartmouth_CHANCE_Project_1.
26. Sutton, Richard S and Barto, Andrew G. Reinforcement learning: an introduction. s.l. :
The MIT press, 2018.
27. Mitchell, Melanie. An Introduction to Genetic Algorithms. s.l. : MIT Press, 1999.
28. Efficient Evolution of Neural Network Topologies. Stanley, Kenneth O and
Miikkulainen, Risto. Austin, TX 78712 : The University of Texas at Austin, 2002,

71

Olav Lillerovde

38848

Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat.
No.02TH8600).
29. Every niching method has its niche: Fitness sharing and implicit sharing compared.
Darwen, Paul and Yao, Xin. 1996, Parallel Problem Solving from Nature â€” PPSN IV ,
pp. 398-407.
30. Risi, Sebastian and Togelius, Julian. Neuroevolution in Games: State of the Art and
Open Challenges. [Online] 2014. [Cited: 13 May 2020.] http://arxiv.org/abs/1410.7326.
31. Asynchronous Methods for Deep Reinforcement Learning. Volodymyr, Mnih, et al.
2016. International Conference of Machine Learning.
32. Wu, Yuhuai, et al. OpenAI Baselines ACKTR & A2C. [Online] 2017. [Cited: 07 Juni
2020.] https://openai.com/blog/baselines-acktr-a2c/.
33. Schulman, John, et al. OpenAI blog. [Online] OpenAI, 2017. [Cited: 14 June 2020.]
https://openai.com/blog/openai-baselines-ppo/.
34. Schulman, John, et al. Proximal Policy Optimization Algorithms. arXiv. 2017.
35. Schulman, John, et al. Trust Region Policy Optimization. [Online] 2015. [Cited: 14
June 2020.] https://arxiv.org/abs/1502.05477.
36. Snes9x Team. Snes9x. [Online] [Cited: 2020 April 17.] http://www.snes9x.com/.
37. Bizhawk Team. [Online] [Cited: 2020 April 17.] http://tasvideos.org/Bizhawk.html.
38.

The

Libretro

Team.

Retroarch.

[Online]

[Cited:

17

April

2020.]

April

2020.]

https://www.retroarch.com/.
39.

OpenAI.

Gym-retro

Documentation.

[Online]

[Cited:

17

https://retro.readthedocs.io/en/latest/index.html.
40. Gotta Learn Fast: A New Benchmark for Generalization in RL. Alex, Nichol, et al. 2018,
pp. 1-21.
41.

Fandom

Wiki.

Castlevania

fandom.

[Online]

[Cited:

26

April

2020.]

https://castlevania.fandom.com/wiki/Super_Castlevania_IV.
42.

Nintendo.

Nintendo

games.

[Online]

[Cited:

26

April

2020.]

https://www.nintendo.com/games/detail/super-castlevania-iv-3ds/.
72

Olav Lillerovde

38848

43. Jupyter Project. Jupyter. [Online] [Cited: 28 April 2020.] https://jupyter.org/.
44. Plotly. Plotly. [Online] [Cited: 26 April 2020.] https://plotly.com/.
45.

Warner

Brothers.

[Online]

[Cited:

4

Mai

2020.]

https://www.warnerbros.com/#/page=video-games.
46. Sammut, Claude and Webb, Geoffrey I. Encyclopedia of Machine Learning and Data
Mining. s.l. : Springer, 2011.
47. Bishop, Christopher M. Pattern Recognition and Machine Learning. s.l. : Springer,
2006.
48. Data Preparation for Data Mining. Schichao, Zhang, Chengqi, Zhang and Qiang, Yang.
17, 2003, Applied Artificial Intelligence, pp. 375-381.
49. Roger, Sapsford and Victor, Jupp. Data Collection and Analysis, second edition. s.l. :
Sage, 2006, pp. 174-175.
50. Mitchell, Tom. Machine Learning. s.l. : McGraw Hill Higher Education, 1997.
51. Ng, Andrew. CS229 Lecture Notes - Supervised Learning. 2012.
52. Logistic Regression Analysis and Reporting: A Primer. Peng, Joanne and Tak-Shing,
So. 2002, Understanding Statistics: Statistical Issues in Psychology, pp. 30-70.
53. Improved protein structure prediction using potentials from deep learning. Senior,
Andrew W, et al. 577, 2020, pp. 706-710.
54. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Vinyals,
Oriol, et al. 575, s.l. : Nature, 2019, pp. 350-354.
55. Webb, Geoffrey I and Sammut, Claude. Encyclopedia of Machine Learning and Data
Mining. s.l. : Springer US, 2010.
56. Ji, Hongjing and Alfarraj, Osama and Tolba, Amr. Artificial Intelligence-Empowered
Edge of Vehicles: Architecture, Enabling Technologies, and Applications. IEEE Access
2020, pp.1-1
57. Hastie, Trevor. Tibshirani, Robert. Friedman, Jerome. The Elements of Statistical
Learning â€“ Data Mining, Inference, and Prediction. Springer 2017. Pp 222

73

Olav Lillerovde

38848

58. Candida, Ferreira. Gene Expression Programming: A new Adaptive Algorithm for
Solving Problems. 2001. Complex Syst. Vol 13.
59. Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron. Deep Learning. 2016 MIT Press.
Pp 200-220. https://www.deeplearningbook.org
60. https://www.wikiwand.com/en/Reinforcement_learning downloaded (05.11.2020)

Figure 1: The relationship between AI, ML, RL, DL and DRL (56) ......................................... 9
Figure 2: Linear regression graph example. Datapoints as blue dots and the fitted line in red 19
Figure 3: Gradient descent example. Each circle represents a point in time and how it moves
closer to the minimum. The black slope on the right only represents the slope of the first circle.
The slope would actually move until becoming a horizontal line at the local minimum......... 21
Figure 4: Sigmoid function graph. Visual representation of the sigmoid function asymptotically
placing it between 0 and 1. ....................................................................................................... 22
Figure 5: Artificial Neuron (22). Shows a single neuron, the input nodes shown as â€œlabelsâ€ on
the left all connect into the neuron in the middle. The input nodes are multiplied with a weight
and is summarized in the neuron. The output is determined by passing the sum through a transfer
function..................................................................................................................................... 25
Figure 6: Neural Network. Node layers are connected forward from left to right, the first set of
neurons form the input layer, the last node on the right forms the output layer, and the series of
layers of nodes in the middle form the hidden layer. ............................................................... 26
Figure 7: Visual representation of reinforcement learning and the different parts making up the
process (60) .............................................................................................................................. 27
Figure 8: A genotype to phenotype example. NEAT algorithm (28). Table containing
information about each node and its connections. And a visual representation of the table below.
.................................................................................................................................................. 30
Figure 9: RAM-Search in Mortal Kombat SNES, main player fighter on left and enemy fighter
on the right, on the left side contains realtime saved virtual memory addresses and their values.
The right top box contains the tool and shows information regarding virtual addresses and their
values. ....................................................................................................................................... 38
Figure 10: Values from data.json Mortal Kombat. Data file contains the following information:
given Variable name, the virtual memory address, and the type of address ............................ 39
Figure 11: Rewards and Penalties used for training in Mortal Kombat ................................... 40
74

Olav Lillerovde

38848

Figure 12: Super Castlevania 4, Simon attacking an enemy with his chain, simon on left, enemy
skeleton on right ....................................................................................................................... 42
Figure 13: Rewards over entry number, A2C 1 million timesteps Super Castlevania 4 .......... 44
Figure 14: AI stuck under staircase Super Castlevania 4. Main character on the left with whip,
and skeleton enemy on the left ................................................................................................. 45
Figure 15: Rewards over entry number, PPO2 1million timesteps Castlevania 4 ................... 46
Figure 16: Comparison Rewards over entry number, A2C and PPO2 1million timesteps
Castlevania 4 ............................................................................................................................ 47
Figure 17: NEAT algorithm training 30 generations Super Castlevania 4 .............................. 49
Figure 18: Adjusted Fitness NEAT algorithm Super Castlevania 4 ........................................ 50
Figure 19:snapshot from the game Mortal Kombat, AI playing as the fighter on the left, SubZero .......................................................................................................................................... 52
Figure 20:Reward over entry number, A2C 1Million timesteps Mortal Kombat .................... 53
Figure 21:Rewards over length A2C Mortal Kombat .............................................................. 55
Figure 22:PPO2 1million timesteps Mortal Kombat................................................................ 56
Figure 23: Rewards over length PPO2 Mortal Kombat ........................................................... 57
Figure 24:Comparison Reward over Time, A2C and PPO2 Mortal Kombat .......................... 58
Figure 25: Comparison reward over length Mortal Kombat .................................................... 59
Figure 26: Neat Algorithm Fitness over generations Mortal Kombat ..................................... 60

Table 1 Evaluation table

14

Table 2: Example dataset with features month, rainfall and umbrellas sold

18

Table 3: Comparison A2C and PPO2 in Super Castlevania 4

47

Table 4: A2C PPO2 value comparison Mortal Kombat

58

75

