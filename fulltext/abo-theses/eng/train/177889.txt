DB2 for IBM i Performance

Mattias Lundell, 34792
Master's Thesis in Computer Science
Supervisors: Mats Aspnäs, Jan
Westerholm
Institutionen för informationsteknologi
Åbo Akademi University
2020

1

Table of contents
1

2

1.1

Problem

2

1.2

The Platform

2

1.3

Scope

2

4

4

The IBM i Ecosystem
The Hierarchical File System
2.1.1 Files
2.1.2 Programs and Service Programs

4

2.2

Process Scoping

5

2.3

The RPG programming language
2.3.1 Activation Groups
2.3.2 Data Types

6

2.1

3

2

Introduction

5
5

6
8
10

DB2 Architecture
3.1

Tables

10

3.2

Indexes
3.2.1 Maintained Temporary Indexes

10

3.3

Data Paths

12

3.4

SQL Query Options

12

3.5

Using Embedded SQL in the RPG Programming Language

15

11

24

Measuring Performance Reliably
General Performance Analysis tools
4.1.1 Index Advisor
4.1.2 System-level Analysis

24

4.2

Benchmarking Framework

26

4.3

Fundamental Benchmarking

30

4.4

Caching
4.4.1 Results caching
4.4.2 Execution plan caching

34

4.1

2

24
26

34
34

4.5
5

38

SQL performance
5.1

5.2

5.3

6

37

Overloaded System

Index Column Ordering
5.1.1 Test Setup
5.1.2 Results

38

Inserts
5.2.1 Indexes
5.2.2 Batch Size

45

Updates
5.3.1 Common Programs and Tables
5.3.2 Loop
5.3.3 Batch Updating
5.3.4 Batch Updating With Temporary Table
5.3.5 Results

52

39
43
45
52
55
58
60
62
64
67

Discussion

Swedish Summary

68

References

72

Appendix

75

3

Abstract
The purpose of this master's thesis is to help programmers working with DB2
for IBM i make up-to-date design decisions. Much has changed in the industry in
the past 20 years, and sources of information on both the Internet and internal
company policies are outdated and misleading.
Underlying concepts of IBM i and DB2 that affect performance are explained. A
framework for benchmarking database programs on IBM i has been created, using
RPG as the programming language and the built-in DB2 database. Its design is
presented, and parameters relating to the performance of both RPG and SQL are
explored, to establish how accurate benchmarking tests can be set up.
Benchmarks were created for a few practical scenarios and the results are
analyzed, and compared to existing literature, when available. The
recommendation of IBM on index column ordering was shown to be suboptimal
when dealing with very large amounts of data. The performance of batch updates
can be improved by creating a temporary table in order to avoid changing the
same data that the selection criteria is based on.
keywords: databases, IBM i, benchmarking, RPG, SQL

1

1 Introduction
1.1 Problem
As a database application developer, knowing what the best way to accomplish a
certain task is often difficult. SQL offers us nearly infinite possibilities, but some
are better than others. Decisions are often based on copying old solutions, and
hearsay from the Internet. The optimal solution often changes over time, and
keeping track of what is the best course of action right now is time consuming.
One of the biggest hardware changes in the 2010s is the industry transitioning
from traditional hard drives (HDD) to fast solid state drives (SSD)[1]. Today a disk
query has a latency perhaps 20 times lower than before. hence, the ideal solution
has changed in many cases, because SSDs bottleneck less than HDDs. Another
large change from the 2000s is parallelization - CPUs today have many cores, and
databases can parallelize tasks internally, improving the performance of certain
queries. Not only the hardware is shifting - with the advent of open APIs (for 3rd
parties) in several industries (public service APIs, open banking and IoT to mention
a few) the way that data is processed in software has also changed. Twenty years
ago, a bank payment took three days to complete, and the data processing was
done purely overnight in large batch runs, but today transactions can be real-time,
requiring fast response times and high availability. Consider all these external
changes combined with how much both IBM i and DB2 have changed since the
early 2000s, and it is no longer strange for people to have misconceptions about
performance.

1.2 The Platform
All databases, and even specific versions of the same database, are different in
terms of behavior. They store data in different ways, have different index data
structures and have distinct features. DB2 on IBM i is the database exclusively
used in this thesis, and the method of communicating with the database is via
embedded SQL in the RPG programming language. Chapter 2 covers the
fundamentals of IBM i, while chapter 3 introduces the aspects of DB2 that are
relevant for the performance benchmarks done in chapter 5. DB2 for IBM i is often
confused with IBM's other DB product "DB2 for Linux, UNIX and Windows", but
despite sharing most of the SQL language features with the IBM i variant, the inner
workings and tool sets are completely different.

1.3 Scope
A complete RPG benchmarking framework was created that can be used to set

2

up various performance tests. The goal was to create a framework that could test
both throughput and latency. Chapter 4 describes how it measures performance
on IBM i, and different RPG and DB2 parameters are compared to establish
methods for performing the tests in chapter 5. Some aspects of SQL are
benchmarked in chapter 5 to show the performance impacts of practical everyday
implementation decisions. A near infinite number of SQL aspects could be
analyzed for performance, but for practical reasons only a few aspects are covered
here. The analyzed aspects have been chosen because they are relevant in the
daily work of database developers and can have a relatively large impact on
performance. Results are presented, analyzed and discussed in each subchapter
of chapter 5.

3

2 The IBM i Ecosystem
Before we delve into performance, we need to explain the terminology used in
the IBM i operating system (previously known as i5/OS and earlier OS/400), as it
differs from what a conventional operating system user is used to.
IBM i runs on IBM servers, with the latest being based on the POWER9 CPU[2].
They are bought from IBM and licensed per CPU core. The server I will be using
has 48 CPU cores in total, but only ten are licensed (i.e. usable). On a single server,
one or more partitions can run. A partition is a virtual machine holding the IBM
i operating system, database and programs. The partition that will be used for
testing here has 0.1 dedicated and 2 virtual CPU cores and 2GB RAM. 0.1 CPU
means that the partition on average is guaranteed a CPU time of 0.1 second every
second. However, as long as the other partitions are under light or moderate load,
the available amount of CPU cores will be larger than 0.1, up to the maximum of 2
cores.
The latest version of IBM i at the time of writing was V7R4, but V7R3 was used in
this work.

2.1 The Hierarchical File System
The traditional file system on the IBM i is the Hierarchical File System (HFS)[3].
It consists of objects which are placed in libraries. The libraries are themselves
objects that are placed in the system library QSYS. A library is most similar to a
folder, except that placing one library inside another is impossible. An object is
what is commonly referred to as a file. In the case of the HFS on IBM i, all files
are binary. There are no simple text files, such as the ones commonly used in
Windows or Linux environments. Although the files are binary, this does not mean
that we can actually read them from within the HFS, as no binary operations are
available in this context. Instead you interact with objects using various CL (Control
Language) commands and programming language constructs - such as SQL.
All objects share the same interface for authorities, but some authorities are
only relevant to certain object types. Authorities can be roughly divided into two
categories: object access and data access. If you have update authority for the
data, you can for example update rows if the object is a database table, but you
cannot make changes to the object. There are many things to consider regarding
authorities, but they are not relevant to the scope of this thesis and will not be
explained any further.

4

2.1.1

Files

The most central type of object when it comes to SQL is the *FILE[4]. A file on the
IBM i is an interface that lets you access physical records from a storage device, or
virtual records that can be a combination of data read from storage and generated
data. The interface is a set of columns with associated names and data types, and
a file may contain zero or more rows of data conforming to the interface. There
are physical files (PF) and logical files (LF).
A physical file (PF) describes a data format, and contains sets of data rows
adhering to the format. One set of data is called a member. SQL does not have
a concept of members. Thus, any physical file created as an SQL table will only
have one member. An example of files having multiple members is the traditional
way of storing source files on IBM i. Each RPG source text is stored in a file called
QRPGLESRC as an individual member.
Logical files (LF) also describe a data format, but they do not say how data is
physically stored in a storage device. They can be either keyed or non-keyed. A
non-keyed logical file simply transforms the format of the physical file into the
format of the logical file. In SQL this translates to a view. One non-keyed logical
file, or view, can join together multiple other physical or logical files into a single
interface. A keyed logical file is an SQL index, which physically stores an ordered
set of column values that is used to efficiently find relevant items in an underlying
table.

2.1.2

Programs and Service Programs

The other essential object type group for this thesis is executable programs[5].
They come in two shapes: programs (*PGM) and service programs (*SRVPGM).
A *PGM is an executable program, while a *SRVPGM is a non-executable set of
exported procedures that can be bound to a program or service program during
runtime. Dynamic Link Library (DLL) is the equivalent to *SRVPGM in Windows.

2.2 Process Scoping
User space processes are called jobs[6]. They are identified by a job number, a
10-character job name and a 10-character user name. A job can have multiple
threads[7], but RPG does not support multi-threading as of release V7R4.
However, the database engine runs tasks in parallel when applicable, even when
called from within RPG. Jobs have individual memory on the application level they do not unintentionally interfere with each other. An exception is resource
locking. If one job does a database operation that locks a row in a table, a second
job will not be able to make changes to that row. Such locks are usually unnoticed

5

because the second job waits for the first to finish instead of raising an error. Jobs
do, however, share certain resources. Examples of shared resources are the OS
level disk cache and the SQL execution plan cache.

2.3 The RPG programming language
RPG has been around in some form since 1959, but in 1994 the current RPG IV
(or ILE RPG) came to be[8]. Since then it has been revised many times. Previously
everything in RPG sources was column-based. This means that to declare variables
or call functions, you had to place variable names and attributes at exactly the
correct column or it did not compile. The line length limit was effectively only 72
characters, due to the six first characters being reserved for short comments and
the line length limit being 80. These limitations have gradually loosened over the
years, and with the release of IBM i V7R1 came "Fully free-form RPG"[9]. RPG was
freed of all remaining column restrictions by typing a special string **FREE at
the beginning of the program. By doing this you also disable the ability to declare
variables and expressions in the old style. In this work fully free-form RPG will be
used exclusively. A hello world example can be seen in the code listing below.

**FREE
ctl-opt
datfmt(*iso) timfmt(*iso)
dftactgrp(*no) actgrp(*caller)
main(printHelloWorld);
dcl-proc printHelloWorld;
dsply 'Hello world ' + %char(%date(20200223));
end-proc;

Listing 1. RPG code that prints a "hello world 2020-02-23" message on the
terminal screen. datfmt(*iso) and timfmt(*iso) tells the compiler
that dates should be in the form YYYYMMDD and times in HHMMSS when
converting to and from numeric types dftactgrp(*no) and
actgrp(*caller) sets the activation group model of the program to be
*caller .

2.3.1

Activation Groups

Activation groups hold the global and static variables of an RPG program, and

6

cache the program itself, enabling faster subsequent executions. invocations[10].
When a program is first called within an activation group, its global and static
variables that have the inz (initialize) keyword will receive the given value. In
listing 2 such a variable can be seen. The program uses actgrp(*caller) ,
hence the second time the program is called, the value will not be initialized again.
However, if the program in listing 3 (which has actgrp(*new) ) is called twice, it
will initialize the value each time, because a new activation group is created upon
program entry.

**FREE
ctl-opt
dftactgrp(*no) actgrp(*caller)
main(testProgram);
dcl-s globalVariable int(10) inz(0);
dcl-proc testProgram;
globalVariable += 1;
dsply %char(globalVariable);
end-proc;

Listing 2. An RPG program that uses the activation group of the calling
program. Calling this from the command line prints "1" the first time, "2"
the second and so on.

7

**FREE
ctl-opt
dftactgrp(*no) actgrp(*new)
main(testProgram);
dcl-s globalVariable int(10) inz(0);
dcl-proc testProgram;
globalVariable += 1;
dsply %char(globalVariable);
end-proc;

Listing 3. An RPG program that is identical to the one in listing 2, with
the exception of actgrp(*new) . A new activation group is created, and
globalVariable will always be initialized to zero, causing the program to
print "1" every time.
Activation groups can also affect the life cycle of SQL cursors depending on the sql
options of the program.

2.3.2

Data Types

Due to how closely interlinked DB2 and RPG have been throughout their
development, the data types in RPG match well with what is commonly used in
SQL.
RPG has most common numeric types, such as 1-, 2- and 4-byte integers, 4- and
8-byte floating-point numbers. An unusual aspect of RPG is its common usage of
the zoned and packed decimal number formats. A zoned numeric supports one
decimal number per byte and is binary compatible with IBM's EBCDIC character
encoding, meaning that a number and a character of the same number has the
same binary representation. A packed decimal has two digits per byte. All the
numeric types can be assigned between each other. Assigning a value larger than
the variable can hold crashes the program.
Strings can be fixed size or static size. All variables are allocated statically in RPG.
Thus, a variable size string takes the same amount of memory space as a fixed size
with an additional two or four bytes for the string length. Still, variable size usually
offers better performance because you can avoid trimming and iterate over less
character data at a time. Strings have a compile-time encoding, which can be

8

set with the ccsid keyword. Assignments between strings with different ccsid
attributes result in an implicit conversion. String assignments have no limitations
on length. If you assign a long string variable to a short string it will be quietly cut
off.
In figure 1 below the most common types and their DB2 equivalents are listed.
RPG
declaration

SQL
Declaration

Description

int(5)
int(10)
int(20)

smallint
integer
bigint

zoned(n,
d)
packed(n,
d)

numeric(n,
A decimal number with a total of n digits
d)
with d of them being after the decimal
decimal(n, separator.
d)

char(n)

char(n)

1-, 2- and 4-byte integers

A fixed-size character string of length n

varchar(n) varchar(n)

A variable-size character string of length
n

date

date

A date. The format of the date can be
specified using the datfmt and datsep
keywords.

time

A time. The format of the time can be
specified using the timfmt and timsep
keywords.

timestamp

timestamp

A timestamp (combined date and time).
The format can be specified using the
same keywords as for date and time.

ind

Not available.
smallint
is commonly
used for
boolean one
or zero.

Indicator - the boolean of RPG.
Technically it is a specially treated
char(1) with value *on ( '1' ) or *off
( '0' ). A smallint can be read directly
into an RPG indicator if converted to a
char and vice versa.

time

Figure 1. Mappings between RPG and SQL data types[11].

9

3 DB2 Architecture
Like all other SQL databases, data can be fetched from DB2 using SQL (Structured
Query Language). DB2 is not only an SQL database though, as it inherits from
its IBM i roots. Many SQL objects are mapped to actual objects in the HFS. They
can be interacted with through CL commands. Examples of such operations are
reorganizing files or indexes and granting authorities to users.

3.1 Tables
Tables are always simple arrays in DB2 - clustering indexes are unavailable. The
physical position of an entry is called RRN - Relative Record Number. When a
deletion is made, it leaves an empty unused position in the array. By default such
holes will be reused, but it can be disabled if, for example, you temporarily need
to treat RRN as a unique ID until a proper ID has been implemented.

3.2 Indexes
In DB2, there are two kinds of database indexes[12]. The default index is a binary
radix tree. Binary means that before being put into a radix tree, all the indexed
columns are concatenated into a single binary string. In practice this makes
indexes data type-agnostic, and the number of columns does not affect index
lookup performance. The leaves in the tree are the RRNs of the table. An example
of a binary radix tree can be seen in figure 2 below. Theoretically it allows for
constant lookup times (O(k), where k is the maximum length in bits of the indexed
column). Both inserts and deletes are based on lookups, hence they are also O(k).
In practice it might be better to treat the scaling as approximately O(log(n)), where
n is the number of rows in the table.

10

Figure 2. From [13]. What a binary radix tree might look like. Data is
compressed as much as possible at each node, and each node has only
two children.
A binary radix index may also be created as a sparse index[14], only containing
rows matching a where statement that is set when creating the index. Another
feature of the binary radix index is indexing on derived keys[15]. Built-in functions
and expressions can be used to modify a column and use that value for the index.
An example is a timestamp column, where we want to find all rows where the
time portion of the timestamp is between 13.00 and 15.00. A lookup by the time
portion results in a full table scan, unless a derived key index has been created for
the time portion of the timestamp.
The second index type is encoded vector indexes (EVI). The data structure used
is a bitmap variant. EVIs are unique to DB2, and are used to index columns with
a limited set of unique values. Their main usage is in statistical analysis of static
data[16] for queries such as "How many new female customers have we gained
this year?". If an SQL statement does an index scan, it's possible that it would be
faster to use an EVI than a binary radix index, because an EVI could store the same
information in less space if the number of distinct values is small.
By default, all indexes are maintained in real time - i.e. when you do a write
operation it updates both the table and all indexes on the table.

3.2.1

Maintained Temporary Indexes

Forgetting to create an index and subsequently suffering from bad performance
is a common occurrence in IT. The SQL engine keeps usage statistics on both
existing indexes, and hypothetical indexes that don't exist. This is a useful source

11

of information when analyzing system performance. In some cases it suggests
nice-to-have indexes that would slightly increase the performance, but there are
also queries for which there are no usable indexes. IBM i automatically suggests
indexes that it thinks should be created and also creates them automatically if
necessary. Indexes created this way are called Maintained Temporary Indexes
(MTI). They are temporary - they will be discarded when an IPL (Initial Program
Load) is done. An IPL is for example done at OS or hardware upgrades, or if you
want to reset the system to its initial configuration state for some reason. One
can run on MTIs for a long time without noticing it, only to suddenly experience
slowdowns after an IPL has been done.

3.3 Data Paths
A data path is a description of how a query accesses data from the system storage
device - i.e. which *FILE or *FILEs must be opened to access the database
records. An Open Data Path (ODP) means that the database files have been left
open, thus the data path can be accessed quickly. Data paths are left open once
a query has been run a certain amount of times. By default a data path is left
open after the second execution. In practice this generally means that the first
two executions are slow, while the following ones are fast. This is important when
worst-case latency needs to be below a set threshold. A service consumer might
have to do additional calls to the consumed service in the initialization phase
in order to be able to guarantee that subsequent real requests run within the
required time. The problem escalates if you introduce multiple worker jobs and
many different queries.

3.4 SQL Query Options
SQL query attributes are stored in a special system table called QAQQINI [17].
There are three classes of QAQQINI tables. By default a global system table is used.
A library can have its own QAQQINI table, and a job can be configured to use that
instead of the global attributes. The third option is a temporary QAQQINI table,
and this is the alternative that will be used later. You can find a complete list of the
options in IBM's documentation[18]. In figure 3 below a list of the ones that are
relevant for this work are explained.
The built-in SQL procedure override_qaqqini [19] in the QSYS2 system library
can be used to create and discard a temporary QAQQINI table for the current job.
When created, it will inherit the current attributes of the job. The same procedure
is also used for setting the QAQQINI attributes. Usage of the procedure is shown in
listing 4 below. It can be called from within an RPG program, or it can be executed
manually before running the RPG program.

12

-- create the temporary QAQQINI table
call qsys2.override_qaqqini(1);
-- disable MTIs
call qsys2.override_qaqqini(2,
'ALLOW_TEMPORARY_INDEXES', '*ONLY_REQUIRED');
-- discard the temporary QAQQINI table
call qsys2.override_qaqqini(3);

Listing 4. An example of how the override_qaqqini procedure is used.

13

Option

Values and descriptions

ALLOW_TEMPORARY_INDEXES

Defaults to *YES. Allows the
creation of MTIs. This is useful for
testing performance, because you
usually want to create indexes
yourself before the engine does it
for you.

CACHE_RESULTS

Query results can be cached on
various levels: none, job or system.
System is the default. For
performance testing this can be
turned off, to prevent subsequent
benchmarks from affecting each
other.

OPEN_CURSOR_THRESHOLD and
OPEN_CURSOR_CLOSE_COUNT

OPEN_CURSOR_THRESHOLD
describes how many cursors can be
left open after being pseudoclosed. By default no limit is
enforced. When a limit is set,
OPEN_CURSOR_CLOSE_COUNT
describes how many pseudo-closed
cursors will be hard-closed when
the limit is reached.

PARALLEL_DEGREE

Decides what kinds of parallel
processing can be done, and to
what extent. Parallel processing
can be either on the disk level or
the CPU level. On the disk level the
number of drives the needed data
is spread out on sets an upper
limit. On the CPU level the number
of available CPU cores limits the
parallel degree. Unless noted
otherwise, I will have this set to
*OPTIMIZE, which means that the
engine decides what is the optimal
parallelism degree.

14

When an open data path is used,
SQE may check the values of the
parameters to see if the access
plan is still good, or if a new one
PSEUDO_OPEN_CHECK_HOST_VARS should be made. The default is to
disallow this. In cases where the
optimal access plan varies greatly
between different parameters, this
option may be better.
SQL_STMT_REUSE

How many times an sql statement
needs to be run in a job before the
data path is left open. The default
value is three.

REOPTIMIZE_ACCESS_PLAN

SQE saves access plans for queries.
With this option you can force it to
re-optimize the access plan instead
of using the cached plan. This is
useful for worst-case performance
testing.

STORAGE_LIMIT

By default the amount of memory
a query can use is unlimited. If a
limit is set, queries will be aborted
if they are estimated to exceed the
storage threshold. If we want to
test memory footprint between
queries, we can use this to roughly
estimate upper limits on storage
usage.

SYSTEM_SQL_STATEMENT_CACHE

When set to *NO, the system
cache is not checked for cached
query plans.

Figure 3. A description of the relevant performance statistics of the
QUSRJOBI API.

3.5 Using Embedded SQL in the RPG Programming
Language
RPG has always had close integration with the database. Previously files (database
tables) were declared in the RPG program and low-level file operations, such
as navigating an index and updating the currently opened row, were applied on
them. However, now SQL statements are written directly in the RPG code, and the
results can be written directly into RPG variables using the into keyword. RPG

15

variables can be freely used in the SQL statements by prefixing them with a colon
(:).
In listing 5 and listing 6 below all ways to interact with SQL is shown in a series
of examples. In the first, a table is created, rows are inserted, read, updated and
deleted. assertSqlSuccess is part of a custom error handling framework. As the
name implies, it checks if an error occured, parses the sql error from the SQL
Control Area (SQLCA) and throws it as a program message[20]. In the second SQL
cursors are used to fetch more than one row with two alternative methods. Both
methods have the same declare and open statements, but when retrieving
data the first fetches one row at a time while the other fetches up to 50 at the
same time.

16

**FREE
ctl-opt
datfmt(*iso) timfmt(*iso)
dftactgrp(*no) actgrp(*caller)
main(main);
// data structure matching the example table
dcl-ds person_t qualified template;
id int(10);
name varchar(50);
hobby varchar(50);
end-ds;
// program-specific sql options
exec sql set option
commit = *none, closqlcsr = *endmod,
datfmt = *iso, timfmt = *iso,
dlyprp = *yes;

dcl-proc main;
createTable();
doInserts();
selectSingleRow();
selectMultipleRows();
selectMultipleRowsBatch();
doUpdates();
doDeletes();
end-proc;

dcl-proc createTable;
exec sql
create table mylib.persons (
id integer primary key,
name varchar(50) not null,

17

hobby varchar(50)
);
assertSqlSuccess(sqlCA);
end-proc;

dcl-proc doInserts;
exec sql
insert into mylib.persons (id, name, hobby)
values (
1, 'Mattias Lundell', 'Programming' ), (
2, 'Luna the cat', 'Sleeping'
);
assertSqlSuccess(sqlCA);
// we leave hobby as null for id=3.
exec sql
insert into mylib.persons (id, name) values
(3, 'Stranger');
assertSqlSuccess(sqlCA);
end-proc;

dcl-proc selectSingleRow;
dcl-s numberOfEntries int(10);
dcl-ds cat likeds(person_t);
dcl-s strangerHobby varchar(50);
dcl-s isStrangerHobbyNull ind;
exec sql
select count(*) into :numberOfEntries
from mylib.persons;
assertSqlSuccess(sqlCA);
// numberOfEntries == 3.

18

// all fields can be read into
// a matching data structure
exec sql
select id, name, hobby into :cat
from mylib.persons
where id = 2;
assertSqlSuccess(sqlCA);
// cat.id == 2
// cat.name == 'Luna The Cat'
// cat.hobby == 'Sleeping'
// when reading a null column we need to pass a
// second RPG variable: an indicator.
// it will be set to *on if hobby is null.
exec sql
select hobby
into :strangerHobby :isStrangerHobbyNull
from mylib.persons
where id = 3;
assertSqlSuccess(sqlCA);
// strangerHobby is undefined
// isStrangerHobbyNull == *on
end-proc;
dcl-proc doUpdates;
dcl-ds stranger likeds(person_t);
// we found out the stranger's hobby
stranger.id = 3;
stranger.hobby =
'Sending GDPR information removal requests';
exec sql
update mylib.persons set hobby =
:stranger.hobby
where id = :stranger.id;

19

assertSqlSuccess(sqlCA);
end-proc;

dcl-proc doDeletes;
exec sql
delete from mylib.persons
where id = 3;
assertSqlSuccess(sqlCA);
end-proc;

Listing 5. An RPG program with embedded SQL. The implementations of
selectMultipleRows and selectMultipleRowsBatch can be seen in listing 6.

20

dcl-proc selectMultipleRows;
dcl-ds entry likeds(person_t);
exec sql
declare cursor1 cursor for
select *
from mylib.persons;
exec sql
open cursor1;
assertSqlSuccess(sqlCA);
dow 1=1;
exec sql
fetch next from cursor1 into :entry
if sqlCode = 100; // "end of file"
leave; // break out of the loop
else;
assertSqlSuccess(sqlCA);
endif;
// do something with entry
process(entry);
enddo;
exec sql
close cursor1;
end-proc;
dcl-proc selectMultipleRowsBatch;
// declare an array of 50 persons
dcl-ds entries likeds(person_t) dim(50);
dcl-s numberOfRows int(10);
exec sql
declare cursor2 cursor for

21

select *
from mylib.persons;
assertSqlSuccess(sqlCA);
exec sql
open cursor2;
assertSqlSuccess(sqlCA);
exec sql
fetch next from cursor2 into :entries
for 50 rows;
assertSqlSuccess(sqlCA);
// the special variable sqler3 contains
// the number of rows that was read
numberOfRows = sqler3;
exec sql
close cursor2;
// do something with the entry array
process(entries: numberOfRows);
end-proc;

Listing 6. An RPG program that uses SQL cursors to retrieve all rows in the
example table.
When RPG encounters an SQL statement, it checks if the statement has already
been executed before, and whether an open data path exists. If a compatible
open data path exists DB2 reuses it. The first time a statement is executed in
a job it queries the system execution plan cache to find a cached execution
plan. If a plan is available, the indicated data path is opened and used. New or
rarely used statements may not be in the cache, and in that case DB2 analyzes
the available indexes and statistics, and creates a new execution plan for the
statement. The process is illustrated in figure 4. A cached plan may be invalidated
by changes to the queried tables, views or indexes. A plan may also be invalidated
for optimization if DB2 estimates that another index would be more efficient
based on usage statistics. In section 5 a situation where constant re-optimization
causes bad performance is analyzed.

22

A query is processed

Is there an open usable
data path in the job?
no
Is a usable execution plan
for the query available in
the system execution plan cache?
yes
Use the execution plan
from the cache

yes
Use the already
opened data path

no
Create new execution plan

Figure 4. What happens when RPG encounters an SQL query and calls the
database engine.

23

4 Measuring Performance Reliably
Measuring performance reliably is difficult, and many books have been written on
the subject[21][22] If you are careless you may end up benchmarking something
other than the thing you are intending to. For example some actions are much
more expensive than others, or an action may be fast in one situation and slow
in another. Factors like these may introduce much more overhead than the actual
test. A reliable test should give the same result every time you run it, and it
should generally be in line with what you expect. If the test results make no sense,
an unknown element has likely not been considered. Decisions cannot be made
based unless the results are understood.
An important factor when benchmarking something is to decide what needs to be
tested. Is it the worst-case latency, the maximum throughput, or something else?
Different scenarios require different testing setups. This section aims to analyze
how reproducible results can be achieved for a single system, when running the
same test multiple times. In order to easily run groups of tests and keep track of
results, I have created a simple benchmarking framework for RPG.

4.1 General Performance Analysis tools
While not being the main focus of this work, DB2 offers a variety of tools to
analyze and improve the performance of SQL queries.

4.1.1

Index Advisor

Index Advisor (IA) is a graphical tool where you enter an SQL statement and
then you are presented with an interactive graph that shows all the steps DB2
does when executing the query. Table scans, the most common culprit of bad
performance, are easy to spot. You can view a detailed information panel that says
why a table scan was used. Sometimes no index is available, but sometimes a table
scan is done because DB2 judges that it will be faster than using an index. This
commonly happens if the table is small. Screenshots from Index Advisor can be
seen in figure 5 and figure 6, while the explained table and SQL select statement
is in listing 7

24

create table cbstestd.mlutmpt (
id integer not null
);
select *
from cbstestd.mlutmpt
where id = 50;
�'13,% P> �& 1/* �,�*9: � ', 2& ',� 8 ��6'1-0 -52.52 ', #%50 N �,� 2&
0 * 6�,2 create table 12�2 + ,2>

�'%50 N> �,� 8 ��6'1-0 1&-71 2&�2 8 �53,% 2& ��� 7'** 0 15*2 ', � 2��*
1��,>

25

�'%50 O> �9 0'%&2�*'�)',% -, 2& 2��* 1��, 7 ��, -. , 2& H��6'1 �
�,� 8 1H 7',�-7 7& 0 ��K *'121 15%% 12 � ',� 8 2&�2 1&-5*� '+.0-6
. 0!-0+�,� >

4/1/2

�-%&� 0��*����!��-%�%

�+,/# �� ,7 >#6A *22" $26 "#>#/24#67 82 >#6,$A 8+#,6 ��� 5=#6,#7 "=6,1* 8+#
"#>#/240#18 4+�7#D 21# "2#7 128 �/?�A7 .12? ?+,!+ ��� 78�8#0#18 ,7 !�=7,1*
462 /#07 ,1 � 7A78#0 /21*#6 "2?1 8+# /,1#F �+�8 ?26.#" ?#// "=6,1* "#>#/240#18
0�A 128 4#6$260 7='!,#18/A 21!# 8+# "�8� 7,B# *62?7 ,182 8+# 0,//,217F ��� ,
7826#7 78�:7:!7 �1" �">,7#" ,1"#@#7 $26 �// ��� 78�8#0#187 ,8 #@#!=8#7D �1" 8+#7#
78�:7:!7 !�1 # 5=#6,#" �8 �1A:0#F � "�8� �7# �"0,1,786�826 0�A ?�18 82 &1"
?+,!+ 78�8#0#187 �6# 6=11,1* 7/2?#78 21 8+# 7A78#0D 26 ?+,!+ 21#7 �6# 6=11,1*
8+# 0278 1=0 #6 2$ :0#7F �+#6# �6# 28+ *6�4+,!�/ 822/7 �1" ��� 7#6>,!#7 82
6#86,#>#��1"�0�1�*#�,1"#@#7��1"�=7�*#�78�:7:!7MTUNF
�+#7# �6# 8+# 822/7 A2= ?2=/" 78�68 ?,8+ ?+#1 86A,1* 82 4,142,18 � 4#6$260�1!#
,77=#F �+# $2!=7 2$ 8+,7 ?26. ,7 ?+�8 A2= "2 ?+#1 A2= "2 #>#6A8+,1* 8+# 822/7
7=**#78� =8�A2=6�78�8#0#187�7://�6=1�7/2?F

4/2 ��!�� �$��!���$� �+"$�
��� 462>,"#7 �1 ��� ��������MTVN $26 0#�7=6,1* >�6,2=7 4#6$260�1!# 78�:7:!7
,17,"# � -2 #8?##1 !�//7 82 8+# ���F �+# ,18#6#7:1* &#/"7 �6# /,78#" ,1 &*=6# Y
#/2?F

26

Field

Description

Elapsed
time

Physical time that has passed.

Disk IO

Total number of disk IO calls.

Disk IO
Sync

Number of synchronous IO calls. A synchronous IO call
means that the job blocks and waits until the resource has
been fetched from disk.

Disk IO
Async

Number of asynchronous IO calls. An asynchronous call is
when an disk operation is started, but instead of waiting for
it to finish the system is able to do other work until it finishes
- thus maximizing resource usage. The more of these in
relation to synchronous calls, the better, in general.

Processing
Time spent utilizing the CPU.
Unit Time
Processing
Unit DB
CPU time spent in the Database.
Time
Lock wait
How much time has been spent waiting on locked resources.
time
How many memory page faults occurred. A page fault is
page fault when the data we need is not available in the memory or
count
disk cache and needs to be read from disk. Reading from
disk is expensive and should be minimized.
Figure 7. A description of the relevant performance statistics of the
QUSRJOBI API.
Individual test cases are entered into a table STATRUNT. Its fields are listed in
figure 8 below:

27

Field

Description

group

Main parameter when running tests. It describes
which tests should be executed together.
Typically one group consists of one test program
with a set of different data sets or runtime
properties (see the properties a few rows down).

program

Which program (*PGM) that should be run.

dataset

What data set configuration the test should use.

actgrp

*NEW or *CALLER. Decides which wrapper
program to use when running the test.

stgmdl

Which storage model the program should use.
Either the new *TERASPACE or the traditional
*SNGLVL).

qaqqiniOverrideGroup

An identifier for which set of QAQQINI properties
to use.

warmupLogger

Should the logger be warmed up before running
the actual test? Normally it should, unless when
we want to show what happens if we don't.

submitToJobQueue

Set if the test should be submitted to the given
job queue instead of being called directly. Used
to test the impact of job-level caching.

orderInGroup

The order in which the program should be run
within the test group. This ensures that tests are
always run in the same order, which reduces
cache-induced randomness in the test results.

Figure 8. A description of the fields in the table STATRUNT.
A second table STATLOGT contains the performance statistics gained from
QUSRAPI. In figure 9 below you can see the full database.

28

STATRUNT
id

bigint
primary key
group
varchar(50)
program
varchar(21)
dataset
varchar(100)
actgrp
varchar(10)
stgmdl
varchar(10)
qaqqiniOverrideGroup varchar(50)
warmupLogger
smallint
submitToJobQueue
varchar(10)
orderInGroup
int
testid -> id
STATLOGT
testid

bigint
foreign key(STATRUNT.id)
logTimestamp
timestamp
testInstanceId
bigint
elapsedTime
decimal(10, 3)
diskIo
bigint
diskIoAsync
bigint
diskIosync
bigint
processingUnitPercent
decimal(10, 1)
processingUnitDbPercent decimal(10, 1)
processingUnitTime
decimal(10, 3)
processingUnitDbTime
decimal(10, 3)
lockWaitTime
decimal(10, 3)
pageFaultCount
bigint

Figure 9. An overview of the tables used by the benchmarking framework.
The program call chain is shown in figure 10 below.

29

Run set of tests by group column from STATRUNT

STATRUNR
Fetches a set of tests to run from STATRUNT by given group.
Runs or submits them in order

STATRUNR01
ACTGRP=*CALLER
STGMDL=*TERASPACE

STATRUNR02
ACTGRP=*CALLER
STGMDL=*SNGLVL

STATRUNR03
ACTGRP=*NEW
STGMDL=*TERASPACE

STATRUNR04
ACTGRP=*NEW
STGMDL=*SNGLVL

STATRUNM
Common implementation of STATRUNR[01-04]
log

call test program

STATRUNS
Warms up logger
Logs performance stats to STATLOGT

A test program that works on a given data set

Figure 10. An overview of the program structure of the logging
framework.
Using the described testing framework, we will explore what effect the various
options has on the performance.

4.3 Fundamental Benchmarking
Using the test framework, we are able to create and configure any test cases
within the parameter space described so far. Included in all tests is calling the
program, parsing the parameters and calling a procedure that does the
computing. In the first benchmark we will only run RPG code. We will run an
RPG for loop for 10000 iterations and see how the parameters affect the gained
statistics. Four different configurations are used. We will vary the activation group
between *CALLER and *NEW, and the test will either be executed directly or
submitted to a new job. Technically there is no difference between the two
activation group options when the job is submitted, but we still test both to verify
that it behaves as expected. Common for all tests is that we will use storage model
*TERASPACE and the logger will be warmed up. Each test configuration is run
twice in a row. Ideally both runs should give the same results.

30

Activation group Submitted Elapsed time (ms)

Disk stats

*CALLER

1.0

Disk IO: 3 (1 async)
Page faults: 2

*CALLER

0.0

Disk IO: 0 (0 async)
Page faults: 0

*CALLER

X

1.0

Disk IO: 1 (1 async)
Page faults: 0

*CALLER

X

1.0

Disk IO: 1 (1 async)
Page faults: 0

*NEW

1.0

Disk IO: 1 (1 async)
Page faults: 0

*NEW

1.0

Disk IO: 1 (1 async)
Page faults: 0

*NEW

X

1.0

Disk IO: 1 (1 async)
Page faults: 0

*NEW

X

1.0

Disk IO: 1 (1 async)
Page faults: 0

Figure 11. Performance statistics from the first test run. Because of it being
called for the first time, the program is not yet available in the disk cache,
and the system experiences two page faults. In all the following tests the
program is cached and no page faults occur. Clearly, running a test group
with ACTGRP(*CALLER) messes with the performance statistics because
it caches everything. Consequently the system needs no disk IO at all when
doing the second call.
In figure 11 the performance statistics from running the test is shown. The first
program call caused two page faults because the program had not recently been
read into memory. However, the second call caused no disk IO at all. Because the
same activation group was active for both program calls the allocated program
was reused in the second one. For reference, the same test groups were executed
a second time to see if the results are persistent. The results can be seen in figure
12.

31

Activation group Submitted Elapsed time (ms)

Disk stats

*CALLER

1.0

Disk IO: 1 (1 async)
Page faults: 0

*CALLER

0.0

Disk IO: 0 (0 async)
Page faults: 0

*CALLER

X

1.0

Disk IO: 1 (1 async)
Page faults: 0

*CALLER

X

1.0

Disk IO: 1 (1 async)
Page faults: 0

*NEW

1.0

Disk IO: 1 (1 async)
Page faults: 0

*NEW

1.0

Disk IO: 1 (1 async)
Page faults: 0

*NEW

X

1.0

Disk IO: 1 (1 async)
Page faults: 0

*NEW

X

1.0

Disk IO: 1 (1 async)
Page faults: 0

Figure 12. Performance statistics from the second test run. The first call
did not cause any page faults this time. The data is identical in all other
senses.
The page faults did not occur the second time since only five minutes passed inbetween the test runs. Besides that, the results are consistent. Activation group
*CALLER in a direct call causes unreliability, and will therefore not be used for
general benchmarking purposes. Since no difference was observed in activation
group option when submitting calls, we will only use *NEW from now on.
We have established that no difference between submitting a job or running the
test program directly when we use *NEW can be observed, as long as the test only
runs RPG code. If the test does a single select the numbers turn out differently,
as seen in figure 13. Again the first execution took a long time since it had never
been run before, and an execution plan had to be created, and the table and
indexes had to be read from disk. The submitted jobs had a minor overhead, which
became only 1-2 ms once the test had been run many times. The same overhead
occurred when doing 10K selects instead of one.

32

Submitted

Elapsed time (ms)

Disk stats

26.0

Disk IO: 30 (4 async)
Page faults: 24

1.0

Disk IO: 0 (0 async)
Page faults: 0

X

8.0

Disk IO: 3 (2 async)
Page faults: 1

X

6.0

Disk IO: 1 (1 async)
Page faults: 0

2.0

Disk IO: 0 (0 async)
Page faults: 0

1.0

Disk IO: 1 (1 async)
Page faults: 0

X

6.0

Disk IO: 1 (1 async)
Page faults: 0

X

6.0

Disk IO: 1 (1 async)
Page faults: 0

...

...

...

1.0

Disk IO: 0 (0 async)
Page faults: 0

1.0

Disk IO: 0 (0 async)
Page faults: 0

X

3.0

Disk IO: 0 (0 async)
Page faults: 0

X

2.0

Disk IO: 1 (1 async)
Page faults: 0

Figure 13. Performance statistics from running the test set multiple times.
The first 8 rows are from the initial run, and the last 4 are what the values
stabilized to after running the test 10 times. The first time the program
was run a lot of disk IO occurred due to it creating the query execution
plan and caching it.
Waiting a few hours in between running the test suite, or running the tests as
another user did not affect the statistics. This shows that the system cache is
shared between jobs efficiently. The extra overhead caused by running the test in
a new job could be explained as the overhead in querying the system execution
plan cache instead of reusing an existing data path. To verify this, a warm up round
was added to the test program, meaning that the tested query is called three
times (due to the default value of QAQQINI option SQL_STMT_REUSE). Instead of
doing one sql select query we will do 10K. After running the test program with

33

both methods 20 times and discarding the results from the 10 first times, the
average elapsed time converged to 272 ms for both methods. Hence, it can be
concluded that if a test does warmup, whether it runs directly or in a new job does
not affect performance under optimal circumstances. Non-optimal circumstances
is covered in the next section.

4.4 Caching
In the tests thus far, the system caches have been able to improve the
performance. However, in real life data is often not cached, and if testing worst
case latency, warming up the query interferes with the result. The disk cache
cannot be turned off, but we can control the DB2 results cache and the query
execution plan cache. In order to accomplish this we will explore some QAQQINI
parameters.
The
most
important
setting
is
to
turn
off
ALLOW_TEMPORARY_INDEXES, in order to control what indexes are available.
No noticeable effect was observed when in figure 14.

4.4.1

Results caching

The first option to turn off is CACHE_RESULTS. As the name implies, it allows
DB2 to cache and reuse the results of a query on the job or system level. By default
it caches on system level. When set to *NONE, the program is forced to always
fetch data from the disk (or disk cache). Running the previous 10K selects test with
CACHE_RESULTS set to *NONE increased the spread of the elapsed time a little,
raising the average by 3 ms for the directly run test, and 10 ms for the submitted
test. The median for them only differed by 1 ms though, hence the data is not
enough to confirm whether submitting the job had a measurable effect or not. The
value of CACHE_RESULTS seems to have no effect on the number of disk IO and
CPU time.

4.4.2

Execution plan caching

The
QAQQINI
options
SYSTEM_SQL_STATEMENT_CACHE
and
REOPTIMIZE_ACCESS_PLAN can be used to prevent the database engine from
accessing the cached query execution plan from the system cache. In figure 14
below statistics from running the test with different parameters. Three sets of
QAQQINI options were used: default values, CACHE_RESULTS and all of the
parameters described in this chapter. Rounds were done on both one and 10K
iterations without warmup, and both by running directly and submitting a new
job. The activation group used was *NEW in all cases. Each test was run 10
times and the aggregated statistics are shown in the fourth column. When doing
the same test with warmup, the time elapsed for the single selects were zero,

34

showing that re-optimization is not done every time a query is run even with
REOPTIMIZE_ACCESS_PLAN set to *FORCE. The submitted jobs were overall
slightly slower, which indicates that the directly exectued programs may have
access to more caching despite all other parameters being the same. In section 5,
submitted jobs will be used exclusively.

35

Submitted

QAQQINI setup

X

results not cached

X

X

results not cached

1

range 3.0 - 11.0ms
average 3.8ms
median 3.0ms

1

range 2.0 - 8.0ms
average 3.4ms
median 3.0ms

1

range 1.0 - 2.0ms
average 1.3ms
median 1.0ms

1

range 4.0 - 19.0ms
average 6.4ms
median 4.5ms
range 2.0 - 4.0ms
average 2.9ms
median 3.0ms

results and statements
1
not cached

range 3.0 - 6.0ms
average 4.1ms
median 4.0ms

10000

range 224.0 - 269.0ms
average 236.1ms
median 230.5ms

10000

range 222.0 - 270.0ms
average 233.0ms
median 228.5ms

10000

range 224.0 - 303.0ms
average 251.1ms
median 252.5ms

10000

range 224.0 - 321.0ms
average 254.5ms
median 236.5ms

results and statements
10000
not cached

range 228.0 - 305.0ms
average 254.2ms
median 243.5ms

results and statements
10000
not cached

range 224.0 - 304.0ms
average 247.6ms
median 231.5ms

results not cached

X

Aggregate stats

results and statements
1
not cached

X

X

Iterations

results not cached

Figure 14. Aggregated data from running each test ten times.

36

4.5 Overloaded System
IBM i hardware is expensive. They are by themselves cloud platforms on which
multiple partitions (separate virtual environments) run. This means that the
system may be used by others while you are running your test. Unless the partition
you test on has sufficient guaranteed CPU cores for your test case, your
performance will suffer if another partition with higher priority (such as a
production server) is under heavy load.
If irregular results are gained from a test, system overload can be detected by
comparing the CPU time and disk IO counts statistics to regular results. If the same
amount of time was spent in the CPU and the same number of disk IO operations
occurred, one can assume that the system was temporarily under heavy load
when that test ran. In this work, I discard such results and run the tests again until
a sufficient amount of consistent results are available.

37

5 SQL performance
IBM recommends a few techniques to improve performance in their official
documentation[25]. For example, they recommend to fetch as many rows as
possible instead of fetching one at a time using the fetch statement[26].

5.1 Index Column Ordering
Index Advisor often suggests indexes in what could intuitively be perceived as the
wrong order. For a developer it might seem natural to place the most selective
column first, i.e. the column that has the most variation in values. In fact, IBM
recommends placing the most selective column first[27], but Index Advisor often
recommends the opposite - the index in chapter 5.4 was suggested by IA. For
example, if we have a database representing options chosen by users we have at
least two columns - the ID of the user and an ID of the option. The user has an
option, therefore the user is the more important column and would be placed first
when we create an index.
The behavior of IA can perhaps be explained by analyzing the structure of the
binary radix index. In order to save space, storing as much common information
as shared prefixes is preferred over having a large amount of different prefixes at
the start. Assume that we have a massive amount of numeric user IDs, that are
connected to only two different option IDs (A or B). A simplified non-binary radix
tree illustrates the difference between the order of the columns in figure 15. An
index with a leading column with few possible values only needs to store each
unique value once. However, an index with the opposite column order must store
the value A or B at the end of every node.

38

�'%50 JN> �& 2-. 20 0 .0 1 ,21 �, ',� 8 7'2& 2& * ��',% �-*5+, � ',%
5,'/5 < �,� 2& �-4-+ 20 �, ',� 8 7'2& 2& * ��',% �-*5+, -,*9 &�6',%
27-��'" 0 ,2�6�*5 1>�� 11�,-� 1�'1�0 /5'0 ��2-�12-0 �2& �1 �-,�>
�1 8+# 28+#6 +�1"D 8+# 828�/ 1=0 #6 2$ 12"#7 8+�8 1##"7 82 # 86�>#67#" 21
/22.=47 ,7 /#77 ,$ 026# 62?7 !�1 # #/,0,1�8#" ?,8+ #�!+ 12"#F �1 46�!:!#D ",7.
�1" ��� ,7 =7=�//A 8+# 2;/#1#!.,1* $�!826F �>#1 ,$ � $#? 026# 86## 12"#7 0=78
# >,7,8#"D 8+# 6#"=!#" 828�/ 7,B# 2$ 8+# ,1"#@ 7+2=/" 0�.# =4 $26 ,8F �2 >#6,$A 8+,7D
8?2 ,"#1:!�/ 8� /#7 ?,8+ ",%#6,1* ,1"#@#7 +�># ##1 !6#�8#"D �1" 28+ ,17#687
�1" 7#/#!87 ?,// # #1!+0�6.#"F �20# "�8� �7#7D 7=!+ �7 �6�!/#D +�># � $#�8=6#
!�//#" Q,1"#@ 7.,4 7!�11,1*QMTZNF �8 �//2?7 �6�!/# 82 =7#D ,1 2=6 !�7#D �1 ,1"#@ 21
(optionId, userId) ?+#1 � 5=#6A ,7 "21# 21 21/A userIdD A 0�.,1*
6#4#�8#" /22.=47 $26 #�!+ >�/=# 2$ 24:21�"F �+# ?2678 !�7# ,1 1=0 #6 2$ /22.=47
,7 8+# 1=0 #6 2$ ",%#6#18 >�/=#7 $26 optionIdF ��T ,7 128 "2!=0#18#" 82 +�>#
8+,7 $#�8=6#D =8 #7,"#7 /22.=47 21 28+ !2/=017D /22.=47 21 21/A 24:21," ?,//
�/72� #� #1!+0�6.#"�$26�6#$#6#1!#F

5/1/1

��%&���&)#

�?2 ,"#1:!�/ 8� /#7 ?,8+ 8?2 !2/=017 ?#6# !6#�8#" K/,7:1* ZLF �+# !2/=01 id ,7

39

=1,5=#D ?+,/# optionId +�7 21/A W ",%#6#18 >�/=#7F �1 8+# &678 8� /# �1 ,1"#@
21 (id, optionId) ?�7 !6#�8#"D �1" 21 8+# 7#!21" 8� /# �1 ,1"#@ ?,8+ 8+#
24427,8# !2/=01 26"#6 ?�7 !6#�8#"F � 462*6�0 82 ,17#68 62?7 ,182 8+# 8� /#7 ?�7
!6#�8#"D =7,1* 8+# 0#8+2" 7+2?1 ,1 /,7:1* [ $26 28+ 8� /#7F �+# id ,7 � =1,5=#
7#5=#1:�/ 1=0 #6 �1" optionid ,7 8+# 6#0�,1"#6 2$ 8+�8 1=0 #6 ",>,"#" A WF
create table index_test_table_1
for system name MLUTIND01T (
id
bigint
not null,
optionId smallint not null
);
create index MLUTIND01I on MLUTIND01T
(id, optionId);
create table index_test_table_2
for system name MLUTIND02T (
id
bigint
not null,
optionId smallint not null
);
create index MLUTIND02I on MLUTIND02T
(optionId, id);
�'13,%�Q>����* 1��,��',� 8 1�!-0�2 13,%�',� 8��-*5+,�-0� 0',%>

40

dcl-proc generateTestData;
dcl-pi *n;
dataset varchar(50) const;
warmup ind const;
dataSize int(10) const;
end-pi;
exec sql
insert into index_test_table_1 (
id, optionId
)
with counter (i) as (
select 1 from sysibm.sysdummy1
union all
select i+1 from counter where i < :dataSize
)
select i, mod(i, 5)
from counter;
assertSqlSuccess(sqlCA);
end-proc;

Listing 9. The method used to insert rows
index_test_table_1 or index_test_table_2.

into

both

The program reading from the table does a variable number of inserts spread out
evenly over the possible id values, as seen in listing 10.

41

dcl-proc testIndexColumnOrderSelects;
dcl-pi *n;
dataset varchar(50) const;
warmup ind const;
dataSize int(10) const;
end-pi;
dcl-s
dcl-s
dcl-s
dcl-s
dcl-s

i int(10);
incrementSize int(10);
id int(20);
optionId int(5);
numberOfRowsInTable int(20);

numberOfRowsInTable = getTableXRowCount();
incrementSize = getIncrementSize(
numberOfRowsInTable: dataSize);
id = 1;
for i = 1 to dataSize;
optionId = %rem(id: 5);
exec sql
select id, optionId
into :id, :optionId
from cbstestd.mlutind0Xt
where id = :id
and optionId = :optionId;
assertSqlSuccess(sqlCA);
id = %rem(id + incrementSize:
numberOfRowsInTable);
endfor;
end-proc;
dcl-proc getIncrementSize;
dcl-pi *n int(20);

42

rows int(20) const;
dataSize int(20) const;
end-pi;
dcl-s incrementSize int(10);
incrementSize = rows/dataSize;
if incrementSize = 0;
incrementSize = 1;
endif;
return incrementSize;
end-proc;

Listing 10. The method used to insert rows into both
index_test_table_1 or index_test_table_2. the table
numbers have been replaced with X.
For testing if DB2 can do so-called skip scans, a query on the id column was done
for table 2 (listing 11). The rest of the test was identical to listing 10.

exec sql
select id, optionId
into :id, :optionId
from cbstestd.mlutind02t
where id = :id;

Listing 11. The same method was used for the index skip scan test, but only
id was specified in the where statement.

5.1.2

Results

For 10M rows in the tables, the index on index_test_table_1 was 202MB,
while the other one was 192MB. When scaling up to 100M rows, the sizes
increased to 2864MB and 1920MB, with the index on index_test_table_2
being smaller. Benchmarking the inserts for 10M and 100M rows produced the
results in figure 16. In either case, the difference in elapsed time when inserting
was small, but slightly more disk IO was required for the second table, and it had
2-4 times more page faults.

43

Iterations
small first
(2)

Elapsed time
(ms)

Disk stats

10M

40678

Disk IO: 16009 (86% async)
Page faults: 73

small last (1) 10M

40213

Disk IO: 15960 (87% async)
Page faults: 20

100M

503547

Disk IO: 158721 (92%
async)
Page faults: 164

small last (1) 100M

484450

Disk IO: 152669 (91%
async)
Page faults: 66

small first
(2)

Figure 16. Performance statistics from doing
index_test_table_1 and index_test_table_2.

inserts

to

Since what could theoretically improve performance is the reduced size of the
index for table 2, the select benchmarks were only done with 100M rows in each
table. As expected, querying the smaller index was more efficient, as can be seen
in figure 17. The elapsed time is proportional to the size of the index, but the
number of disk operations is over 7 times less for the smaller index. The smaller
index probably fit better in the disk cache, while the larger didn't. The conclusion
is that ordering index columns with the least variable column first can save both
space and time for very large tables, but for smaller tables it may not be worth it,
which is possibly why IBM recommends to place the most selective column first.
Elapsed time (ms)

Disk stats

small first

24926.0

Disk IO: 6179 (0% async)
Page faults: 6173

small last

36254.0

Disk IO: 45574 (0% async)
Page faults: 45568

Figure 17. Performance statistics from doing 100K selects from
index_test_table_1 and index_test_table_2, each having
10M rows.
The skip scan test did not finish in under 30 minutes even when reducing the table
size to 10M rows. Index Advisor (figure 18) confirmed that a full table scan was
used for the SQL statement in the for loop. DB2 clearly does not support the index
skip scan feature.

44

�'%50 �JQ>��,� 8���6'1-0�-52.52�!-0�2& �����12�2 + ,2�', *'13,%�JJ>

5/2 �!%�$&%
�� /# ,17#6:217 ,1 �� �6# "21# A #,8+#6 74#!,$A,1* >�/=#7 ",6#!8/A 26 A =7,1*
� 7#/#!8 78�8#0#18F �+# values .#A?26" ,7 =7#" 82 ,17#68 >�/=#7 ",6#!8/AF �+#
4#6$260�1!# 2$ ,17#6:21 A 7#/#!:21 1�8=6�//A "#4#1"7 21 8+# 4#6$260�1!# 2$
8+# 7#/#!8 78�8#0#18 =7#"D ?+,/# ,17#6:21 A >�/=#7 +�7 !2178�18 4#6$260�1!#F
� !2046#+#17,># �1�/A7,7 2$ ��T ,17#68 4#6$260�1!# ?�7 "21# A �,/.,17 ,1
TRRVMT[NF

5/2/1

�!��,�%

�20021 $26 28+ ,17#6:21 0#8+2"7 ,7 8+�8 8+# �02=18 2$ ,1"#@#7 �%#!8 8+#
4#6$260�1!#D #!�=7# 8+# ,17#68 �""7 � 62? 82 #�!+ ,1"#@ �7 ?#//F �,/.,1P7
4#6$260�1!# 6#7=/87 ?+#1 "2,1* SRR� ,17#687 ,1 �8!+#7 2$ SRRR ?,8+ ",%#6#18
1=0 #6 2$ ,1"#@#7 +�># ##1 !204,/#" ,1 &*=6# S[F �+# 8� /# 7+2?7 8+�8 ,1 TRRV
8+# ,17#6:21 4#6$260�1!# 7!�/#" /,1#�6/A ?,8+ 8+# 1=0 #6 2$ ,1"#@#7D �1" 8+�8
!6#�:1*�,1"#@#7��)#6�,17#6:1*�,7�0=!+�$�78#6F

45

�������
!��

����&��

�������

�-�',� 8 1

LJ>NJ�1

J

�, �',� 8

NL>PL�1

J>PJ�?�K�T
I>QO

�7-�',� 8 1

QL>KO�1

K>OM�?�L�T
I>QQ

�&0 �',� 8 1

JIQ>KJ�1

L>ML?�M�T
I>QNPN

�-50�',� 8 1

JMJ>OL�1

M>MR�?�N�T
I>RI

',1 021��-, �7'2&-52�',� 8<�2& ,�M�',� 8 1
�0 �2 ���$ 07�0�1

LR>MM�1

J>KN�?�N�T
I>KN

�'%50 JR> �'*)',G1 0 15*21 -, ',� 8 1 7& , ',1 03,% JII� 0-71 ', ��2�& 1
-! JIII> �-0 2& 1��*',% !��2-0< 7 �'6'� � 2& *�.1 � 3+ 7'2& 2&
*�.1 ��3+ �!-0�2& �',� 8@* 11���1 >
�+#26#:!�//A ��T !2=/" =4"�8# 0=/:4/# ,1"#@#7 ,1 4�6�//#/ ?+#1 "2,1* ,17#687F
�/8+2=*+ ,8 ?�7 12 8+# !�7# ,1 TRRVD 82 >#6,$A ,$ 4�6�//#/,B�:21 2!!=67 82"�AD 8+6##
0#8+2"7 2$ "2,1* SRR� ,17#6:217 ?#6# 8#78#" ?,8+ ",%#6#18 �02=187 2$ ,1"#@#7F
�+# &678 0#8+2" ,7 � 7,1*/# �� ,17#68 K/,7:1* SULD 8+# 7#!21" �1 �66�A ,17#68 ?,8+
7,B# SRRR K/,7:1* SVLD D ,F#F �,/.,1P7 7#8=4D �1" 8+# 8+,6" ,7 ,17#6:21 $620 � 7,1*/#
��� 6#!=67,># ?,8+ 78�8#0#18 K/,7:1* SWLD �+# 8� /# =7#" ,7 7+2?1 ,1 /,7:1* ST �1"
8+#�,1"#@�!6#�:21�462*6�0�,7�,1 /,7:1*�TT ,1�8+#��44#1",@F
create table table_for_index_count_tests
for system name MLUTINS01T (
column1 bigint not null,
column2 bigint not null,
column3 bigint not null,
column4 bigint not null
);
�'13,% JK> �& 2��* 51 � !-0 2 13,% &-7 2& ,5+� 0 -! ',� 8 1 �" �2
',1 02�. 0!-0+�,� >

46

dcl-proc doInserts;
dcl-pi *n;
dataset varchar(50) const;
warmup ind const;
dataSize int(10) const;
end-pi;
dcl-s i int(10);
for i = 1 to dataSize;
exec sql
insert into table_for_index_count_tests
values ( :i, :i, :i, :i);
assertSqlSuccess(sqlCA);
endfor;
end-proc;

Listing 13. Inserting one row at a time.

47

dcl-proc doInserts;
dcl-pi *n;
dataset varchar(50) const;
warmup ind const;
dataSize int(10) const;
end-pi;
dcl-s i int(10);
dcl-s j int(10);
dcl-ds array dim(1000) qualified;
column1 int(20);
column2 int(20);
column3 int(20);
column4 int(20);
end-ds;
for i = 1 by 1000 to dataSize;
for j = 1 to 1000;
array(j).column1 = i + j;
array(j).column2 = i + j;
array(j).column3 = i + j;
array(j).column4 = i + j;
endfor;
exec sql
insert into table_for_index_count_tests
1000 rows values (:array);
assertSqlSuccess(sqlCA);
endfor;
end-proc;

Listing 14. Inserting 1000 rows at a time using an array.

48

dcl-proc doInserts;
dcl-pi *n;
dataset varchar(50) const;
warmup ind const;
dataSize int(10) const;
end-pi;
exec sql
insert into table_for_index_count_tests
with counter (i) as (
select 1 from sysibm.sysdummy1
union all
select i+1 from counter where i < :dataSize
)
select i, i, i, i
from counter;
assertSqlSuccess(sqlCA);

Listing 15. Inserting with SQL.
The results for the three methods can be seen in figure 20, figure 21 and figure 22.

49

Index
count

Scaling
ratio

Elapsed time
(ms)

Disk stats

0

1.0

34291.0

Disk IO: 8340 (95% async)
Page faults: 2

1

0.54

37155.0

Disk IO: 8758 (91% async)
Page faults: 3

3

0.31

42735.0

Disk IO: 9365 (86% async)
Page faults: 1

5

0.23

48085.0

Disk IO: 9974 (80% async)
Page faults: 1

10

0.16

60984.0

Disk IO: 11509 (69%
async)
Page faults: 20

94366.0

Disk IO: 14568 (55%
async)
Page faults: 42

20

0.13

Figure 20. Performance statistics from doing 100K insertions one row at a
time using RPG.
For the simple RPG single insert, the scaling ratio decreases as the number of
indexes is increased. This means that the effect of having many indexes does
not affect the performance of inserts linearly. Having 20 indexes instead of none
only increases the elapsed time by a factor of three. The high percentage of
asynchronous IO when having few indexes imply that the indexes are updated in
the background in between the insert calls. One hypothesis to explain the results,
is that when the numbers of indexes increase, the background processes cannot
keep up and it starts spending time waiting for the background operations to
complete before continuing.

50

Index count Scaling ratio Elapsed time (ms)

Disk stats

0

1.0

486.0

Disk IO: 755 (60% async)
Page faults: 1

1

2.97

2896.0

Disk IO: 1338 (52% async)
Page faults: 0

3

2.29

4463.0

Disk IO: 1956 (35% async)
Page faults: 2

5

2.89

8440.0

Disk IO: 2507 (23% async)
Page faults: 6

10

2.2

11809.0

Disk IO: 4533 (24% async)
Page faults: 30

20

2.46

25187.0

Disk IO: 7585 (14% async)
Page faults: 44

Figure 21. Performance statistics from doing 100K insertions 1000 rows at
a time using RPG.
The difference between having no indexes and one index is especially big when
inserting in batches of 1000, which is what Wilkins did in figure 19. The scaling
factors are roughly three times larger today than they were 2004, but they are still
constant, with some variations. The improvement in hardware and DB2 since 2004
is apparent with the 60-fold improvement for the test run without indexes.
Index
count

Scaling
ratio

Elapsed time
(ms)

Disk stats

0

1.0

1177.0

Disk IO: 651 (56% async)
Page faults: 5

1

1.24

2919.0

Disk IO: 1711 (66% async)
Page faults: 1

3

1.15

5446.0

Disk IO: 3347 (63% async)
Page faults: 2

5

1.07

7586.0

Disk IO: 4361 (58% async)
Page faults: 18

10

1.01

13120.0

Disk IO: 6887 (50% async)
Page faults: 38

20

0.99

24702.0

Disk IO: 10092 (34%
async)
Page faults: 159

Figure 22. Performance statistics from doing 100K insertions all at once
using SQL.

51

The SQL variant shows improving performance per index as the number of indexes
grow, converging to linear scaling somewhere between 10 and 20 indexes.
If the main method of inserting rows into a table is one at a time using RPG, a
moderate number of indexes have little effect on performance. For inserting in
large batches, however, it may be better to create the indexes after inserting rows.
The indexes and data used in this test were not realistic. All rows in the table had
unique values for every column, leaving no space optimization possibilities for the
radix tree indexes. The hardware used here was limited to a maximum of two CPU
cores, limiting parallelizing compared to more powerful systems.

5.2.2

Batch Size

Wilkins concluded that the batch size of an insert significantly affects
performance, and IBM officially recommends to insert many rows at once[30]. His
results from doing 10K inserts in different batch sizes are compiled in figure 23.
Increasing the batch size from 10 to 1000 increased the speed almost five-fold.
This shows that performance scales positively in roughly log₁₀(n), where n is the
batch size in the range 10 to 1000.
batch size

Elapsed time

10

118.18 s

100

50.65

1000

23.43 s

Figure 23. Wilkin's results on insert performance depending on insert
batch size when doing 10K insertions.
However, I suspect that there is a typo or mistake in Wilkin's report, because batch
size one was missing in his comparative table, while the elapsed time for size 10 is
larger than his numbers for single row inserts.

5.3 Updates
The performance of updates and deletes is usually in line with selects, since they
generally do the same things:
1. Identify the row(s) matching the search criteria.
2. Update or delete the matching rows.
3. Update indexes.
However, when you change a column that your search criteria is based on, an

52

update-statement can behave radically different from its equivalent selectstatement variant. The difference in computing time between a select and an
update in such a case can be very big. Personally I have experienced a select that
executed in one minute (50 million table entries, advanced criteria). Using the
same criteria to update was impossible, it would have taken approximately a week
for it to finish. While that exact SQL cannot be used for legal reasons, an equivalent
situation will be analyzed.
The particular case that will be tested here is legacy log conversion. Back in the
80s and 90s, creating tables without row IDs was common, and the log files
naturally had no such IDs either. When viewing a log, the user needed to guess
which entries belonged to the same entity. At one point or another it becomes
necessary to create IDs for these tables. Adding a new column to the active table
and populating it with an ID is easy. Doing the same for the log file is difficult.
Let us assume that we have a table accounts containing account IDs, and
a second table account_attributes which contains additional business
attributes connected to specific accounts. The attributes consists of an
attribute_type field, which is a unique ID for the account, and an
attribute_data field that can contain anything depending on
attribute_type (such as references to other accounts, users or dates). In
practice one would probably have many data fields, but for simplicity's sake we'll
stick to just one here.
Field
account_id

Description
foreign key to the accounts table

attribute_type code specifying the type of the attribute.
attribute_data Various data specific to the attribute type.
new_id

The new account attribute ID that was added in
retrospect.

Figure 24. The account_attributes table.

53

Field
account_id

Description
foreign key to the accounts table

attribute_type code specifying the type of the attribute.
attribute_data Various data specific to the attribute type.
log_action

What this log entry is about: 'CREATED' ,
'UPDATED' or 'DELETED' .

log_timestamp

When the change was made.

new_id

The new account attribute ID that was added in
retrospect.

Figure 25. The account_attributes_log table. It has the same fields
as account_attributes with additional fields that tells us whether the
account attribute was created, updated or deleted and when it happened.
Let us establish some assumptions about the tables to reduce the scope of the ID
creation process:
1. No universal unique key exists for account_attributes .
2. An account does not have and has never had more than one of each
attribute type at the same time.
3. The logged data is complete. Every 'CREATED' and 'UPDATED' entry
has either a coupled 'DELETED' or an active entry in
account_attributes .
An algorithm to create IDs retroactively for both tables is as follows.
1. Create an SQL sequence that will generate the IDs.
2. Assign IDs from the sequence to the active entries in
account_attributes .
3. Assign IDs from the sequence to the 'DELETED' entries in
account_attributes_log .
4. For each 'CREATED' and 'UPDATED' log entry A, find the oldest
'DELETED' entry that happened after the entry. If such an entry is
found, save its ID to the entry A. If a 'DELETED' entry is not found, find
a corresponding active entry and use its ID for A.
This algorithm can be implemented in many ways. The performance of three
different implementations will be compared. A shared implementation of steps
one to three is used, as the steps are very straightforward. The shared
implementation will also generate the data using a deterministic random number

54

*#1#6�826D�82�#17=6#�8+�8�,"#1:!�/�"�8��,7�*#1#6�8#"�#�!+�:0#F

5/3/1

�"

"!��$"�$� %��!�������%

�+# ��� 78�8#0#187 $26 !6#�:1* 8+# "#7!6, #" "�8� �7# �6# /,78#" ,1 /,7:1* SX
#/2?F
create or replace table account_attributes
for system name MLUTACCT (
account_id
bigint
not null,
attribute_type int
not null,
attribute_data varchar(50) not null default '',
new_id
bigint
not null default 0,
primary key (account_id, attribute_type)
);
create or replace table account_attributes_log
for system name MLUTLOGT (
account_id
bigint
not null,
attribute_type int
not null,
attribute_data varchar(50) not null default,
log_action
char(7)
not null,
log_timestamp timestamp
not null default,
new_id
bigint
not null default
);
create index mlutlogi on mlutlogt
(log_action, attribute_type, account_id);
�'13,%�JO>��& �27-�� 1�0'� ��2��* 1��,��0 /5'0 ��',� 8�B15%% 12 ���9���C>
� 462*6�0 ?�7 !6#�8#" 82 "A1�0,!�//A !6#�8# 8+# 8#78 "�8� ,1 � "#8#60,1,7:!
$�7+,21F �,678 N �!:># #186,#7 �6# ,17#68#" ,182 account_attributesF ��!+
�!!2=18 ,7 �77,*1#" 8+6## �;6, =8#7F �26 �// �!:># #186,#7D � CREATED #186A ,7
,17#68#" ,182 account_attributes_log :0#78�04#" TRSRF �26 8+# &678
+�/$ 2$ 8+# �!:># #186,#7D �1 UPDATED #186A ,7 �""#" �7 ?#//F �26 8+# 7#!21" +�/$D
CREATED �1" DELETED #186,#7 �6# ,17#68#" ,1 TRRR �1" TRR[F �+# :0#/,1# ,7
,//=786�8#"�,1 &*=6#�TX �1"�8+#�$=//�462*6�0�,7�7+2?1�,1 /,7:1*�TS ,1�8+#��44#1",@F

55

2000

2009

First half
Second half

CREATED

DELETED

2010

2015

Now

CREATED

UPDATED

Active

CREATED

UPDATED

Active

Figure 26. Timeline of the test data.
A second program (listing 17) handles the common steps one to three of the
algorithm.

56

**FREE
ctl-opt
option(*srcstmt:*nodebugio:*nounref)
datfmt(*iso) timfmt(*iso)
dftactgrp(*no) actgrp(*caller)
bnddir('CEXCS00104')
main(generateTestData);
/copy QPROTSRC,CEXCS00104
//-Sql options-----------------------------------exec sql set option
commit = *none, closqlcsr = *endmod,
datfmt = *iso, timfmt = *iso,
dlyprp = *yes;
//-----------------------------------------------dcl-proc assignIDs;
dcl-pi *n;
dataset varchar(50) const;
warmup ind const;
dataSize int(10) const;
end-pi;
// 1. create a temporary sequence
// to generate the new IDs
exec sql
drop sequence qtemp.mlutmpseq;
assertSqlSuccess(sqLCA: SEQ_NOT_FOUND);
exec sql
create sequence qtemp.mlutmpseq as bigint
start with 1 increment by 1
cache 100;
assertSqlSuccess(sqLCA);

57

// 2. assign IDs to all active entries
exec sql
update account_attributes
set new_id = next value for qtemp.mlutmpseq;
assertSqlSuccess(sqLCA);
// 3. assign IDs to deleted entries
exec sql
update account_attributes_log
set new_id = next value for qtemp.mlutmpseq
where log_action = 'DELETED';
assertSqlSuccess(sqLCA);
end-proc;

Listing 17. A program implementing steps one to three of the algorithm.

5.3.2

Loop

In listing 18 the simplest version of step four of the algorithm has been
implemented. The CREATED and DELETED entries are looped through one at a
time, and for each entry we try to find a matching deleted entry in the log. If no
entry exists, the new id is fetched from the active table instead. Then the current
row is updated with the new id. This implementaiton is expected to perform
poorly due to the overhead involved in making many separate SQL calls, and the
inability for the DB engine to parallelize the work.

58

dcl-proc loop;
dcl-s
dcl-s
dcl-s
dcl-s

accountId int(20);
attributeType int(10);
ts timestamp;
newAttributeId int(20);

exec sql
declare loopCursor cursor for
select
account_id, attribute_type, log_timestamp
from account_attributes_log
where log_action in ('CREATED', 'UPDATED')
for update;
exec sql
open loopCursor;
assertSqlSuccess(sqlCA);
dow 1=1;
exec sql
fetch next from loopCursor
into :accountId, :attributeType, :ts;
if sqlCode = 100;
leave; // break loop
endif;
assertSqlSuccess(sqlCA);
// find the oldest DELETED entry newer
// than the current entry
exec sql
select new_id into :newAttributeid
from account_attributes_log
where account_id = :accountId
and attribute_type = :attributeType
and log_action = 'DELETED'

59

and log_timestamp > :ts
order by log_timestamp asc
fetch first 1 rows only;
if sqlCode = 100;
// use active entry if deleted not found
exec sql
select new_id into :newAttributeid
from account_attributes
where account_id = :accountId
and attribute_type = :attributeType;
assertSqlSuccess(sqlCA);
else;
assertSqlSuccess(sqlCA);
endif;
exec sql
update account_attributes_log
set new_id = :newAttributeId
where current of loopCursor;
assertSqlSuccess(sqlCA);
enddo;
exec sql
close loopCursor;
end-proc;

Listing 18. A naive implementation of step four of the algorithm.

5.3.3

Batch Updating

In listing 19 the same results are accomplished by only two SQL update
statements. The first sets the new IDs for all CREATED and DELETED entries for
which no corresponding DELETED entry is found in the log. The second sets the
IDs for the rest. Almost no SQL preparation overhead should occur since only two
statements are executed. However, it should not perform as good as the third test
due to it updating the same data that is queries in the implicit SQL update loop.

60

dcl-proc loop;
// add IDS for active
exec sql
update account_attributes_log upd
set new_id = (
select new_id
from account_attributes active
where upd.account_id = active.account_id
and upd.attribute_type =
active.attribute_type
)
where log_action in ('CREATED', 'UPDATED')
and not exists (
select 1
from account_attributes_log deleted
where deleted.log_action = 'DELETED'
and upd.account_id = deleted.account_id
and upd.attribute_type =
deleted.attribute_type
and deleted.log_timestamp >=
upd.log_timestamp
);
assertSqlSuccess(sqlCA);
// add IDS for the rest
exec sql
update account_attributes_log upd
set new_id = (
select new_id
from account_attributes_log deleted
where deleted.log_action = 'DELETED'
and upd.account_id = deleted.account_id
and upd.attribute_type =
deleted.attribute_type
and deleted.log_timestamp >=
upd.log_timestamp

61

order by deleted.log_timestamp asc
fetch first 1 rows only
)
where new_id = 0;
assertSqlSuccess(sqlCA);
end-proc;

Listing 19. Batch updating.

5.3.4

Batch Updating With Temporary Table

In listing 20 update statements equivalent to those in listing 19 are done, but a
temporary table is used instead of fetching the DELETED entries from the log
table. The total amount of DB writes will be larger in this case since we insert
rows into the temporary table and create an index on it, but the fact that we
afterwards can avoid updating the same table we are querying should cover the
loss in execution time.

62

dcl-proc batchWithTempTable;
// drop index if it exists
// this improves insert performance
exec sql
drop index deleted_entries_index;
assertSqlSuccess(sqlCA: INDEX_NOT_FOUND);
exec sql
insert into account_attributes_log_deleted
select account_id, attribute_type,
log_timestamp, new_id
from account_attributes_log
where log_action = 'DELETED';
assertSqlSuccess(sqlCA);
// recreate the index
exec sql
create index deleted_entries_index
on account_attributes_log_deleted
(attribute_type, account_id,
log_timestamp, new_id);
assertSqlSuccess(sqlCA);
// add IDS for active
exec sql
update account_attributes_log upd
set new_id = (
select new_id
from account_attributes active
where upd.account_id = active.account_id
and upd.attribute_type =
active.attribute_type
)
where log_action in ('CREATED', 'UPDATED')
and not exists (
select 1

63

from account_attributes_log_deleted deleted
where upd.account_id = deleted.account_id
and upd.attribute_type =
deleted.attribute_type
and deleted.log_timestamp >=
upd.log_timestamp
);
assertSqlSuccess(sqlCA);
// add IDS for the rest
exec sql
update account_attributes_log upd
set new_id = (
select new_id
from account_attributes_log_deleted deleted
where upd.account_id = deleted.account_id
and upd.attribute_type =
deleted.attribute_type
and deleted.log_timestamp >=
upd.log_timestamp
order by deleted.log_timestamp asc
fetch first 1 rows only
)
where new_id = 0;
assertSqlSuccess(sqlCA);
end-proc;
Listing 20. Batch updating using a temporary table for the DELETED
entries.

5.3.5

Results

The results for data sizes between 10K and 20M can be seen in figure 27. As
expected, the loop performed poorly compared to the two batch implementations
because of the overhead from making one to two SQL calls in each loop iteration.
The two batch implementations performed similarly, despite the second spending
time on filling a temporary table with rows. Up to 1M rows, almost no page

64

faults occurred, which indicates that the affected data set fit in disk caches and
RAM. At 10M and 20M, the number of page faults increased dramatically, but
it increased more for the simple batch solution, despite the other one filling a
table and creating an index on it. When a row is updated, the cached disk or
memory page it resides on will be invalidated and must be read again. If DB2 used
multiple threads to process the data, they may have interfered with each other by
invaliding the pages that the other thread was reading from. In the first batch run,
the percentage of asynchronous IO decreased from 97% with 1M rows to 64%,
while the batch run with the temporary table only decreased from 87% to 83%,
thus showing better scalability in this regard.
A few possible explanations for the smaller-than-expected differences between
the two batch methods have been identified. The ratio of asynchronous IO is
higher when using the temporary table, but the maximum number of CPU cores
available on the development environment is two, while in the real life scenario,
that the test was based on, the maximum number of CPUs was ten. The data
was inserted in a sequential fashion, which likely reduces the amount of page
faults due to less jumping between disk pages to retrieve similar entries. Another
difference is that the test table had very few non-key columns, while the
production table had many. This means that less irrelevant data filled up the disk
pages in this test, and consequently the number of page faults was relatively low.
An advantage of using a temporary table is that the table could be created
beforehand, thus reducing the time and resources required in the actual ID
conversion. If the conversion had to be done within a specific time window, the
temporary table could be created the previous day, and only be appended with
newly deleted attributes to make it complete.

65

Iterations

Elapsed time
(ms)

Disk stats

loop

10K

2268.0

Disk IO: 38 (97% async)
Page faults: 0

batch

10K

799.0

Disk IO: 100 (35% async)
Page faults: 0

batch with temporary
table

10K

1308.0

Disk IO: 485 (27% async)
Page faults: 67

loop

100K

24242.0

Disk IO: 132 (98% async)
Page faults: 1

batch

100K

4579.0

Disk IO: 170 (95% async)
Page faults: 0

batch with temporary
table

100K

4673.0

Disk IO: 993 (61% async)
Page faults: 57

loop

1M

237047.0

Disk IO: 814 (69% async)
Page faults: 241

batch

1M

45018.0

Disk IO: 987 (97% async)
Page faults: 14

batch with temporary
table

1M

49672.0

Disk IO: 6501 (87%
async)
Page faults: 17

batch

10M

491261.0

Disk IO: 47676 (82%
async)
Page faults: 8525

batch with temporary
table

10M

465204.0

Disk IO: 63209 (84%
async)
Page faults: 6508

batch

20M

949243.0

Disk IO: 72938 (64%
async)
Page faults: 26143

batch with temporary
table

20M

911013.0

Disk IO: 135893 (83%
async)
Page faults: 16740

Figure 27. Performance statistics from doing 100K insertions all at once
using SQL.

66

6 Discussion
The initial goal of producing a benchmarking framework that could test all kinds
of situations was not fully met, but using it for throughput testing with different
implementations and data sizes turned out to be very easy. Splitting data
generation and the actual benchmark into two different test units allowed for
flexible tests that were quick and easy to configure. Many alternate program
layouts were tried before reaching the final one, but they were insufficient in
various ways. Configuring RPG or DB2 parameters was too inconvenient in some,
and others became increasingly complex when the test had to do warmup and
have data sizes on multiple levels (for example to insert 1M into the table, and
then benchmark doing 10K selects).
Overall the benchmarks produced expected results, although the generic
benchmarking explored in chapter 4 time and again produced unexpected or
no results, which made the research process very slow. Especially the QAQQINI
options related to caching did not behave as expected, and more analysis is
needed in order to understand them. Due to time and scope constraints only a few
aspects of them were explored in this work, and the unclear results affected what
kind of tests could be done in chapter 5. Worst case latency tests were skipped.
I believe that my observations and theory are correct, but that the options only
show results in specific situations.
Being limited to a relatively weak virtual machine with a maximum of two
available CPU cores could have limited the parallelism degree of the tested
queries. To reinforce the results, the same benchmarks should also be run on a
more powerful machine. An improvement to all test setups would be to insert
data in a more randomized fashion, to avoid ideal situations regarding indexing
and disk operation performance, which could reduce the correctness of the tests,
compared to similar production situations.

67

Swedish Summary
För utvecklare som arbetar med databaser är prestanda en viktig komponent,
men det är svårt att veta exakt hur man ska strukturera sin databas och göra
förfrågningar mot den för att uppnå tillräcklig prestanda. Förändringar inom både
hårdvara och mjukvara under de senaste 20 åren har gjort att många tidigare
rekommendationer har upphört att gälla. Trots detta jobbar många fortfarande
enligt gamla tankesätt. Syftet med detta arbete är att skapa ett ramverk som
underlättar skapandet av prestandatester, och att jämföra prestandan mellan olika
lösningar för några problem. Databasen som undersöks är DB2 for IBM i (version
V7R4), anropad från program skrivna i programmeringsspråket RPG.
Databasprodukten DB2 for IBM i är hårt bunden till operativsystemet IBM i, som
har inbyggd funktionalitet för databasfiloperationer på en mer konkret nivå än
SQL erbjuder. Till skillnad från vanliga operativsystem har IBM i inte vanliga binära
filer, utan filsystemet (HFS) består av bibliotek som innehåller objekt av olika
typer. Objekttypen *FILE har två varianter: fysiska och logiska filer. Fysiska filer
används för lagring av data i tabellformat. Ett dataformat definieras som beskriver
hur kolumnerna i tabellen ser ut. Logiska filer är transformationer eller index
av fysiska filer. DB2 är ett lager ovanpå dessa filer, och mycket av
objektfunktionaliteten är idag endast tillgänglig med hjälp av SQL.
Tabeller i DB2[12] är enkla listor där insättningar alltid görs sist. Indextypen som
huvudsakligen används är binära radixträd, som i teorin skalar i förhållande till
antalet bitar i de indexerade kolumnerna för alla operationer (uppslag, insättning,
uppdatering och borttagning), d.v.s. är konstant. DB2 för statistik över hur tabeller
och index används och utnyttjar den för att föreslå och automatiskt skapa
temporära index om så krävs. För att undersöka hur en viss sql-sats kommer att
exekveras kan man använda verktyget Index Advisor, som grafiskt visar vilka index
som används eller vilka index vars skapande skulle snabba upp exekveringen. Man
kommer även åt statistiken och föreslagna index med SQL-procedurer[23]. Delar
av databasens beteende, som användningen av cachning och resursbegränsningar,
kan styras med parametrar i ett specialregister kallat QAQQINI[17].
Prestandamätning är ett komplext ämne som det har skrivits flera böcker om
[21][22]. Prestandamätningar kan klassiciferas som bl.a. responstids- och
genomströmningstester. Som en del av detta arbete utvecklades ett ramverk i RPG
för att mäta prestanda på olika sätt genom parametersättning av funktionalitet i
RPG-program och SQL. Ramverket loggar tids- och resursåtgång för varje test som
körs. Resurserna som loggas är bl.a. processortid, sidfel och antal diskoperationer
(totalt, synkrona och asynkrona). Tekniskt enkla tester utfördes med olika
parameteruppsättningar, och medan vissa hade en tydlig inverkan på

68

exekveringstiderna, hade QAQQINI-parametrarna för att stänga av cachning ingen
mätbar inverkan. Orsaken till det är oklar, vilket gjorde att det inte gick att tvinga
testerna att ge samma resultat varje gång. Ju fler gånger ett test kördes, desto
snabbare blev det upp till en viss gräns. För mindre tabeller kunde mängden
diskoperationer gå ner mot noll efter några körningar, då hela tabellen fanns
i RAM-minnet. Två sätt att undvika cachningen är att använda mycket stora
datamängder eller att skapa nya data inför varje test.
IBM har en del allmänna rekommendationer för hur en utvecklare kan
programmera för att förbättra prestanda[25]. En intressant rekommendation är
att index ska skapas med den mest selektiva kolumnen först, d.v.s. kolumnen
med flest olika värden[27]. Index Advisor rekommenderar ofta motsatsen, och
ett binärt radixträd kan lagra data effektivare om de lagrade strängarna har färre
gemensamma prefix. Ett test skapades med två identiska tabeller, där en kolumn
hade unika värden, medan den andra endast hade fem olika värden. De fick varsitt
index i båda kolumnerna. Den första tabellen hade den mest selektiva kolumnen
först och den andra tabellen hade den sist i indexet. Sen gjordes först 10 M och
sen 100 M insättningar i tabellerna. Storleken på indexen vid 10 M var endast
10 MB (ca fem % av totala storleken), men när tabellerna fick 100 M poster
blev skillnaden 944 MB, vilket resulterade i att indexet med den mest selektiva
kolumnen först var 49 % större än det andra. Dock tog det lite längre att göra
insättningarna i tabellen med det mindre indexet. 100 K uppslag gjordes mot båda
tabellerna då de innehöll 100 M poster, och testet som gick mot tabellen med
det större indexet tog 50 % längre tid och krävde ca sju ggr mer diskoperationer.
Slutsatsen är att man ska sätta den minst selektiva kolumnen först om man har
stora mängder data och gör mer uppslag än insättningar.
I en artikel från 2004[29] redogjorde Wilkins omfattande för prestanda på
insättningar. Han konstaterade att insättningar blir linjärt långsammare i
förhållande till antal index på tabeller, då man gör insättningar i klumpar på 1000.
Med tanke på framstegen inom parallellisering sedan 2004 kunde man tänka
sig att prestandan inte längre skulle vara linjär. Teoretiskt sett borde DB2 kunna
uppdatera flera index samtidigt om flera processortrådar är tillgängliga. Tre tester
som gjorde insättningar på olika sätt kördes för olika antal index (noll, ett, tre,
fem, tio, tjugo) Det första testet satte in en rad i taget från RPG-variabler, det
andra satte in 1 000 i taget från en RPG-räcka och det tredje satte in alla på en
gång genom att generera data med hjälp av SQL. Både insättning genom räcka
och SQL visade att prestandan fortfarande försämras linjärt i de fallen, men testet
som endast satte in en post i taget gav ett annorlunda resultat. Att göra 100K
insättningar utan index tog 34 s, medan det med 20 index tog 94 s trots tjugofaldig
arbetsmängd. Det hade högre andel asynkrona diskoperationer, vilket tyder på att
mer kunde göras parallellt. Dock bör det påpekas att den totala exekveringstiden
var 3–70 gånger högre än för de två andra testfallen oavsett antal index. I praktiken

69

har man ofta endast en rad tillgänglig då man ska sätta in en post i en tabell, och
ökningen av exekveringstiden då man ökade från 10 till 20 index var bara 50% mer
i snitt, vilket visar att prestandan inte drabbas linjärt då antalet index ökar, förutom
vid stora klumpoperationer.
Uppdateringar av poster i tabeller har i regel samma prestandaegenskaper som
att göra uppslag, med en liten omkostnad för indexuppdateringar och skrivning
till disk. Om man däremot gör ändringar i klump och urvalskriteriet finns på
samma tabell som man ändrar på kan det orsaka prestandaproblem då det gör
minnessidor ogiltiga, vilket gör att parallella operationer störs av varandra. Ett
fall där detta kan ske är tillägg av id-kolumn i gamla entitetshistoriktabeller som
saknar det. En sån tabell innehåller data som entiteten haft under sin livscykel,
tidsstämpel och typ av ändring (skapande, uppdatering, borttagning). En algoritm
utformades för att skapa IDn åt historikposterna. Först fick alla aktiva och
borttagna entiteter ett genererat ID från ett sekvensobjekt. För varje skapningsoch ändringspost i historiktabellen kontrollerades först om det fanns en
motsvarande borttagen post i historiktabellen och ID:t togs i så fall från den,
annars togs ID:t från motsvarande post i den aktiva tabellen. Tre olika versioner
gjordes av det sista steget i algoritmen. Den första implementationen var en RPGloop som gjorde ett SQL-anrop för varje kontroll. Den andra implementationen
åstadkom samma resultat med endast två update-satser, medan den tredje först
skapade en tillfällig tabell för de borttagna historikposterna, och gick mot den
istället för historiktabellen vid de två update-satserna.
Som förväntat presterade den loopbaserade implementationen mycket dåligt
jämfört med klumpuppdateringarna, oavsett om man ser på tids- och
resursåtgång. Vid radantal på 1 M presterade klumpvarianten utan temporär
tabell lite bättre tidsmässigt, samtidigt som mängden diskoperationer var 6,6
ggr högre. Vid radantalen 10 M och 20 M blev mängden sidfel väsentligt, vilket
betyder att all data inte längre rymdes i närminnet, och disken blev tvungen
att jobba mera. Lösningen med temporär tabell exekverades fem % snabbare,
samtidigt som antalet sidfel var betydligt lägre, och mängden parallella
diskoperationer högre. Det totala antalet diskoperationer var förstås högre p.g.a.
skapandet av den temporära tabellen. Större skillnad i tidsåtgång förväntades
mellan lösningarna, men det finns många faktorer som kan ha påverkat. Sättet
som insättningarna gjordes på var helt sekventiellt, vilket kan ha reducerat antalet
sidfel. Maskinen som testet kördes på hade endast två processorkärnor, vilket
kan ha reducerat mängden parallellisering som gjordes. En annan fördel med
att använda en temporär tabell är att den kan fyllas i på förhand, ifall
konverteringsmomentet behöver göras under ett begränsat underhållstidsfönster.
Överlag funkade ramverket för prestandamätning bra. Testsviterna var modulära
och det var enkelt att kombinera olika storlekar på insättningar och uppslag. Tyvärr

70

betedde sig inte QAQQINI-parametrarna som IBM:s dokumentation indikerade att
de skulle göra, utan mer forskning krävs för att man parametrarna ska kunna
användas.

71

References
[1] M. Anderson, R. Gagliardi, H. May, G. McCright, S. Tlusty, W. Varela.
Performance Value of Solid State Drives using IBM i. URL:
https://www.ibm.com/developerworks/community/wikis/
form/anonymous/api/wiki/
beb2d3aa-565f-41f2-b8ed-55a791b93f4f/page/
a7c972c1-bc9a-430e-ab01-68823d7a5a6e/attachment/
3b56d348-5c4b-4643-843f-b27cf199929d/media/SSD_IBMi.pdf
(2020-05-10).
[2] The revolutionary IBM POWER9 processor chip. URL:
https://www.ibm.com/it-infrastructure/power/power9
(2020-02-22).
[3] Objects and libraries. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 74/rbam6/objec.htm (2020-05-10).
[4] Physical Files and Logical Files. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 72/rzasc/hpflf.htm (2020-05-10).
[5] Programming ILE Concepts Version 7.4. URL: https://www.ibm.com/
support/knowledgecenter/ssw_ibm_i_ 74/ilec/
sc415606.pdf?view=kc (2020-05-10).
[6] Jobs. URL: https://www.ibm.com/support/knowledgecenter/
ssw_ibm_i_ 74/rzal2/rzal2jobs.htm (2020-05-10).
[7] Jobs and job resources. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 72/rzahw/rzahwourco.htm
(2020-05-10).
[8] HISTORY OF THE IBM RPG PROGRAMMING LANGUAGE. URL:
https://www.nicklitten.com/history-of-the-ibm-rpgprogramming-language/ (2020-05-10).
[9] Fully free-form RPG - new in 7.1 and 7.2. URL: https://www.ibm.com/
support/pages/fully-free-form-rpg-new-71-and-72
(2020-02-23).
[10] Introduction to ILE RPG Activation Groups. URL:
https://www.mcpressonline.com/programming/rpg/
introduction-to-ile-rpg-activation-groups (2020-05-10).
[11] Determining equivalent SQL and ILE RPG data types. URL:
https://www.ibm.com/support/knowledgecenter/ssw_ibm_i_
73/rzajp/rzajpequivsqlirpg.htm (2020-04-12).
[12] Sql Programming. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 74/db2/rbafzprintthis.htm
(2020-02-22).
[13] File:Patricia trie.svg. URL: https://en.wikipedia.org/wiki/
File:Patricia_trie.svg (2020-02-23).
[14] Sparse indexes. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 72/rzajq/
rzajqsparseindex.htm (2020-05-10).

72

[15] Derived key indexes. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 72/rzajq/
rzajqderivedindex.htm (2020-05-10).
[16] Recommendations for EVI use. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 72/rzajq/
recommendationsforevi.htm (2020-05-10).
[17] Controlling queries dynamically with the query options file QAQQINI. URL:
https://www.ibm.com/support/knowledgecenter/ssw_ibm_i_
74/rzajq/qryoptf.htm (2020-02-01).
[18] QAQQINI query options. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 74/rzajq/qryopt.htm
(2020-02-01).
[19] OVERRIDE_QAQQINI procedure. URL: https://www.ibm.com/
support/knowledgecenter/ssw_ibm_i_ 73/rzajq/
rzajqprocoverrideqaqqini.htm (2020-04-27).
[20] Messages. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 74/rzal2/rzal2messages.htm
(2020-05-10).
[21] I. Molyneaux. The Art of Application Performance Testing 2e. ISBN:
(9781491900543).
[22] B. Wescott, A. Macijeski. Every Computer Performance Book. ISBN:
(9781482657753).
[23] Performance Services. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 74/rzajq/
rzajqservicesperf.htm (2020-05-10).
[24] Retrieve Job Information (QUSRJOBI) API. URL: https://www.ibm.com/
support/knowledgecenter/ssw_ibm_i_ 74/apis/
qusrjobi.htm#HDRJOBI9 (2019-12-15).
[25] Programming techniques for database performance. URL:
https://www.ibm.com/support/knowledgecenter/ssw_ibm_i_
74/rzajq/progtech.htm (2020-05-10).
[26] Use FETCH FOR n ROWS. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 74/rzajq/fetchnrows.htm
(2020-05-10).
[27] Proactive approach to tuning. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 74/rzajq/rzajqproact.htm
(2020-05-12).
[28] Index Skip Scanning. URL: https://oracle-base.com/articles/9i/
index-skip-scanning (2020-05-07).
[29] B. Wilkins. Tips for improving INSERT performance in DB2 Universal
Database. URL: https://developer.ibm.com/technologies/
databases/ articles/dm-0403wilkins/ (2020-04-30).
[30] Use INSERT n ROWS. URL: https://www.ibm.com/support/
knowledgecenter/ssw_ibm_i_ 74/rzajq/insertnrows.htm
(2020-05-10).

73

74

Appendix
Source listings

75

**FREE
ctl-opt
option(*srcstmt:*nodebugio:*nounref)
datfmt(*iso) timfmt(*iso)
dftactgrp(*no) actgrp(*caller)
bnddir('CEXCS00104')
main(generateTestData);
/copy QPROTSRC,CEXCS00104
//-Sql options-----------------------------------exec sql set option
commit = *none, closqlcsr = *endmod,
datfmt = *iso, timfmt = *iso,
dlyprp = *yes;
//-----------------------------------------------dcl-proc generateTestData;
dcl-pi *n;
dataset varchar(50) const;
warmup ind const;
dataSize int(10) const;
end-pi;
dcl-s count int(10);
// clear any old data from the tables
exec sql
delete from account_attributes;
assertSqlSuccessOr100(sqlCA);
exec sql
delete from account_attributes_log;
assertSqlSuccessOr100(sqlCA);
// put 3 attributes on each account,

76

// with a total of 5 different attribute types
exec sql
insert into account_attributes (
account_id, attribute_type
)
with counter (i) as (
select 1 from sysibm.sysdummy1
union all
select i+1 from counter where i < :dataSize
)
select i/3, mod(i, 5)+1
from counter;
assertSqlSuccess(sqlCA);
// insert CREATED entries for
// all existing attributes 2010 ->
exec sql
insert into account_attributes_log (
account_id, attribute_type,
log_action, log_timestamp
)
select account_id, attribute_type, 'CREATED',
timestamp(
date('2010-01-01'),
time('00:00:00')
+ mod(account_id, 1000) hours
+ (10 * attribute_type) minutes
)
from account_attributes;
assertSqlSuccess(sqlCA);
// insert UPDATED entries for
// the second half of the active entries
// 2015 ->
exec sql

77

insert into account_attributes_log (
account_id, attribute_type,
log_action, log_timestamp
)
select account_id, attribute_type, 'UPDATED',
timestamp(
date('2015-01-01'),
time('00:00:00')
+ mod(account_id, 1000) hours
+ (10 * attribute_type) minutes
)
from account_attributes
order by account_id desc
fetch first :dataSize/2 rows only;
assertSqlSuccess(sqlCA);
// insert DELETED entries for
// the first half of the active entries
// 2009 ->
exec sql
insert into account_attributes_log (
account_id, attribute_type,
log_action, log_timestamp
)
select account_id, attribute_type, 'DELETED',
timestamp(
date('2009-01-01'),
time('00:00:00')
+ mod(account_id, 1000) hours
+ (10 * attribute_type) minutes
)
from account_attributes
order by account_id asc
fetch first :dataSize/2 rows only;
assertSqlSuccess(sqlCA);

78

// insert CREATED entries for
// the deleted entries
// 2000 ->
exec sql
insert into account_attributes_log (
account_id, attribute_type,
log_action, log_timestamp
)
select account_id, attribute_type, 'CREATED',
timestamp(
date('2000-01-01'),
time('00:00:00')
+ mod(account_id, 1000) hours
+ (10 * attribute_type) minutes
)
from account_attributes_log
where log_action = 'DELETED';
assertSqlSuccess(sqlCA);
// clear the temp table for batch #2
exec sql
delete from account_attributes_log_deleted;
assertSqlSuccessOr100(sqlCA);
end-proc;

Listing 21. Generation of data for the log id creation benchmark.

79

//-----------------------------------------------dcl-proc setupIndexes;
dcl-pi *n;
dataset varchar(50) const;
warmup ind const;
dataSize int(10) const;
end-pi;
exec sql
delete from table_for_index_count_tests;
assertSqlSuccessOr100(sqlCA);
exec sql drop index MLUTINS01T_1234;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_1243;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_1324;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_1342;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_1423;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_1432;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_2134;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_2143;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_2314;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_2341;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_2413;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_2431;

80

assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_3124;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_3142;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_3214;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_3241;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_3412;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_3421;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_4123;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_4132;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_4213;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_4231;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_4312;
assertSqlSuccess(sqlCA: -204);
exec sql drop index MLUTINS01T_4321;
assertSqlSuccess(sqlCA: -204);

if dataSize >= 1;
exec sql
create index MLUTINS01T_1234
on MLUTINS01T
(column1, column2, column3, column4);
assertSqlSuccess(sqlCA);
endif;

81

if dataSize >= 2;
exec sql
create index MLUTINS01T_1243
on MLUTINS01T
(column1, column2, column4, column3);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 3;
exec sql
create index MLUTINS01T_1324
on MLUTINS01T
(column1, column3, column2, column4);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 4;
exec sql
create index MLUTINS01T_1342
on MLUTINS01T
(column1, column3, column4, column2);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 5;
exec sql
create index MLUTINS01T_1423
on MLUTINS01T
(column1, column4, column2, column3);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 6;
exec sql
create index MLUTINS01T_1432
on MLUTINS01T
(column1, column4, column3, column2);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 7;

82

exec sql
create index MLUTINS01T_2134
on MLUTINS01T
(column2, column1, column3, column4);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 8;
exec sql
create index MLUTINS01T_2143
on MLUTINS01T
(column2, column1, column4, column3);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 9;
exec sql
create index MLUTINS01T_2314
on MLUTINS01T
(column2, column3, column1, column4);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 10;
exec sql
create index MLUTINS01T_2341
on MLUTINS01T
(column2, column3, column4, column1);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 11;
exec sql
create index MLUTINS01T_2413
on MLUTINS01T
(column2, column4, column1, column3);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 12;
exec sql

83

create index MLUTINS01T_2431
on MLUTINS01T
(column2, column4, column3, column1);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 13;
exec sql
create index MLUTINS01T_3124
on MLUTINS01T
(column3, column1, column2, column4);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 14;
exec sql
create index MLUTINS01T_3142
on MLUTINS01T
(column3, column1, column4, column2);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 15;
exec sql
create index MLUTINS01T_3214
on MLUTINS01T
(column3, column2, column1, column4);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 16;
exec sql
create index MLUTINS01T_3241
on MLUTINS01T
(column3, column2, column4, column1);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 17;
exec sql
create index MLUTINS01T_3412

84

on MLUTINS01T
(column3, column4, column1, column2);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 18;
exec sql
create index MLUTINS01T_3421
on MLUTINS01T
(column3, column4, column2, column1);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 19;
exec sql
create index MLUTINS01T_4123
on MLUTINS01T
(column4, column1, column2, column3);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 20;
exec sql
create index MLUTINS01T_4132
on MLUTINS01T
(column4, column1, column3, column2);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 21;
exec sql
create index MLUTINS01T_4213
on MLUTINS01T
(column4, column2, column1, column3);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 22;
exec sql
create index MLUTINS01T_4231
on MLUTINS01T

85

(column4, column2, column3, column1);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 23;
exec sql
create index MLUTINS01T_4312
on MLUTINS01T
(column4, column3, column1, column2);
assertSqlSuccess(sqlCA);
endif;
if dataSize >= 24;
exec sql
create index MLUTINS01T_4321
on MLUTINS01T
(column4, column3, column2, column1);
endif;
end-proc;

Listing 22. Creation of indexes for the index count insertion test.

86

