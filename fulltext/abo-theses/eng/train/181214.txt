3D CN-LSTM for prediction of
medical nanoparticle properties

Vid Sustar, 1901627
Supervisors: Sepinoud Azimi Rashti, Sébastien Lafond
Faculty of Science and Engineering
Åbo Akademi University
2021

Vid Sustar
Abstract
Cancer is an increasing and already one of the most common causes of death
in developed countries. One way to fight cancer tumours is with targeted antitumour drug delivering nanoparticles (NPs). NPs can be composed of gold
core covered with a variety of drug and supporting substances (SS) in varying
ratios.
Since chemical synthesis of all potential NPs is costly, to find the most
optimal drug and drug-SS ratios out of many potential candidates, NPs are
simulated in silico in molecular dynamics (MD) simulations. To further lower
the costs and expand coverage of potential optimal NP compositions, computationally demanding MD simulations of NPs could in part be replaced with
Deep Learning (DL) neural networks. Here the properties of NPs at later
stages of MD simulation would be predicted with DL from NP properties from
starting stages of MD simulations.
As MD simulations are time series and NPs simulated are 3D objects, one
can join two types of DL: recurrent neural networks (RNN) and convolutional
neural networks (CNN) to create a suitable DL network. The scope of this
master’s thesis is running MD simulations, finding proper DL architecture for
the model, refining the input and assessing the predictions of the refined model.
The architecture giving the best prediction of NP drug exposure is a combination of concatenated 3D CNN for NP structure input and dense layers for
other types of input fed into Long Short-Term Memory (LSTM) RNN.
Keywords: nanoparticles, molecular dynamics, deep learning, RNN, CNN,
LSTM

ii

Vid Sustar

Abbreviations:
CNN - Convolutional Neural Network
DL - Deep Learning
LSTM - Long Short-Term Memory
MD - molecular dynamics
NP - nanoparticle
RNN - Recurrent Neural Network
SS - supporting substance

iii

Vid Sustar

Acknowledgements
I would like to thank my supervisors Sepinoud Azimi Rashti and Sébastien Lafond for
introducing me to the exciting topic of my thesis. I am also grateful to professor Jan
Westerholm for his help in setting up the computations on the local cluster Dione,
the assistants Valenting Soloviev and Luca Zelioli for their explanations about bash
scripting, deep learning, 3D convolution and LSTMs. I am also grateful to colleague
Otto Lindfors as well as other students for discussions and suggestions about deep
learning and tensorflow.
Last, but not least I thank my wife Mira for her patience and for allowing me to
sacrifice some of our family life for the pursuit of a master’s in computer science.

iv

Vid Sustar

CONTENTS

Contents
1 Introduction
1.1 Cancer and EVO-NANO project . . . . . . . . . . . . . . . . . . . . .
1.2 Nanoparticles and Molecular Dynamics shortcuts . . . . . . . . . . .

1
1
2

2 Background
2.1 Deep Learning . . . .
2.2 Convolutional Neural
2.3 RNN and LSTM . .
2.3.1 RNN . . . . .
2.3.2 LSTM . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

4
4
6
7
8
9

3 Methods
3.1 Software and hardware used . . .
3.2 Molecular Dynamics Simulations
3.3 Data pre-processing . . . . . . . .
3.3.1 SASA . . . . . . . . . . .
3.3.2 PDB processing . . . . . .
3.4 Technical DL definitions used . .
3.5 Models used . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

12
12
13
16
16
18
24
24

. . . . . .
Networks
. . . . . .
. . . . . .
. . . . . .

4 Results

28

5 Discussion

40

6 Conclusions

44

7 Appendix

47

v

Vid Sustar

1

1 INTRODUCTION

Introduction

In this section the topic of the thesis is explained with introduction to cancer and
drug delivering nanoparticle simulations and its possible deep learning upgrade.

1.1

Cancer and EVO-NANO project

Cancer is one of the leading causes of death in developed countries [1]. The main
reasons for the high incidence of cancer are mainly consequences of modern lifestyle
and toxic environment: tobacco smoking, consumption of alcohol, a diet low in fruit
and vegetables, physical inactivity, obesity, sexual transmission of human papillomavirus, exogenous hormones, UV radiation, etc. [2]. Since cancer is an increasing
cause of death, it is of even greater importance to find the possible remedies. The
best approach is preventive care through avoidance of the above list of cancer-causing
agents. However, when the cancer is already present, there are several possible treatments depending on the stage and type of cancer, e.g. surgery, radiation therapy,
chemotherapy, targeted therapies, immunotherapy, hormonal therapy, angiogenesis
inhibitors and synthetic lethality. Each of these has its own benefits and drawbacks.
More novel approaches, as for example the approach of cancer-targeting nanoparticles
which can carry an array of drugs, may be a combination of the above.
This master’s thesis is a subproject of the ’Evolvable platform for programmable
nanoparticle-based cancer therapies’ (EVO-NANO) EU project which, in turn, is a
project in Horizon 2020, Cordis framework [3]. The main aim of EVO-NANO is to
create a cross-disciplinary platform for assessment of drug-delivering nanoparticles
(NPs) in-silico. The NPs are proving to be an ever more interesting anti-cancer
therapeutical approach, however, their effective distribution in the body remains
its limitation. It is crucial to understand the behaviour of the multitude of the
synthetic NPs interacting with tumour in its natural environment. In order to find
effective NPs, new types of algorithms need to be developed that will satisfy several
conditions: creation of new anti-cancer strategies, expansion of the space of possible
solutions, being adaptable to changing scenarios. Finally, the best selected candidates
will be synthesised and tested in vivo and in vitro on tumour cells (xenografts and
microfluidic testbeds). The research stage is directly continued in production of
commercialised product, which can attract additional investors and give the project
additional potential. With its relatively fast development and assessment cycle, EVONANO is at the forefront of cancer nanomedicine. The project will produce tangible
tools to help people fight cancer.

1

Vid Sustar

1.2

1 INTRODUCTION

Nanoparticles and Molecular Dynamics shortcuts

The NPs used are composed of a gold core and a fixed total number of hydrocarbon
chains that carry either drug or supporting substance (SS) in varying ratios (see
Figure 1). The role of the gold core is as a surface to be covered with desired
molecules under controlled conditions and is, otherwise, chemically inert. As such,
the NP core is stable and its behaviour predictable. Gold reflects infrared wavelengths
and allows potential long wavelength multiphoton tracking. Different drug types also
have different water solubility properties. SS improves the solubility of NP-attacheddrug in water (blood). Exact theoretical prediction of the behaviour of many drug
molecules chained to NP via hydrocarbons is not possible and neither is SS vs drug
interaction possible, as it is a matter of interactions of thousands of atoms, and
there can be emergent properties. For this reason, and because chemical synthesis of
nanoparticles is slow and costly, varying drug and drug vs complementary substance
are simulated via molecular dynamics (MD) simulations in-silico.

Figure 1: The sliced simulated nanoparticle with gold core (yellow), drug molecules
(red), supporting substance (turqoise) and hydrocarbon chains connecting them to
NP (redish and greenish hues). The diameter is 5 nm.

2

Vid Sustar

1 INTRODUCTION

Molecular dynamics (MD) is a computer simulation method for analysing the
physical movements of atoms in molecules. The atoms are set to interact for a defined period of time, which gives a representation of dynamic “evolution” of the
system. Usually, trajectories of atoms are determined by usage of Newtonian motion
laws for interacting particles, where forces between atoms are calculated with interatomic potentials or molecular mechanics force fields. NPs used in MDs need to have
exactly defined composition. The nanoparticles used in this study, for example, have
6000 gold atoms, 50 000 atoms belonging to combined 420 SS and drug molecules
and 50 000 atoms belonging to water molecules. All in all, there are more than 100
000 interacting atoms. The MD is calculated to show the stabilisation of “theoretical” nanoparticles from vacuum into water solvent and its stabilised behaviour in
water. This requires the simulation of NPs in water for a range between 300 and 600
nanoseconds. To calculate such a multitude of interactions a modern top shelf GPU
needs to calculate one MD simulation for a week.
To further lower the costs and expand coverage of potential optimal NP compositions, computationally demanding MD simulations of NPs could, in part, be
replaced with Deep Learning (DL) neural networks, where the properties of NP at
later stages of MD simulation would be predicted with DL from NP properties from
starting stages of MD simulations. As MD simulations are time series and nanoparticles are 3D objects, one can join two types of DL: recurrent neural networks (RNN)
and convolutional neural networks (CNN) to create a suitable DL network. The
scope of this thesis is running MD simulations, finding proper DL architecture for
the model, and refining the input for chosen DL architecture.

3

Vid Sustar

2

2 BACKGROUND

Background

In this section deep learning and its basic building blocks, principles are explained
as well as more specific solutions to extraction of features from image/matrix data
and from sequential data in the form of Convolutional Neural Networks and Long
Short-Term Memory.

2.1

Deep Learning

Machine learning is an array of computer algorithms that improve through experience
[4]. Machine learning can be either unsupervised or semi/supervised. In supervised
machine learning, the algorithms are trained using labeled data, whereas in unsupervised machine learning the algorithms are trained on unlabeled data. Artificial
neural networks (ANNs) are part of supervised machine learning methods [5].
Artificial neural networks, as the name implies, were inspired by biological neural
networks (BNNs). There are major differences between the two, as ANNs tend to be
static and digital, whereas BNNs are dynamic and analog [6].
The artificial neural networks are composed of neurons. Each artificial neuron has
one or more inputs and produces an output that can be simultaneously sent forward
as input to multiple other neurons.
The output of each neuron is produced in the following manner:
1. All the inputs to the neuron have weights, the weighted inputs are summed up.
2. A bias value is added to the above sum, which forms activation.
3. Activation is passed through nonlinear activation function to produce the output.
The output of a neuron is an activation function to the neurons in the next layer,
it determines whether the neurons in the next layer should be activated (“fired”) or
not. The activation function introduces the non-linearity which separates a neural
network from simpler regression models [7]. An example representation of an artificial
neuron with inputs, weights, bias, that compose activation function is in figure 2.

4

Vid Sustar

2 BACKGROUND

Figure 2: A diagram of an artificial neuron with: inputs x, weights w, bias b, composing an activation function f. From [8].
Neurons are organised in parallel into layers: parallel input neurons - input layer,
intermediary and output layers. When there is more than one intermediary hidden
layer between input and output layer such a network is called “deep”, hence deep
learning neural networks.
In case of the input layer, the input values might be pixel values of an image, or
vectorised characters of a string, whereas the final outputs of the output layer can
be annotated categories of an image, etc. An example of a simplified representation
of DL network is in figure 3.

Figure 3: Simplified representation of a DL network, from [9].
Neural networks learn through the algorithm of backpropagation from errors in
predictions. The errors (inaccuracy) in predictions are measured and calculated by
loss function. The aim of deep learning models is to minimize the loss function value
through the process of optimization. Optimization is an algorithm that modifies the
5

Vid Sustar

2 BACKGROUND

weights of neural network through gradient descent. During each iteration, weights
are being tweaked to decrease the loss function gradually approaching a minimum or
convergence, as shown in figure 4 [10].

Figure 4: Gradient descent: cost of loss function minimisation with DLN weights
optimisation through iterative learning steps, from [10].
When optimising the weights, weights can be adjusted to different extents. The
adjustment can be considered as a step size or a learning rate of iterative gradient
descent. Higher learning rate means faster descent (learning) by smaller number of
iterations, however, it can be at a cost of missing the convergence [11], p. 247.
There are many optimisation algorithms used, one of the most commonly used is
for example Adam, which computes adaptive individual learning rates for different
parameters from estimates of first and second moments of the gradients [12], p. 1.

2.2

Convolutional Neural Networks

When two functions are merged into a third function, this process is called convolution. Among artificial neural networks, there is a type that is even more closely
mimicking biological neural networks. Convolutional neural networks are organised
similarly to visual cortex. In animal visual cortex, single neurons respond to the stimulus covering just a small, restricted subregion of the cortex. Similarly, convolutional
neural networks apply filters also known as kernels that cover and extract features of
individual subregions of multidimensional input. Through this process, CNNs capture the temporal and/or spatial dependencies in data through the application of
corresponding filters.
In figure 5 is shown an example of kernel feature detection with dot product multiplication between 3x3 kernel and same-sized part of input matrix. The destination
pixel on the resulting matrix is the sum of the aforementioned multiplication product.
6

Vid Sustar

2 BACKGROUND

The kernel scans through the whole input matrix, creating a feature map composed
of product sums.
Many different kernels are used to detect different types of initial simple features
like edges etc. The feature maps of each kernel are combined in an output of convolutional layer [13]. Important parameters in the process are strides and padding.
Strides are the number of pixels a kernel scans over the source matrix. Padding are
the additional edge pixels, if the kernel multiple does not fit into the source matrix.
The initial filters can be relatively simple, however, by assembling several consecutive convolutional layers on top of each other, each additional layer contributes to
recognition and extraction of patterns of additional levels of complexity. Padding
can be done in two ways: zero padding, by adding of zeroes to the edges or via valid
padding, by removing a sufficient number of edge cells/pixels both with a goal for
kernel multiple to fit into the matrix [14].

Figure 5: Principle of feature extraction with convolutional kernels of an image (pixel
array), from [13].
To extract the features, pooling layers are used to reduce the width and height
of feature maps, whereas the depth is preserved. The benefit of pooling is that
the most informative features are preserved while required computational power is
reduced. Pooling layers have two variations: max pooling, which returns max value
within a considered area and average pooling, which returns the average value. Since
max pooling extracts dominant features better, it performs better and is more widely
used.

2.3

RNN and LSTM

In this section the deep learning solutions to sequential data are explained, namely
Recursive Neural Networks and Long Short-Term Memory.
7

Vid Sustar

2.3.1

2 BACKGROUND

RNN

The CNN is a type of ANN that is a feedforward network, which learns from dataset,
where the order of the input is not important and does not affect the output result.
However, if time-dependent data are being analysed and the objective is to predict
the future events, one needs to employ Recursive Neural Networks. RNNs are capable
of modelling sequencesvby remembering past information and processing new events
accordingly [15], p. 539. ANNs and RNNs have many components in common, such
as neurons, cost functions and back-propagation, but the major difference is that
RNNs use sequential data as input. Sequential data contain not only the sample
information and their features as such, but also additional information about their
order. Few examples of such information are video sequences and sentences. These
would lose their original meaning if the order were changed. The data in this thesis,
the molecular dynamics trajectories, are sequential data.
In RNNs, besides doing forward and backward propagation, information also flows
in cycles. Through cycles, the information of present timepoint influences the future
timepoint [15], p. 368.
These cycles are done within the hidden layers of a network, see figure 6 of an
RNN, where they are coloured orange. On the right side of the figure, the main
difference between simple feedforward ANNs and RNNs is apparent, while the hidden neurons of ANNs receive only one (initial, “outside”) input, the RNNs receive
two; both outside input, and the output from the preceding sequence element as an
input. Since the next time step refers to a previous one, recurrence occurs, hence
the name Recurrent NN. As such, RNN can approximate recurrent functions, unlike
feedforward ANNs.

Figure 6: Simple RNN structure, with hidden are coloured orange, input blue, and
output green. Looped arrow in the left part represents the cycles in hidden layers,
exposed in the unfolded representation on the right, where the hidden layer from
a previous sequence element (“timepoint”) is connected to the subsequent element.
From [15], p. 544.
Sequential data contain ordered progression, and can therefore provide so-called
parameter sharing to RNNs and need fewer examples, compared to ANNs [16], p.
371.
8

Vid Sustar

2.3.2

2 BACKGROUND

LSTM

The RNN uses feedback connections for the storage of input-representations in a form
of activations, creating a short-term memory (as opposed to a long-term memory in
slower weight-changing). However, in their basic form, RNNs have a drawback: error
signals going backwards in time may blow up or disappear, since backpropagated
error depends on the size of the weights exponentially. In the first case, weights will
oscillate, whereas in the second case, bridging long time lags consumes too much
time, or cannot be accomplished [17].
Long Short-Term Memory (LSTM) type of RNNs solves the aforementioned issues
of basic RNNs [18]. LSTM transforms the inputs in a sophisticated manner. As can
be observed in figure 7, the inputs to each LSTM cell are manipulated and processed
to create two outputs, which represent ‘long-term memory’ (upper horizontal line of
arrows) and ‘short-term memory’ (lower output).

Figure 7: Comparison of RNN (above) and LSTM (below). The repeating cell in
RNN contains only one layer, whereas LSTM contains four interacting layers: the
original tanh layer + 3 gates (sigmoid NN layer + pointwise operation in red frame)
that control the flow of information to the next cell. Modified from [19].
Vectors which go along the upper long-term memory channel may pass without
9

Vid Sustar

2 BACKGROUND

any changes. The gates (a combination of a sigmoid NN layer and a following pointwise operation) in the figure may block or add information. This way the network
can retain data from an arbitrary number of sequence points (cells) in the past.
LSTM works in the following fashion:
1. The preceding hidden state and the current input are combined (concatenated)
into a “combine”.
2. The combine is fed into the forget gate, which removes non-relevant data.
3. The combine is used to create a “candidate” gate, which holds possible values
to add to the cell state.
4. The combine is fed into the “input” gate, which decides what data from the
candidate should be added to the new cell state.
5. The cell state is calculated after the gates are computed, from the gate values
and the previous cell state.
6. The output is then computed.
7. Pointwise multiplication of the output and the new cell state creates a new
hidden state.

The equations for the gates of the LSTM are the following [20]:
1. Input gate, determines what new information will be stored in the cell state:
it = δ(wi ) [ht−1 , xt ] + bi )
2. Forget gate determines which information to discard from the cell state:
ft = δ(wf ) [ht−1 , xt ] + bf )
3. Output (candidate) gate which activates the final output of the LSTM cell at
timespoint ‘t’:
ot = δ(wo ) [ht−1 , xt ] + bo )
4. Candidate cell state:
c̃t = tanh(wc ) [ht−1 , xt ] + bc )
5. Cell state:
ct = (ft ) ∗ ct−1 + it ∗ c̃t
6. Final output:
ht = (ot ) ∗ tanh(ct )
10

Vid Sustar

2 BACKGROUND

Where:
it → represents input gate
ft → represents f orget gate
ot → represents output (candidate) gate
δ → represents sigmoid f unction
wx → weight f or the respective gate (x) neurons
ht−1 → output of the previous LST M cell
xt → input at currenttimepoint
bx → biases f or the respective gates (x)
ct → cell state (memory) at timepoint (t)
c̃t → represents candidate f or cell state at timepoint (t)
The ability of long-term information retention expands the network’s attention
compared to the basic RNNs. In addition to being able to access the previous cells
states, it can also access the learning from the past cells, which enables referencing
the context [19].
Because of the additional tanh and three sigmoid layers, compared to the original
RNNs the drawback of LSTMs is the need for greater memory and computational
power [21].

11

Vid Sustar

3

3 METHODS

Methods

The main goal of the model is to predict the solvent accessible surface area of a drug
on the nanoparticles in future timepoints with simulation, given the input of earlier
timepoints. The input is based on molecular dynamics simulation of the nanoparticles. From these simulations, several parameters are extracted via pre-processing and
fed into a model. The whole data creation, pre-processing and modelling pipeline is
shown in figure 8.

Figure 8: Whole pipeline: Molecular dynamics simulation (red box), data preprocessing (green box), neural network (blue box).

3.1

Software and hardware used

The practical work of this thesis was performed on the following clusters and programs:
1. The molecular dynamics simulations and conversions to PDB format were performed with Amber, a software suite for biomolecular simulation and analysis,
version 18 [22] on a CSC cPouta cluster and Dione local cluster, co-owned by
UTU/ÅA, both with Nvidia Tesla P100-PCIE 16 Gb graphical cards.
2. The drug SASA (Solvent Accessible Surface Area) exposure calculations were
performed with VMD (Visual Molecular Dynamics for LINUXAMD64, version
12

Vid Sustar

3 METHODS

1.9.4a43 [23]) software on CSC cPouta cluster.
3. NP visualisation was perfomed with UCSF Chimera (UCSF ChimeraX version
1.0, [24]) on a personal laptop with GeForce RTX 2060 6 GB graphical card.
4. The Deep Neural Networks were created with TensorFlow with Keras API in
Python. The trial neural networks were tested on the aforementioned personal
laptop, whereas longer trainings were performed on CSC cPouta and Dione
clusters.

3.2

Molecular Dynamics Simulations

Molecular dynamics simulations (MDS) is a method for analysing the physical movements of atoms and molecules. In MDS, particles (atoms, molecules) interact for a
given period of time, providing dynamic evolution of the whole system. Usually, the
trajectories of the particles are determined by numerically solving Newton’s equations
of motion.
In this project, Amber, a software suite for biomolecular simulation and analysis,
was used to perform MDS. PMEMD (Particle Mesh Ewald Molecular Dynamics) is
the primary engine for running molecular dynamics simulations with Amber.
The input files for PMEMD, are parametric files, which describe the initial positions of the atoms, their associations and system temperature:
1. A primary file in Amber defines the system topology and the parameters for
the force field for that system. It has an extension “*.prmtop”, sometimes
preceded by “*.solv.prmtop”, which indicates the presence of a solvent, in this
case water. The first few lines of such afile are presented in figure 9.

Figure 9: First few lines of an exemplary “.prmtop” file, containing the system
topology and the parameters defining the force field for that system. Au stands for
NP gold core atoms.

13

Vid Sustar

3 METHODS

2. Coordinate/restart file has initial coordinates of the atoms with the extension:
“*.inpcrd”. First few lines of such a file are presented in figure 10.

Figure 10: First few lines of exemplary “.inprcd” file, defining initial coordinates of
atoms.
3. File “heat.in” defines the temperature of the system that is being controlled
throughout the simulation, in a similar fashion as a thermostat - the kinetic
energy of the particles is readjusted to keep a set temperature, as in figure 11.

Figure 11: Example of “heat.in” file, the set temperature of the system throughout
the simulation.
4. File “density.in” defines the pressure and density of the system throughout the
MDS, as exemplified in figure 12.

14

Vid Sustar

3 METHODS

Figure 12: Example of “density.in” file, the set pressure of the system throughout
the simulation.
These input files were produced by our collaborator Marina Kovacevic from University of Novi Sad, Serbia.

PMEMD was run in CUDA version of the program, to use with parallelisation
with GPUs. The MDS were run to simulate 300 nanoseconds (ns) of particle interactions. While PMEMD is running, the progress and speed of calculations can be
inspected by examining the file “mdinfo”. It contains information on how many steps
have been completed, how many nanoseconds of simulation can be run per day with
this system and how much time there is left before the specific job is finished [25].
Exemplary script to run PMEMD on CSC cPouta virtual machine can be found
in the appendix listing 1.
The output of the MDS are a set of files containing trajectories with continuous
information on the positions of the particles and the final parameters for every 10 ns
of the simulation.
An example of the descriptive file after the completion of the molecular dynamic
simulation, which describes the calculated parameters of the system, is found in figure 13 with 10 ns of simulation.

15

Vid Sustar

3 METHODS

Figure 13: A descriptive file after the completion of a molecular dynamic simulation,
which describes the calculated parameters of the system.

3.3
3.3.1

Data pre-processing
SASA

The trajectories obtained from the MD simulations can be used directly for nanoparticle observation in programs such as Chimera or VMD.
Another, more important direct usage of the PMEMD trajectories for our studies,
is the calculation of the solvent accessible surface area (SASA) of the drug vs the
whole NP surface in VMD.
SASA is measured by rolling a ball of a diameter that is equal to the radius of the
solvent, in this case water (1.4 Å), across the surface of the NP with a command in
VMD:
”measure sasa 1.4 $all”
The SASA can be limited only to the SASA of the drug. The percentage of wateraccessible surface of the drug vs the total water-accessible surface of the NP is calculated and used as the main parameter of the drug exposure in NP. The goal of the
EVO-NANO project is to find this drug and drug vs supporting substance ratio.
An example of calculated SASAs of a NP with Panobistat in ratio vs supporting
substance (1:1) can be seen in figure 14 and the corresponding graphs can be seen in
figure 15 and figure 16.

16

Vid Sustar

3 METHODS

Figure 14: The first 18 timepoints of the SASAs, calculated for each ns of the MD
simulation of a NP consisting of a gold core, Panobistat and a supporting substance
(in ratio 1:1).

Figure 15: A graph of the drug and the total NP surface SASAs, calculated for all
300 ns of the MD simulation of a NP consisting of a gold core, Panobistat and a
supporting substance (in ratio 1:1).

17

Vid Sustar

3 METHODS

Figure 16: A graph of the percentage of a drug per total NP surface SASAs, calculated
for all 300 ns of the MD simulation of a NP consisting of agold core, Panobistat and
a supporting substance (in ratio 1:1) (in blue) and the calculated 100 ns moving
average (in orange).
A script to run VMD SASA calculations on CSC cPouta virtual machine can be
found in the appendix listing 2.
In the graphs in figure 15 and figure 16 (blue line), it can be observed that the
SASA behaviour of the NP is stochastic, increasing and decreasing from timepoint to
the next for several square Å. The stochasticity does not provide valuable information,
but rather a noise that is more difficult to predict with DL. Therefore, the original
SASA calculations were smoothed with a moving average for 100 timepoints with a
Python Pandas library with “.rolling” command, as can be seen in appendix 3. The
smoothened data example can be seen in figure 16 (orange line).
3.3.2

PDB processing

In order to simplify the extraction of nanoparticle properties, Amber MDS trajectory
files can be converted to PDB files. This conversion is performed with an Amber cpptraj program. An example bash script of this can be found in the listing 4.
PDB files are lists of atomic positions in 3D space at a given timepoint. Throughout
this study, the conversion was for one PDB file per ns, so that 300 ns long MDS
yielded 300 PDB files. Besides containing the information of the spatial positions of
the atoms, PDB also contains the information on atomic associations, the covalent
bonds that chain the atoms into the molecules. The first few exemplary lines of this
file can be seen in figure 17.

18

Vid Sustar

3 METHODS

Figure 17: First few lines of exemplary “.pdb” file of a NP consisting of a gold
core, Panobistat and a supporting substance (in ratio 2:1) with atom associations
and their coordinates in 3D space, where the units are Å, and finally the occupancy,
temperature factor and the element name.
PDB files are the basis for further processing. The main goal of this thesis is to
model the SASA percentage as mentioned in 3.3.1, with as little pre-processing of
the data as possible and to relay the task of the important feature extraction to the
extensive DL. The principle of least-processed data is presented in the mapping of
the atomic positions from the simulated trajectories (or its PDB “snapshot”) into a
4D array (3D positional data + timepoints). PDB files are opened with PandasPDB
Python library as a Pandas data frame. The atoms associated into residues were
assigned an “intensity” value based on their association with the drug (higher values
close to 255) or supporting substance (lower values, closer to 100). An example of
this can be seen in figure 18.

19

Vid Sustar

3 METHODS

Figure 18: An example of mapping 3D atomic positions of NP into a 3D array. The
intensities are assigned based on atomic associations with either a drug or a supporting substance. The example is made from Panobistat and a supporting substance (in
ratio 1:2).
The initial mapping of all the NP atoms in PDB required a 3D matrix with a
minimum size of 200 (“resolution”) in order to avoid the overwriting of the atomic
positions with close by atoms. However, the resulting matrices were too large for the
GPU memory, also considering the time dimension (x 300). GPU memory is needed
in training the DL with parallel processing offered by the GPU. Therefore, only the
geometrically central atoms of each residues were considered, reducing the required
minimum resolution (size of the 3D matrix) to 120. Because the reduced resolution
of 120 was still too large for the GPU memory, an additional approach was to slice
the spherical NP into 8 quadrants and flip them into the same orientation: upper
front left. In this way, not only was the input made 8 times smaller (matrix of size
60), but also the number of inputs was 8 times greater. An example of this can be
seen in figure 19.
20

Vid Sustar

3 METHODS

Figure 19: Reducing the size of the 3D matrix input by mapping just the centres of
residues and slicing the NPs into individual quadrants. The example is made from
quinolinol and a supporting substance (in ratio 5:1).
Each quadrant was additionally flipped in y and z axis by 90 degrees, resulting in
a three-fold increase of the input. To additionally increase the amount of input, the
coordinate positions were rotated in 15 steps by an angle of 25 degrees concurrently
in y and z axis prior to the mapping into matrices, effectively producing 15 times
more input. All geometric operations to increase the number of input can be seen in
figure 20. The total input was 11 000 samples.

21

Vid Sustar

3 METHODS

Figure 20: Presentation of all geometric operations to increase the number of samples:
rotation by angle of 25 degrees (x15) concurrently in y and z axis (A), splitting the
NP into 8 quadrants (B) and flipping each quadrant by 90 degrees in y and z axis
(C). The operations are multiplicative, resulting in 360 fold increase of samples from
initial whole NPs.
Another set of information from the atom positions in PDBs is a pre-extracted
set of features, the statistics on the distances between the atoms. From the positions
of the atoms, it is possible to calculate the Radial Distance Function (RDF). The
RDF informs about the distance from the centre of the NP. The RDF is calculated
by setting the central golden atom as an origin and converting the original Cartesian
coordinates of the residues into polar coordinates and then measuring the radial
component. Another set of measurements are the distances between molecules of the
same type, to reveal a possible clustering of the residues. An example of the statistics,
the mean, median, maximum and minimum distances, can be seen in figure 21.
All of the measured distance-statistics in time were averaged with a 100 timepoint
window to reflect the averaging of the target data; the SASA percentage as mentioned
in 3.3.1 ). The script used to map the PDB atomic positions into Numpy arrays,
perform the rotations, quadrants selections, and calculate the radial and inter-residue
distances can be found in The Python script used to build, train and use the model for
predictions can be found in 5. The output numpy arrays were flipped and combined
with distances tables and SASA calculations into Numpy compressed files for the
input during training and prediction of the model with the script6.
22

Vid Sustar

3 METHODS

Figure 21: The first few timepoints of an exemplary list of statistics of calculated
distances of molecules, either from the core of NP - Radial Distance Function or
between the molecules of same type, as a measurement of clustering. The example
is made from Panobistat and a supporting substance (in ratio 2:1), units are Å.

Figure 22: A graph from the list in figure 21, where Drg/SupRDMdn stands for the
median of all of the radial distances of a drug (or a supporting substance) molecule
from the centre of NP and Drg/SupIntDMdn stands for the median distance between
the corresponding types of molecules. The example is made from Panobistat and a
supporting substance (in ratio 2:1), units are Å.
The code to map atom positions in PDB files into 4D numpy arrays (3D positional
23

Vid Sustar

3 METHODS

data + timepoints) and to measure the distances statistics, can be found in listing.

3.4

Technical DL definitions used

The dataset is split into three partitions: The largest part, around 80 percent, of the
dataset is used for training of the neural networks. The weights and biases of the
neural network are being updated with a training subset of the dataset. About 10
percent of the whole dataset is partitioned into a validation dataset. The validation
dataset is used for evaluation of the model after predictions and tuning of the model’s
hyperparameters. Also, about 10 percent of the whole dataset is dedicated to a test
subset. The test subset is only used once the training is completely finalised for the
final evaluation. The ratios of training:validation:test can change in accordance to
the size of the whole dataset. For large datasets with over 1 million samples, the
ratio would be closer to 98:1:1 for training:validation:test, respectively [26].
Out of the many features in the data some have true correlations, whereas some
features only appear to be correlated. However, these correlations are actually random. Training data may not be representative in its features in regard to testing
data. If this is the case, training the model might capture the random correlations
or noise which are occurring only within training data. When the model fits very
well only to training data, but not to more general data, this is called overfitting. In
contrast, when true correlations are not present well enough or the training is too
short, the model cannot capture these correlations. This is called underfitting. In
both cases, new datasets cannot be predicted well with under or overfit models [27].
To reduce overfitting a special technique called dropout is used, which drops out
units and their connections during training randomly. Another approach to reduce
overfitting is batch normalization, which normalizes the input layer by adjusting and
scaling the activations. The data is fed into the neural networks in smaller parts as
it cannot fit all at once due to memory constraints. These parts of the data that are
fed at once are called batches. An epoch is one forward and backward passage of the
whole training dataset through the neural network.

3.5

Models used

The general architecture used is composed of concatenated 3D CNN for NP structure
input and dense layers for other types of input fed into Long Short-Term Memory
(LSTM) RNN, as presented in figure 23.

24

Vid Sustar

3 METHODS

Figure 23: Graphical representation of model architecture inputs, concatenations and
DL network components.
This model is composed of four types of input:
1. 3D positions of atoms in NP are input into two 3D convolutional layers with
60x60x60 input units. The 3D convolution layer has kernels of size three. First
convolutional layer has 32 kernels, which double in each consecutive convolutional layer. Greater the number of different kernels greater number of distinct
features can be detected.
Each 3D convolution layer is followed by 3D max pooling layer, batch normalization layer and dropout layer with the dropout rate of 0.5 added to prevent
overfitting. These are followed by a flattening layer to get a one-dimensional
vector to be more easily combined with other input. The resulting vector is
relatively large, so an additional dense layer is added to reduce the 3D input
to the size order of other inputs.
2. SASA drug exposure input which is fed into a dense layer of the same size as
the input.
25

Vid Sustar

3 METHODS

3. Average distances of centres of molecules from the centre of the NP and distances between atoms which are fed into a dense layer of the same size as the
input.
4. Drug type used and the ratio of drug vs supporting substance, also here the
input is fed into the same-sized dense layer as the input.
All of the above layers are time distributed as they are input to LSTM layer, with
timepoints distributed over time, each input in separate LSTM cell. The inputs cover
75 timepoints.
As can be seen in figure 24, the 3D convoluted layers are concatenated to dense
layer with “drug exposure” input. The concatenated output of the aforementioned
is, in turn, concatenated to a dense layer with “distances” input. This concatenated
output is finally concatenated to a dense layer from “drug type” input. The final
concatenation is the input into a single LSTM layer which is an output to two dense
layers. The LSTM layer had 256 neurons, as well as the following dense layer. The
second dense layer had 170 neurons. The very last layer is again a time distributed
dense layer, to distribute the output in timely fashion to predict the SASA exposure
at different timepoints in the future. The model architecture is shown in figure 24.
Adam (derived from ADAptive Moment estimation) was used as an adaptive
learning rate optimisation algorithm. The Python script used to build, train and use
the model for predictions can be found in 7.

26

Vid Sustar

3 METHODS

Figure 24: Abstraction of a simple model architecture with its inputs and concatenations.

27

Vid Sustar

4

4 RESULTS

Results

To obtain optimal architecture of the DL model, several architectures were tested.
The basic architecture was composed of two 3D convolutional layers concatenated to
other input, as described in subsection 3.5, but without the input of the calculated
average distances between the types of residues and the average distances from the
centre of the NP.
One epoch contained about 11 000 batches, each containing one training sample.
The loss function on training data and validation data was observed after each epoch.
The model summary can be observed in figure 25. After ten epochs, the training loss
was 4.8 and the validation loss was 7.7 (MAE, % of drug vs total NP SASA), as can
be seen in figure 26.

Figure 25: Model summary of the most basic architecture.

28

Vid Sustar

4 RESULTS

Figure 26: Training performance with basic input, averaged over 100 timepoints.
To improve the loss function, also the calculated average distances between the
types of residues and the average distances from the centre of the NP input were
included as input. This training resulted in lower validation and training loss after
ten epochs, 4.5 and 4.3 MAE, respectively, compared to the training without distances
input. The loss per epoch is shown in figure 26.

Figure 27: Training performance with basic averaged input with added calculated
distances input.
To show the effect of smoothing (averaging) of the input data as a comparison,
29

Vid Sustar

4 RESULTS

also the training on the non-averaged data was performed. After ten epochs the
validation and training loss were 6.0 and 6.0 MAE, respectively, as can be seen in
figure 28.

Figure 28: Training performance with added calculated distances input and no averaging.
Next, the effect of additional 3D convolutional layers was examined. After the
initial two 3D convolutional layers each followed by a 3D max pooling layer, a batch
normalisation layer and a dropout layer, also a third convolutional layer with aforementioned auxiliary layers was added. This type of architecture resulted in the
validation and training loss to be 0.8 and 1.1 MAE, respectively, after ten epochs, as
can be seen in figure 29.

30

Vid Sustar

4 RESULTS

Figure 29: Training performance with added calculated distances input, averaging
and an additional 3D convolutional layer.
Additional decrease of loss function was achieved by adding the fourth 3D convolutional layer. Such architecture resulted in the validation and training loss to be
0.7 and 0.8 MAE, respectively, after ten epochs, as can be seen in figure 30.

Figure 30: Training performance with added calculated distances input, averaging
and additional two 3D convolutional layers.
By adding the fifth 3D convolutional layer with 256 kernels, the training loss
31

Vid Sustar

4 RESULTS

remained the same, whereas the validation loss decreased slightly to 0.7 MAE after
ten epochs, as can be seen in figure 31.

Figure 31: Training performance with added calculated distances input, averaging
and additional three 3D convolutional layers.
The sixth layer of 3D convolution could not be applied, since already the five
layers of the 3D convolution (and corresponding dropout layers) reduced the number
of neurons to one per 3D matrix dimension and, thus, could not be convoluted and
reduced further.
Next, the changing of the dropout rate was examined. The purpose of the dropout
layer is to randomly set input units to zero with a given rate, which helps to prevent
overfitting. When changing the dropout rate from initial 0.5 to 0.2, the validation
and training loss were 0.6 and 0.8 MAE, respectively, after ten epochs, as can be seen
in figure 32.

32

Vid Sustar

4 RESULTS

Figure 32: As in figure 31, but with a dropout rate of 0.2 instead of 0.5.
The effect of the number of kernels was examined by increasing the number of
kernels in the last fifth 3D convolutional layer from 256 to 512. The dropout rate was
0.5. The model summary can be observed in figure 33. Such a setup of parameters
resulted in the validation and training loss of 0.7 and 0.5 MAE, respectively, after
ten epochs, as can be seen in figure 34. The 0.5 MAE of the validation loss after
ten epochs is an overall best performance for all the tested models. Additional ten
epochs lowered the validation and training loss to 0.6 and 0.4 MAE, respectively.

33

Vid Sustar

4 RESULTS

34
Figure 33: Model summary of the best performing
setup, performance of which can
be seen in figures 34 and 35.

Vid Sustar

4 RESULTS

Figure 34: As in figure 31, but with 512 kernels in the last 3D convolutional layer
instead of 256.

35

Vid Sustar

4 RESULTS

Figure 35: Performance of the model from figures 33 and 34 after 20 epochs training,
the average of 14 predictions of drug vs whole NP SASA percentage in red with one
standard deviation in gray and ground truth in blue. The 14 predictions were based
on the 14 quadrants with different angles of the same NP composition, which were
not used in the training process. The NP is composed of Panobistat and a supporting
substance (2:1).
The effect of the additional dense layer with 170 neurons after LSTM layer, was
examined. Training on this architecture resulted in the validation and training loss
of 0.7 and 0.9 MAE, respectively, after ten epochs, as can be seen in figure 36.

36

Vid Sustar

4 RESULTS

Figure 36: As in figure 34, but with an additional dense layer with 170 neurons after
LSTM.
Next, the effect of the type of loss function on the training was inspected. A loss
function commonly used for regression is the mean squared error (MSE). To be able
to compare this to the previous results, also MAE was calculated. The usage of an
MSE as a loss function resulted in the validation and training loss of 0.8 and 1.8
MSE, respectively, and validation and training loss of 0.7 and 1.1 MAE, respectively,
as can be seen in figure 37.

37

Vid Sustar

4 RESULTS

Figure 37: As in figure 36, but with the mean squared error (MSE) for loss function
instead of the mean average error (MAE). MAE was calculated in addition for easier
comparison.
Finally also the test of possible smaller number of input timpoints to predict
also smaller number of final timepoints was examined. The goal of the study was
to shorten the simulations as much as possible and be able to predict the final few
timepoints. When only first 10 timepoints as opposed to 75 were used as input to
predict last 10 timepoints (as opposed to 75), the validation and training loss after
10 epochs were 0.7 and 1.1 of MAE respectively. The result can be seen in figure 38.

38

Vid Sustar

4 RESULTS

Figure 38: As in figure 34, but with 10 timepoints as prediction for 10 last timepoints
SASA ratio, as opposed to 75 in prior cases.

39

Vid Sustar

5

5 DISCUSSION

Discussion

The aim of this thesis was to establish a set of procedures to prepare the input data
for a deep learning model and to develop the deep learning model itself, which would
predict the properties of a medical nanoparticle.
The aim of the running molecular dynamics simulations was achieved. In total,
107 simulations were run. Of these, 63 simulations of nanoparticles were run in basic
conditions by 27 degrees Celsius and without added ions. The NPs were covered with
seven different drugs in various ratios to the supporting substances. Additional 45
simulations were done with the same drugs, but with smaller number of ratios (3:1,
1:1, 1:3) for 27 degrees with ions and 37 degrees with and without ions present in
the NP solution. The aforementioned 63 simulations in basic conditions were used
for the creation of the deep learning dataset.
In order to create a deep learning dataset, first the comparable solutions used by
other researchers were examined. As the goal was to extract features from 3D objects
(nanoparticles), one of the closest experiments where 3D convolution was used was
finding tumours in computer tomography scans of lungs (citation). There, however,
the largest 3D images used were maximally twenty 8-bit voxels in each dimension and
no time dimension was used that could additionally increase the size of an individual
sample.
Due to a previous experience in the usage of a software for the visualisation of large
macromolecular structures, Chimera software was used to visualise the nanoparticles.
A script was created in Python to slice through a nanoparticle in stepwise fashion
in Chimera, while saving each clip as a separate image, as can be seen in figure
39. However, after optimising a script, the output was not satisfactory. The depth
resolution could not reach the resolution of an x,y -plane. The properties of a human
friendly visualisation of objects in Chimera created artifacts that added noise and
could interfere with the deep learning method. For example, the atoms are coloured
with a set colour, however, their edges change the intensity depending on the depth
of a slice, which can not be zero. In addition, the created set of slices per each
timepoint of the simulation required both a large hard disk storage capacities as well
as a large running memory for the graphics card. The optimisation of this approach
required time and effort that could have been used for the modeling itself.

40

Vid Sustar

5 DISCUSSION

Figure 39: The initial approach of representing nanoparticles in 3D space as a stack
of 2D images made with a combination of Python and Chimera. The approach is
similar to the 3D CT scans of lungs as a known case of 3D convolutional DL used
for detecting tumours. Left is top-down view, right is side view.
The impasse of representing the nanoparticle as a stack of slices led to a search
for an alternative, more direct approach to map the atomic coordinates from pdb
files directly into a 3D matrix representation. The solution found was a Python
library named BioPandas and its submodule PandasPdb. This approach allowed
first the loading of the pdb files as pandas dataframes and then selective filtering
of the atoms of interest. The atom coordinates from the pdb are translated into a
position in the 3D matrix. Similarly as colour coding in an image representation of a
nanoparticle in figure 1, the value stored in a given matrix position could encode the
type of the atom, whether it is a part of the drug molecule itself, a part of the PEG
chain connecting the drug to the gold core or the supporting substance or its PEG
chain. The encoding of such properties was also possible with the combined pandas
dataframe and the numpy matrix approach, as can be seen in figure 18.
To perform the direct mapping of the atomic coordinates into the 3D numpy
matrix via PandasPdb, the matrix of a sufficient size had to be created in order
not to overwrite the already occupied cells in the matrix with the nearby atoms.
Empirically, the size of 200 was concluded. However, as already mentioned in 3.3.2,
the produced input was too large to be fed into the 3D convolutional LSTM model.
Therefore, several actions of additional pre-processing were taken. Firstly, only the
geometrically central atoms of the molecules were considered to be mapped into 3D
matrices. This reduced the minimal non-overlapping matrix size to 120. As this step
was not sufficient, additionally only the individual quadrants of the matrix were fed
separately into the neural network. The quadrants of the same matrix were flipped
for all to match the same orientation as to normalise them. The individual quadrant
matrix size was reduced to 60 cells in each dimension, which allowed the training of
the deep learning model on the graphs cards at disposal.
The separation of the matrix into eight quadrants produced an eight-fold increase
of the input data. Additional increase of the data was augmentation of the whole
41

Vid Sustar

5 DISCUSSION

NP atom coordinates prior to the mapping into a matrix by incrementally rotating
the NP (atomic coordinates) by a small angle. Through these multiplicative steps,
the total number of samples was increased by 360-fold.
The goal of the Evonano project is to develop nanoparticles that deliver antitumour drugs to the site of the tumour. It is desirable that as much drug as possible
is delivered per nanoparticle. The varying drugs used can have varying levels of
hydrophobicity, which lowers their exposure to a water-based environment, like blood,
as well as to the targeted receptors of the tumour. To compensate these properties,
a supporting substance is added to a nanoparticle. By empirically trying out various
ratios of drug vs supporting substance, the optimal ratio is obtained.
At the beginning of the study, the metric for the drug exposure given by our
Serbian collaborators was the ratio of the drug molecules which had no neighbouring
molecules in a given radius vs those that did. Later, this metric was replaced by the
solvent accessible area (SASA) of a drug relative to SASA of the whole nanoparticle.
This thesis developed a non-GUI automatised approach to calculate this metric based
on instructions from the aforementioned collaborators. This metric is more widely
used and faster to calculate.
SASA ratio is the main objective of the DL prediction. If it were possible to
predict the SASA ratio after 300 ns of simulations based on the first few timepoints
(ns), then the actual simulation could be shortened to those few initial timepoints.
In the first models (not presented), only the next timepoints were predicted. Those
models performed relatively well. However, as already mentioned, the goal of this
thesis was to predict the last portion of the simulation based on the first portion.
The last model settled to the 75 last timepoints based on the first 75 points, so that
the first timepoint would be predictive of the 226th timepoint. The reason for this
number of timepoints was the limitations of the graphics card memory.
Effects of preprocessing were examined, where the input was averaged with 100
timepoint moving average and compared to non-averaged unprocessed input. The
smoothing of data by averaging showed to have improved the learning rate and
resulted in a smaller validation loss.
To find the optimal architecture number of 3D convolutional layers and corresponding pooling, batch normalisation and dropout layers was examined. Adding
maximum possible number of 3D convolutional layers, given the input dimensions
prooved to be best performing. The initial dimensions of one quadrant matrix were
60x60x60, after each 3D convolutional layer the dimension was reduced by two, and
after each max pooling layer by half, which allowed for five 3D convolutional and
max pooling layers to finaly extract the matrix dimensions to 1x1x1.
The dropout rate was examined as it helps to prevent overfitting higher dropout
rate of 0.5 resulted in smaller validation loss compared to lower dropout rate of 0.2.
Greater number of kernels in 3D convolutional layers resulted in smaller validation
loss, as well as using of mean average error comparred to mean squared error as loss
function.
Also the length of input in regard to number of timepoints was examined. The
goal of the study was to shorten the simulations as much as possible and be able
to predict the final few timepoints. Using 75 initial timepoints to predict last 75
timepoints resulted in smaller validation loss compared to 10 timepoint input-output.

42

Vid Sustar

5 DISCUSSION

The best result of predicting the SASA ratio in the last 75 timepoints of the
simulation based on the first 75 timepoints (or 226 timepoints in the future), can be
seen in figure 35. This is still far from the level where it could be used beside the
simulations itself. It does, however, show a similar trend in the decline of the SASA
ratio through time. It was also improved with a larger number of learning epochs. It
might be possible to improve it further with an even larger number of epochs. If the
memory limitations could be overcome to input whole NP matrices, the prediction
might improve as well, since the range of error (standard deviation) observed in figure
35 stems from the predictions of 14 quadrants at different angles of the same NP.
Also, the simplification of the constituent molecules of the nanoparticles to their
central atoms might have brought a penalty to the predictive power of the model.
The model could be upgraded to include an attention layer that would report on
which input had the greatest contribution in the prediction of the SASA ratio. This
would create so called explainable artificial intelligence (XAI), which is on the rise
for its understandability, as opposed to the classical ”black box” deep learning.

43

Vid Sustar

6

6 CONCLUSIONS

Conclusions

The main aims of the study were achieved:
1. Running of molecular dynamics simulations on CSC and local cluster, gathering
and sharing of obtained MDS trajectories.
The basis code for running MDS on CSC cluster was preset. In this study the
transfer to running MDS on local cluster with 24 GPUs was made, which allows
much greater throughput in MDS.
2. Processing of the MDS trajectories into the input of interest.
MDS trajectories were used with VMD software to calculate the SASA ratio
of drug vs whole NP at given time resolution. MDS trajectories were used
to create PDB lists of atomic positions in chosen timepoints. PDB lists were
the source to create 3D matrix representation of NP via PandasPdb as input
for 3D convolution. Additionally PDBs were used to calculate the radial and
inter-residue distances as an additional abstracted information input for the
final model.
3. Selection of deep learning approaches and their fusion into one architecture.
3D convolutional layers were used to extract features from 3D matrix NP representation and were concatenated to other linear vector input as SASA ratios,
distances and drug types. The concatenated input was fed into LSTM layer.
4. Optimisation of the concatenated 3D convolutional-LSTM DL model was made.
Several versions of the model were tried with assesment of validation loss function. The architecture and parameters with lowest loss function/fastest learning
at set number of epochs was identified.
In the future, an attention layer could be added to create explainable AI to allow
comprehension of the most important contributing input in DL and also other more
sophisticated types of LSTM could be considered, such as social LSTM, to predict
the behaviour of individual atoms or molecules.

44

Vid Sustar

REFERENCES

References
[1] Jemal A, Siegel R, and Ward E. Cancer statistics, 2008. CA Cancer J Clin, 58
:(2):71–96, 2008.
[2] Doll R and Peto R. The causes of cancer: quantitative estimates of avoidable
risks of cancer in the united states today. J. Natl. Cancer Inst.,
66(6):1191–308., 1981.
[3] Evolvable platform for programmable nanoparticle-based cancer therapies.
https://cordis.europa.eu/project/id/800983, 2018. Accessed: 2021-01-21.
[4] Tom Mitchell. Machine Learning. McGraw Hill, 1997.
[5] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review
and new perspectives. IEEE Transactions on Pattern Analysis and Machine
Intelligence., 35(8):1798–1828, 2013.
[6] Adam H. Marblestone, Greg Wayne, and Konrad P. Kording. Toward an
integration of deep learning and neuroscience. Frontiers in Computational
Neuroscience, 10:94, 2016.
[7] G. Cybenko. Approximation by superpositions of a sigmoidal function.
Mathematics of Control, Signals, and Systems, 4:303–314, 12 1989.
[8] Wei Hu. Towards a real quantum neuron. Natural Science, 10:99–109, 01 2018.
[9] Qin T. Deep Learning Basics. In: Dual Learning. Springer, 2020.
[10] Agnes Sauer. Quick guide to gradient descent and it’s variants.
https://morioh.com/p/15c995420be6, 2020. Accessed: 2021-02-21.
[11] Kevin P. Murphy. Machine learning : a probabilistic perspective. MIT Press,
Cambridge, Mass. [u.a.], 2013.
[12] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic
optimization, 2017.
[13] D. Cornelisse. An intuitive guide to convolutional neural networks.
https://medium.com/free-code-camp/an-intuitive-guide-toconvolutionalneural-networks-260c2de0a050, 2018. Accessed:
2021-01-21.
[14] Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for
deep learning. https://arxiv.org/abs/1603.07285, 2018.
[15] Sebastian Raschka and Vahid Mirjalili. Python Machine Learning: Machine
Learning and Deep Learning with Python, Scikit-Learn, and TensorFlow, 2nd
Edition. Packt Publishing, 2nd edition, 2017.

45

Vid Sustar

REFERENCES

[16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning, Adaptive
computation and machine learning. MIT Press, 2016.
http://www.deeplearningbook.org.
[17] J. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. diploma
thesis, institut fur informatik, lehrstuhl prof. brauer, technische universitat
munchen. www7.informatik.tu-muenchen.de/ hochre/, 1991.
[18] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
computation, 9:1735–80, 12 1997.
[19] C. Olah. colah’s blog: Understanding lstm networks.
http://colah.github.io/posts/2015-08-Understanding-LSTMs/, 2015.
[20] Divyanshu Thakur. Lstm and its equations.
https://medium.com/@divyanshu132/lstm-and-its-equations-5ee9246d04af,
2018.
[21] Aditi Sinha. Understanding of lstm networks.
https://www.geeksforgeeks.org/understanding-of-lstm-network, 2020.
[22] Romelia Salomon-Ferrer, David A. Case, and Ross C. Walker. An overview of
the amber biomolecular simulation package. WIREs Computational Molecular
Science, 3(2):198–210, 2013.
[23] William Humphrey, Andrew Dalke, and Klaus Schulten. Vmd: Visual
molecular dynamics. Journal of Molecular Graphics, 14(1):33–38, 1996.
[24] Huang CC Meng EC Couch GS Croll TI Morris JH Ferrin TE. Pettersen EF,
Goddard TD. Ucsf chimerax: Structure visualization for researchers,
educators, and developers. Protein Sci., 30(1):70–82, 2021.
[25] Abigail Held and Maria Nagan. Running md with pmemd.
https://ambermd.org/tutorials/basic/tutorial14/index.php, 2020.
[26] Goodacre R Xu Y. On splitting training and validation set: A comparative
study of cross-validation, bootstrap and systematic sampling for estimating the
generalization performance of supervised learning. J Anal Test, 3(2):249–262,
2018.
[27] Tom Dietterich. Overfitting and undercomputing in machine learning. ACM
Comput. Surv., 27(3):326–327, September 1995.

46

Vid Sustar

7

7 APPENDIX

Appendix

The used code
1
2

export syn = ’ PegCY5_11 . solv ’
export inpcrd = ’ PegCY5_11 . solv ’

3
4
5

export C U D A _ V I S I B L E _ D E V I C E S =1
export NCPU =20

6
7
8
9

mkdir -p equil
mkdir -p prod
mkdir -p pre_equil

10
11

12
13
14
15
16

mpirun - np $NCPU - mca btl ^ openib pmemd . MPI -O -p " $syn " . prmtop -i
./ mdin / min_sol . in \
-o ./ pre_equil / min_sol_ " $syn " . out \
-x ./ pre_equil / min_sol_ " $syn " . nc \
-r ./ pre_equil / min_sol_ " $syn " . rst \
-c
" $inpcrd " . inpcrd \
- ref " $inpcrd " . inpcrd

17
18

19
20
21
22
23

mpirun - np $NCPU - mca btl ^ openib pmemd . MPI -O -p " $syn " . prmtop -i
./ mdin / min_all . in \
-o ./ pre_equil / min_all_ " $syn " . out \
-x ./ pre_equil / min_all_ " $syn " . nc \
-r ./ pre_equil / min_all_ " $syn " . rst \
-c
./ pre_equil / min_sol_ " $syn " . rst \
- ref ./ pre_equil / min_sol_ " $syn " . rst

24
25

26
27
28
29
30

mpirun - np $NCPU - mca btl ^ openib pmemd . MPI -O -p " $syn " . prmtop -i
./ mdin / heat . in \
-o ./ pre_equil / heat_ " $syn " . out \
-x ./ pre_equil / heat_ " $syn " . nc \
-r ./ pre_equil / heat_ " $syn " . rst \
-c
./ pre_equil / min_all_ " $syn " . rst \
- ref ./ pre_equil / min_all_ " $syn " . rst

31
32

33
34
35
36
37

mpirun - np $NCPU pmemd . MPI -O -p " $syn " . prmtop -i ./ mdin / density . in
\
-o ./ pre_equil / density_ " $syn " . out \
-x ./ pre_equil / density_ " $syn " . nc \
-r ./ pre_equil / density_ " $syn " . rst \
-c
./ pre_equil / heat_ " $syn " . rst \
- ref ./ pre_equil / heat_ " $syn " . rst

38
39

40
41
42
43
44

mpirun - np $NCPU pmemd . MPI -O -p " $syn " . prmtop -i ./ mdin / density2 . in
\
-o ./ pre_equil / density2_ " $syn " . out \
-x ./ pre_equil / density2_ " $syn " . nc \
-r ./ pre_equil / density2_ " $syn " . rst \
-c
./ pre_equil / density_ " $syn " . rst \
- ref ./ pre_equil / density_ " $syn " . rst

45
46
47
48

pmemd . cuda -O -p " $syn " . prmtop -i ./ mdin / equil . in \
-o ./ equil / equil1_ " $syn " . out \

47

Vid Sustar

49
50
51
52
53
54
55
56
57
58
59
60
61
62
63

7 APPENDIX

-x ./ equil / equil1_ " $syn " . nc \
-r ./ equil / equil1_ " $syn " . rst \
-c
./ pre_equil / density2_ " $syn " . rst \
- ref ./ pre_equil / density2_ " $syn " . rst
export x =2
while [[ $x - le 10 ]] ; do
let y =x -1
pmemd . cuda -O -p " $syn " . prmtop -i ./ mdin / equil . in \
-o ./ equil / equil " $x " _ " $syn " . out \
-x ./ equil / equil " $x " _ " $syn " . nc \
-r ./ equil / equil " $x " _ " $syn " . rst \
-c
./ equil / equil " $y " _ " $syn " . rst \
- ref ./ equil / equil " $y " _ " $syn " . rst
let x +=1
done

64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81

pmemd . cuda -O -p " $syn " . prmtop -i ./ mdin / NPT_BER . in \
-o ./ equil / NPT_BER1_ " $syn " . out \
-x ./ equil / NPT_BER1_ " $syn " . nc \
-r ./ equil / NPT_BER1_ " $syn " . rst \
-c
./ equil / equil10_ " $syn " . rst \
- ref ./ equil / equil10_ " $syn " . rst
export x =2
while [[ $x - le 8 ]] ; do
let y =x -1
pmemd . cuda -O -p " $syn " . prmtop -i ./ mdin / NPT_BER . in \
-o ./ equil / NPT_BER " $x " _ " $syn " . out \
-x ./ equil / NPT_BER " $x " _ " $syn " . nc \
-r ./ equil / NPT_BER " $x " _ " $syn " . rst \
-c
./ equil / NPT_BER " $y " _ " $syn " . rst \
- ref ./ equil / NPT_BER " $y " _ " $syn " . rst
let x +=1
done

82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99

pmemd . cuda -O -p " $syn " . prmtop -i ./ mdin / NPT_MCeq . in \
-o ./ equil / NPT_MC1_ " $syn " . out \
-x ./ equil / NPT_MC1_ " $syn " . nc \
-r ./ equil / NPT_MC1_ " $syn " . rst \
-c
./ equil / NPT_BER8_ " $syn " . rst \
- ref ./ equil / NPT_BER8_ " $syn " . rst
export x =2
while [[ $x - le 5 ]] ; do
let y =x -1
pmemd . cuda -O -p " $syn " . prmtop -i ./ mdin / NPT_MCeq . in \
-o ./ equil / NPT_MC " $x " _ " $syn " . out \
-x ./ equil / NPT_MC " $x " _ " $syn " . nc \
-r ./ equil / NPT_MC " $x " _ " $syn " . rst \
-c
./ equil / NPT_MC " $y " _ " $syn " . rst \
- ref ./ equil / NPT_MC " $y " _ " $syn " . rst
let x +=1
done

100
101
102
103
104

pmemd . cuda -O -p " $syn " . prmtop -i ./ mdin / NPT_MC . in \
-o ./ prod / NPT_MC1_ " $syn " . out \
-x ./ prod / NPT_MC1_ " $syn " . nc \
-r ./ prod / NPT_MC1_ " $syn " . rst \

48

Vid Sustar

105
106
107
108
109
110
111
112
113
114
115
116
117

7 APPENDIX

-c
./ equil / NPT_MC5_ " $syn " . rst \
- ref ./ equil / NPT_MC5_ " $syn " . rst
export x =2
while [[ $x - le 60 ]] ; do
let y =x -1
pmemd . cuda -O -p " $syn " . prmtop -i ./ mdin / NPT_MC . in \
-o ./ prod / NPT_MC " $x " _ " $syn " . out \
-x ./ prod / NPT_MC " $x " _ " $syn " . nc \
-r ./ prod / NPT_MC " $x " _ " $syn " . rst \
-c
./ prod / NPT_MC " $y " _ " $syn " . rst \
- ref ./ prod / NPT_MC " $y " _ " $syn " . rst
let x +=1
done

Listing 1: Example script to run MDS with Amber PMEMD on CSC cPouta Virtual
Machine
1
2

touch " ../ vmd_SASA_calc . tcl " # creating a command text file with
commands to be executed within VMD

3
4
5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

# printing the lines ( commands ) into command text file
echo " mol new { prod / $FN . solv . prmtop } type { parm7 } first 0 last -1
step 1 waitfor 1 " >> " ../ vmd_SASA_calc . tcl " # opening the
trajectory file
echo " mol addfile { prod / NPT_MC1_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC2_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC3_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC4_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC5_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC6_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC7_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC8_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC9_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC10_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC11_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC12_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC13_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC14_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC15_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC16_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "

49

Vid Sustar

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36
37
38

39
40

41

42
43

44
45

46

47

48
49

50
51
52
53

54

7 APPENDIX

echo " mol addfile { prod / NPT_MC17_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC18_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC19_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC20_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC21_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC22_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC23_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC24_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC25_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC26_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC27_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC28_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC29_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " mol addfile { prod / NPT_MC30_$FN . solv . nc } type { netcdf } first 0
last -1 step 10 waitfor all " >> " ../ vmd_SASA_calc . tcl "
echo " set selmode all " >> " ../ vmd_SASA_calc . tcl "
echo " set all [ atomselect top all ] " >> " ../ vmd_SASA_calc . tcl "
echo ’ set drg [ atomselect top " resname ’ $DRUG_NAME ’ "] ’ >> " ../
vmd_SASA_calc . tcl "
echo " set n [ molinfo top get numframes ] " >> " ../ vmd_SASA_calc . tcl "
echo ’ set output [ open " SASA_$selmode . csv " w ] ’ >> " ../ vmd_SASA_calc .
tcl "
echo ’
puts $output " timepoint , SASArestricted , SASAtotal ,
SASAperc " ’ >> " ../ vmd_SASA_calc . tcl "
# sasa calculation loop
echo " for { set i 0} {\ $i < \ $n } { incr i } { " >> " ../ vmd_SASA_calc . tcl
"
echo "
molinfo top set frame \ $i " >> " ../ vmd_SASA_calc . tcl "
echo "
set sasar [ measure sasa 1.4 \ $all - restrict \ $drg ] " >> "
../ vmd_SASA_calc . tcl "
echo "
set sasat [ measure sasa 1.4 \ $drg ] " >> " ../ vmd_SASA_calc
. tcl "
echo "
set sasaperc [ expr {(\ $sasar / \ $sasat ) *100}] " >> " ../
vmd_SASA_calc . tcl "
echo "
set ione [ expr {\ $i +1}] " >> " ../ vmd_SASA_calc . tcl "
echo ’
puts $output " $ione , $sasar , $sasat , $sasaperc " ’ >> " ../
vmd_SASA_calc . tcl "
echo ’
’ >> " ../ vmd_SASA_calc . tcl "
echo " } " >> " ../ vmd_SASA_calc . tcl "
echo ’ puts " Done ." ’ >> " ../ vmd_SASA_calc . tcl "
echo ’ puts " output file : SASA_GEM_test_ \ $selmode . csv " ’ >> " ../
vmd_SASA_calc . tcl "
echo " close \ $output " >> " ../ vmd_SASA_calc . tcl "

50

Vid Sustar

55
56
57
58
59

7 APPENDIX

echo " mol delete 0 " >> " ../ vmd_SASA_calc . tcl "
echo " exit " >> " ../ vmd_SASA_calc . tcl "
cd ..
echo " 2 pwd "
pwd

60
61
62

vmd - dispdev text -e vmd_SASA_calc . tcl

63
64

end = ‘ date +% s ‘; runtime = $ (( end - start ) ) ; hours = $ (( runtime / 3600) ) ;
minutes = $ (( ( runtime % 3600) / 60 ) ) ; seconds = $ (( ( runtime %
3600) % 60 ) ) ;

65
66

echo " Runtime : $hours : $minutes : $seconds ( hh : mm : ss ) - VMD SASA
exposure " >> " exec_report . txt "

Listing 2: Part of trajectory processing Bash script that calculates NP SASA drug
exposures via VMD in non GUI mode. Bash script first writes a custom TCL script
which is in turn read by VMD.
1
2
3
4

import
import
import
import

pandas as pd
numpy as np
glob
csv

5
6
7
8
9

path = ’/ home / user / Downloads / MastersNNinput / NDL / SASA100 / ’
input_files_csv = sorted ( glob . glob ( path + " *. csv " ) )
wind =100

10
11
12
13

for k , filename_csv in enumerate ( input_files_csv ) :
print ( " FILENAME : " , filename_csv )
print ( " K : " ,k )

14
15
16
17
18
19
20
21

22
23

24
25

fn_idx_strt = filename_csv . rfind ( ’/ ’)
print ( " Substring ’ csv ’ found at index : " , fn_idx_strt )
fn_idx_end = filename_csv . find ( ’. csv ’)
print ( " Substring ’ csv ’ found at index : " , fn_idx_end )
df = pd . DataFrame ( pd . read_csv ( filename_csv ) )
for i in range ( len ( df . columns ) -1) :
df . iloc [: , i +1] = df . iloc [: , i +1]. rolling ( window = wind ,
min_periods =1) . mean ()
df . head ()
filename_csv_avg = filename_csv [: fn_idx_end ] + ’ _avg_ ’+ str (
wind ) + filename_csv [ fn_idx_end :]
print ( filename_csv_avg )
df . to_csv ( filename_csv_avg , sep = ’ , ’ , encoding = ’utf -8 ’ , index
= False )

Listing 3: Python script to smoothen SASA drug exposure with moving average
1
2
3
4

# !/ bin / bash
# prior to execution run :
# chmod + x pdb_auto_08 . sh
# sudo chmod 777 pdb_auto_08 . sh

5
6

# author : Vid Sustar

51

Vid Sustar

7 APPENDIX

7
8
9
10
11

# this script :
# 1. copies given folder
# 2. creates pdbs
# 3. renames , copies pdbs

12
13
14

# ./ pdb_auto_07 . sh NCL12 NCL 2.2 10 2 1 2 200
# new way of exposure calculations with VMD

15
16

# do screen

17
18

19

INPUT_FOLDER = $1 # assigning the variable for drug folder name from
command line input parameter #1
DRUG_NAME = $2 # initialising the variable for drug name ( pdb name ,
later also for exposure accessibility calculations ) from command
line input parameter #2

20
21
22

echo " Input folder set to : " $INPUT_FOLDER # printing
echo " Drug set to : " $DRUG_NAME # printing

23
24

start = ‘ date +% s ‘ # to measure total execution time

25
26

cp -r " $INPUT_FOLDER " " $INPUT_FOLDER " _5 # copying the folder to new
name ( added _2 )

27
28
29

30
31
32

export AMBERHOME =/ home / cloud - user / amber
export AMBERHOME =/ home / cloud - user / amber / amber18 # if one path not
found , no problem , script will continue to execute
source / home / cloud - user / amber / amber . sh
source / home / cloud - user / amber / amber18 / amber . sh
export CUDA_HOME =/ usr / local / cuda

33
34

35
36
37
38
39
40
41
42
43
44
45
46
47
48

49

50
51
52

53

pattern = " $INPUT_FOLDER /*. solv . prmtop " # all the files in folder found
with this extension
files =( $pattern )
FN = " $ { files [0]} " # first one
echo " pattern set to : " $pattern # printing
echo " files set to : " $files # printing
echo " FN init set to : " $FN # printing
FN = $files
FN = $ { FN # #*/} # taking just the part after the path /
FN = $ { FN %.*} # taking the part in front of last .
FN = $ { FN %.*} # as above
echo " FN finaly set to : " $FN # printing
cd " $INPUT_FOLDER " _5
pwd
# Create input file for cpptraj7
touch " ../ cpp_traj_input . txt " # creating a command text file with
commands to be executed within cpptraj
echo " set sysname = " $FN >> " ../ cpp_traj_input . txt " # printing the
lines ( commands ) into command text file
echo " parm $FN . solv . prmtop " >> " ../ cpp_traj_input . txt "
# 1. batch of commands
echo " trajin equil / equil ? _$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin equil / equil ?? _$FN . solv . nc 1 last 1 " >> " ../

52

Vid Sustar

54
55

56
57
58

59
60
61
62

63

7 APPENDIX

cpp_traj_input . txt "
echo " autoimage " >> " ../ cpp_traj_input . txt "
echo " trajout equil / equil_2ns_100psf . $FN . solv . nc offset 5 " >> " ../
cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " strip : WAT outprefix noWAT " >> " ../ cpp_traj_input . txt "
echo " trajout equil / equil_2ns_20psf . $FN . noWAT . nc " >> " ../
cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " clear trajin " >> " ../ cpp_traj_input . txt "
echo " exit " >> " ../ cpp_traj_input . txt "
cat ../ cpp_traj_input . txt | cpptraj # running cpptraj with above
cpptraj txt command file
rm ../ cpp_traj_input . txt # removing the command . txt file

64
65
66

67

68
69
70

71
72

73
74
75

76
77
78
79

80

touch " ../ cpp_traj_input . txt " # creating a command text file with
commands to be executed within cpptraj
echo " set sysname = " $FN >> " ../ cpp_traj_input . txt " # printing the
lines ( commands ) into command text file
echo " parm $FN . solv . prmtop " >> " ../ cpp_traj_input . txt "
# 2. batch of commands
echo " trajin equil / NPT_BER ? _$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " autoimage " >> " ../ cpp_traj_input . txt "
echo " trajout equil / NP T_B ER _8 ns _5 00 ps f . $FN . solv . nc offset 5 " >> " ../
cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " strip : WAT outprefix noWAT " >> " ../ cpp_traj_input . txt "
echo " trajout equil / NP T_B ER _8 ns _1 00 ps f . $FN . noWAT . nc " >> " ../
cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " clear trajin " >> " ../ cpp_traj_input . txt "
echo " exit " >> " ../ cpp_traj_input . txt "
cat ../ cpp_traj_input . txt | cpptraj # running cpptraj with above
cpptraj txt command file
rm ../ cpp_traj_input . txt # removing the command . txt file

81
82
83

84

85
86
87

88
89

90
91
92

93
94

touch " ../ cpp_traj_input . txt " # creating a command text file with
commands to be executed within cpptraj
echo " set sysname = " $FN >> " ../ cpp_traj_input . txt " # printing the
lines ( commands ) into command text file
echo " parm $FN . solv . prmtop " >> " ../ cpp_traj_input . txt "
# 3. batch of commands
echo " trajin equil / NPT_MC ? _$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " autoimage " >> " ../ cpp_traj_input . txt "
echo " trajout equil / NP T_MC_5 ns_500 psf . $FN . solv . nc offset 5 " >> " ../
cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " strip : WAT outprefix noWAT " >> " ../ cpp_traj_input . txt "
echo " trajout equil / NP T_MC_5 ns_100 psf . $FN . noWAT . nc " >> " ../
cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " clear trajin " >> " ../ cpp_traj_input . txt "

53

Vid Sustar

95
96

97

7 APPENDIX

echo " exit " >> " ../ cpp_traj_input . txt "
cat ../ cpp_traj_input . txt | cpptraj # running cpptraj with above
cpptraj txt command file
rm ../ cpp_traj_input . txt # removing the command . txt file

98
99
100
101
102

103

104
105
106

107
108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

touch " ../ cpp_traj_input . txt " # creating a command text file with
commands to be executed within cpptraj
echo " set sysname = " $FN >> " ../ cpp_traj_input . txt " # printing the
lines ( commands ) into command text file
echo " parm $FN . solv . prmtop " >> " ../ cpp_traj_input . txt "
# 4. batch of commands
echo " set ProdRange = 000 -300 ns " >> " ../ cpp_traj_input . txt " # setting
the time - resolution , how many pdbs per ns
# instead of for loop , because it does not work :
echo " trajin prod / NPT_MC1_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC2_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC3_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC4_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC5_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC6_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC7_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC8_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC9_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC10_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC11_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC12_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC13_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC14_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC15_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC16_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC17_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC18_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC19_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC20_$FN . solv . nc 1 last 1 " >> " ../

54

Vid Sustar

128

129

130

131

132

133

134

135

136

137

138
139

140
141
142
143

144
145

7 APPENDIX

cpp_traj_input . txt "
echo " trajin prod / NPT_MC21_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC22_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC23_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC24_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC25_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC26_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC27_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC28_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC29_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajin prod / NPT_MC30_$FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " autoimage " >> " ../ cpp_traj_input . txt "
echo " trajout prod / NPT_MC_000 -300 ns_1nsf . $FN . solv . nc offset 10 " >> "
../ cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " strip : WAT outprefix noWAT " >> " ../ cpp_traj_input . txt "
echo " autoimage " >> " ../ cpp_traj_input . txt "
echo " trajout prod / N P T _ M C _ $ P r o d R a n g e _ 1 0 0 p s f . $FN . noWAT . nc " >> " ../
cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " clear trajin " >> " ../ cpp_traj_input . txt "

146
147

echo " exit " >> " ../ cpp_traj_input . txt "

148
149

150
151
152

153
154
155

156
157

cat ../ cpp_traj_input . txt | cpptraj # running cpptraj with above
cpptraj txt command file
rm ../ cpp_traj_input . txt # removing the command . txt file
# ###################################################################
echo " copying " $INPUT_FOLDER " _5 / " $FN . solv . prmtop " " $INPUT_FOLDER " _5 /
prod / "
pwd
echo " 1 "
cp -r " $INPUT_FOLDER " _5 / " $FN . solv . prmtop " " $INPUT_FOLDER " _5 / prod / #
copying the *. solv . prmtop to prod folder
echo " 5 "
cp -r $ ( pwd ) / $FN . solv . prmtop $ ( pwd ) / prod / $FN . solv . prmtop # ing the *.
solv . prmtop to prod folder

158
159
160
161
162
163
164

165

cd $ ( pwd ) / prod # executing cpptraj inside of prod folder
# cpptraj commands withing prod folder to produce pdbs there
echo " 1 pwd "
pwd
touch " ../ cpp_traj_input . txt " # creating a command text file with
commands to be executed within cpptraj
echo " set sysname = " $FN >> " ../ cpp_traj_input . txt " # printing the

55

Vid Sustar

166
167

168

169
170
171
172
173

174
175
176

7 APPENDIX

lines ( commands ) into command text file
echo " parm $FN . solv . prmtop " >> " ../ cpp_traj_input . txt "
echo " trajin NPT_MC_000 -300 ns_1nsf . $FN . solv . nc 1 last 1 " >> " ../
cpp_traj_input . txt "
echo " trajout $DRUG_NAME . pdb multi # exchange OQL with drug name " >>
" ../ cpp_traj_input . txt "
echo " go " >> " ../ cpp_traj_input . txt "
echo " exit " >> " ../ cpp_traj_input . txt "
echo " 2 pwd "
pwd
cat ../ cpp_traj_input . txt | cpptraj # running cpptraj with above
cpptraj txt command file
rm ../ cpp_traj_input . txt # removing the command . txt file
echo " 3 pwd "
pwd

Listing 4: Example Bash script to run Amber Cpp-traj to convert trajectory files
into PDB files on CSC cPouta Virtual Machine
1
2
3

print ( " INPUT FORMAT : ’ python3 SCRIPT ( pdb2mtrx09 . py ) DRUGNAME ( PAN )
RATIODRUG (2) RATIO SUPSUB STANCE (1) MTRXSIZE (50) ALSO - SUPD (1) ALSO OTHRESID (1) ’ " )

4
5
6
7
8
9
10

import sys
import h5py
import os , psutil , numpy as np
from biopandas . pdb import PandasPdb
import glob
import pandas as pd

11
12

from datetime import datetime

13
14

15

drug_list =[ ’ PAN ’ , ’ OQL ’ , ’ ZIL ’ , ’ NHQ ’ , ’ GEM ’] # ADD TO THE LIST ! make
sure same order and length for drug proximals
drugproximals =[ ’ OCP ’ , ’ OCQ ’ , ’ OCZ ’ , ’ OCN ’ , ’ OCG ’] # CHECK and add drug
proximal for NHQ !

16
17

n o n z e r o _ e l e m e n t s _ l i s t =[]

18
19

20

comp ressio n_leve l =9 # h5 file gzip level of compression , 9 is highest
- > smallest files , possibly slower opening
path = os . getcwd () + ’/ ’

21
22
23

24
25
26

27
28
29

# INPUT PARAMETERS
comp ressio n_leve l =9 # h5 file gzip level of compression , 9 is highest
- > smallest files , possibly slower opening
drugname = sys . argv [1] # finddrug ( filename ) # change appropriately
drug = drugname
rdrug = int ( sys . argv [2]) # int ( ratio [0]) # check SYS replace / uncomment
appropriately
rsupd = int ( sys . argv [3]) # int ( ratio [1])
size = int ( sys . argv [4])
supd = int ( sys . argv [5]) # if one wants only drug residues make 0 , if
also supdrug 1

56

Vid Sustar

30

31
32
33

7 APPENDIX

resid = int ( sys . argv [6]) # if one wants also other non drug / sup subs
residues 1 else 0
ending = " s " + str ( supd ) + " r " + str ( resid )
window_list =[1 ,50 ,100] # list of averaging window lenghts
angle_step = int ( sys . argv [7]) # there will be 360/ anglestep versions of
the matrix calculated each cumulatively rotated by anglestep in
y and z direction !

34
35
36
37
38

39

def usage () : # function to show the usage of memory
process = psutil . Process ( os . getpid () )
print ( " procces memory : " , process . memory_info () [0] / float (2 **
20) )
print ( " memory percent : " , process . memory_percent () )

40
41

print ( " intial " , usage () )

42
43
44

def finddrugprox ( drugname ) :

45
46
47

drugproximal = " "

48
49

for i in range ( len ( drug_list ) ) :

50
51
52
53

if drug_list [ i ] == drugname :
drugproximal = drugproximals [ i ]
# print (" drug_list [ i ] , drugname : " , drug_list [ i ] , drugname )

54
55

return drugproximal

56
57
58
59

from copy import deepcopy
from scipy . spatial import distance
def transform_rotate ( atoms , azim_angle , polar_angle ) : # function from
Otto Lindfors

60
61
62

# copy in order to not manipulate the original object
df = deepcopy ( atoms )

63
64
65
66
67
68
69
70
71
72
73
74
75

# Rotational matrices (" clockwize " rotation )
R_z = np . array ([
[ np . cos ( polar_angle ) , np . sin ( polar_angle ) , 0] ,
[ - np . sin ( polar_angle ) , np . cos ( polar_angle ) , 0] ,
[0 , 0 , 1]
])
R_y = np . array ([
[ np . cos ( azim_angle ) , 0 , - np . sin ( azim_angle ) ] ,
[0 , 1 , 0] ,
[ np . sin ( azim_angle ) , 0 , np . cos ( azim_angle ) ]
])
R = np . dot ( R_y , R_z )

76
77
78

# coordinates
A = df [[ ’ x_coord ’ , ’ y_coord ’ , ’ z_coord ’ ]]. values . T

79
80

# Rotate

57

Vid Sustar

81

7 APPENDIX

A = np . dot (R , A )

82
83
84

# Save
df . loc [: , [ ’ x_coord ’ , ’ y_coord ’ , ’ z_coord ’ ]] = A . T

85
86
87

# Update or add the spherical coordinates
df = a d d _ s p h e r i c a l _ c o o r d i n a t e s ( df )

88
89

return df

90
91
92
93
94
95

96
97
98
99
100
101
102

def s h i f t _ a t o m s _ t o _ o r i g i n ( atoms , ce nt er _a to m_ num be r =1) :
atoms = deepcopy ( atoms )
# the nanoparticle center
center_atom = atoms . loc [ atoms [ ’ atom_number ’] ==
ce nt er _a to m_ nu mb er ]
x_center = center_atom [ ’ x_coord ’ ][0]
y_center = center_atom [ ’ y_coord ’ ][0]
z_center = center_atom [ ’ z_coord ’ ][0]
# Center the nanoparticle to origo
atoms . loc [: , ’ x_coord ’] -= x_center
atoms . loc [: , ’ y_coord ’] -= y_center
atoms . loc [: , ’ z_coord ’] -= z_center

103
104
105
106
107
108

return atoms
def a d d _ s p h e r i c a l _ c o o r d i n a t e s ( atoms ) : # function from Otto Lindfors
# add radius column to atoms df
r = PandasPdb . distance_df ( atoms )
atoms [ ’ radius ’ ]= r

109
110
111
112

x = atoms [ ’ x_coord ’]
y = atoms [ ’ y_coord ’]
z = atoms [ ’ z_coord ’]

113
114
115

116

# polar angle
atoms [ ’ polar_angle ’] = np . arctan2 (y , x )
# angles in
range [ - pi , pi ]
atoms . loc [ atoms [ ’ polar_angle ’] < 0 , ’ polar_angle ’] += 2 * np . pi
# add np . pi to negative angles to make the angles in range [0 , 2
pi [

117
118
119
120

# azimuth angle
atoms [ ’ azim_angle ’] = np . arccos ( z / r )
atoms . loc [ atoms [ ’ azim_angle ’ ]. isna () , ’ azim_angle ’] = 0

121
122

return atoms

123
124
125

def s h i f t _ a t o m s _ f r o m _ o r i g i n ( atoms , gdata , ce nt er _a to m_ num be r =1) :
""" Centers the atoms so that ce nt er _a to m_n um be r is at origin """

126
127

atoms = deepcopy ( atoms )

128
129
130

131

# the nanoparticle center
center_atom = gdata . loc [ atoms [ ’ atom_number ’] ==
ce nt er _a to m_ nu mb er ]
x_center = center_atom [ ’ x_coord ’ ][0]

58

Vid Sustar

132
133
134
135
136
137

7 APPENDIX

y_center = center_atom [ ’ y_coord ’ ][0]
z_center = center_atom [ ’ z_coord ’ ][0]
# Center the nanoparticle to origo
atoms . loc [: , ’ x_coord ’] += x_center
atoms . loc [: , ’ y_coord ’] += y_center
atoms . loc [: , ’ z_coord ’] += z_center

138
139

return atoms

140
141
142

143
144

145

146

147

def resid_distance ( pdata , residuename ) :
# function to calculate radial distances between centre of
nanoparticle , origin -> and their mean , median , max , min )
pdata_residue = pdata [ ’ residue_name ’] == residuename
p d a t a _ r e s i d u e _ r d i s t a n c e _ m e a n = pdata . loc [ pdata_residue , ’ radius ’
]. mean ()
p d a t a _ r e s i d u e _ r d i s t a n c e _ m e d i a n = pdata . loc [ pdata_residue , ’
radius ’ ]. median ()
p d a t a _ r e s i d u e _ r d i s t a n c e _ m a x = pdata . loc [ pdata_residue , ’ radius ’
]. max ()
p d a t a _ r e s i d u e _ r d i s t a n c e _ m i n = pdata . loc [ pdata_residue , ’ radius ’
]. min ()

148
149

150
151

152

153

# function to calculate distances between all the residues of
certain type -> and their mean , median )
pdata_residue_2 = pdata [ pdata [ ’ residue_name ’] == residuename ]
pdata_residue_2 = pdata_residue_2 [[ ’ x_coord ’ , ’ y_coord ’ , ’ z_coord ’
]]
distances = distance . cdist ( pdata_residue_2 , pdata_residue_2 , ’
euclidean ’)
distances = skip_ diag_s trided ( distances ) # def skip_ diag_s trided ( A )
: # function to remove diagonal 0 s from euclidean distance matrix
( self distances ) to keep only calculated distances between
residues

154
155
156

p d a t a _ r e s i d u e _ i n t r _ d i s t a n c e _ m e d i a n = np . median ( distances )
p d a t a _ r e s i d u e _ i n t r _ d i s t a n c e _ m e a n = np . mean ( distances )

157
158

159

d i s t a n c e _ l i s t _ r e s i d u e =[ pdata_residue_rdistance_mean ,
pdata_residue_rdistance_median , pdata_residue_rdistance_max ,
pdata_residue_rdistance_min , p data_r esidue _intr_ distan ce_med ian ,
pdata_residue_intr_distance_mean ]
return d i s t a n c e _ l i s t _ r e s i d u e

160
161

162
163
164
165

def s kip_di ag_str ided ( A ) : # function to remove diagonal 0 s from
euclidean distance matrix ( self distances ) to keep only
calculated distances between residues
m = A . shape [0]
strided = np . lib . stride_tricks . as_strided
s0 , s1 = A . strides
return strided ( A . ravel () [1:] , shape =( m -1 , m ) , strides =( s0 + s1 , s1 ) )
. reshape (m , -1)

166
167
168
169
170

def p a nd a s df _ n p_ a p pe n d s ( pdata , k ) :
Row_list =[]
# Iterate over each row
for index , rows in pdata . iterrows () :

59

Vid Sustar

171
172

173
174
175
176

177

7 APPENDIX

# Create list for the current row
my_list =[ rows . atom_number , rows . atom_name , rows .
residue_name , rows . residue_number , rows . x_coord , rows . y_coord ,
rows . z_coord , rows . element_symbol , rows . intensity ]
# append the list to the final list
Row_list . append ( my_list )
pdata_np = np . array ( Row_list )
pdata_np = np . expand_dims ( pdata_np , axis =1) # adding an extra
dimension so later next time points can be added
return pdata_np

178
179
180
181
182
183
184

185
186

187
188
189
190
191
192
193
194
195

196
197
198
199
200

def pdb2matrix (k , filename , angle , sec_port ) :
if 1:
fulpath = filename
i n pu t _ fi l e _n a m e_ i d x = fulpath . rfind ( " / " )
input_file_name = fulpath [ i n p ut _ f il e _ na m e _i d x +1: len ( fulpath )
-4]
i n p u t _ f i l e _ n a m e _ s q n u m _ i d x = input_file_name . rfind ( " _ " )
i n p u t _ f i l e _ n a m e _ s q n u m = input_file_name [
i n p u t _ f i l e _ n a m e _ s q n u m _ i d x +1: len ( input_file_name ) ]
ppdb = PandasPdb () . read_pdb ( fulpath ) # reading from path
# Using DataFrame . insert () to add a column
ppdb . df [ ’ ATOM ’ ][ " intensity " ] = np . nan
# CALCULATE AND ADD INTENSITIES based on atom and residue :
ppdb . df [ ’ ATOM ’ ]. head (3)
data = ppdb . df [ ’ ATOM ’]
# usage () # initial memory usage
# drop unnecessary columns
pdata = data . drop ([ ’ record_name ’ , ’ blank_1 ’ , ’ alt_loc ’ , ’ blank_2 ’
, ’ chain_id ’ , ’ insertion ’ , ’ segment_id ’ , ’ occupancy ’ , ’ b_factor ’ , ’
charge ’ , ’ line_idx ’ , ’ blank_3 ’ , ’ blank_4 ’] , axis =1)
# check dropped columns
pdata . head (3)
num_pdb_rows = len ( pdata )
num_pdb_columns = len ( pdata . columns )
# calculating positions of residues in PDB , to get the ranges
for colorations

201
202
203
204
205
206
207
208

ngoldresidues =2869
c o mm o n _d r u gr e s id u e s =[ " C11 " ," OCB " ," OCC " ," OCC " ]
drug_addit_res =[ " OCC " ," OCC " ]
sup_drugresidues =[ " OCN " ," ONC " ]
# filename_front = input_file . split (" _ ") [0]
# print ( filename_front )
n_total =420

209
210
211
212
213
214
215
216

217

trat = rdrug + rsupd
ndrug = int (( n_total / trat ) * rdrug )
nsupd = int (( n_total / trat ) * rsupd )
drugproximal = finddrugprox ( drugname )
# print (" drugproximal " , drugproximal )
drugresidues =[ drugproximal , drugname ]
dr ug _a ss c_ re si du es = c o m mo n _ dr u g re s i du e s + drug_addit_res +
drugresidues
s u p p d r u g _ a s s c _ r e s i d u e s = c o m mo n _ dr u g re s i du e s + sup_drugresidues

60

Vid Sustar

218
219

220
221

222
223

7 APPENDIX

s t a r t i n g _ d r u g _ r e s n u m = ngoldresidues +1
en di ng _d ru g_ re sn um = s t a r t i n g _ d r u g _ r e s n u m +( ndrug * len (
dr ug _a ss c_ re si du es ) ) -1
s t a r t i n g _ s u p d r u g _ r e s n u m = en di ng _d ru g_ re snu m +1
e n d i n g _ s u p d r u g _ r e s n u m = s t a r t i n g _ s u p d r u g _ r e s n u m +( nsupd * len (
s u p p d r u g _ a s s c _ r e s i d u e s ) ) -1
# COLORING !
# could be by residue number range and then residue name and
element symbol

224
225
226

227
228

229
230

231

# gold brightest
pdata [ ’ intensity ’] = np . where ( pdata [ ’ element_symbol ’] == ’ AU
’ ,255 , pdata [ ’ intensity ’ ])
# brighter grey tones
l i s t _ d r u g _ r e s i d u e _ c o l o r s = [205 , 206 , 207 , 208 , 209 , 210 ,
211 , 212 , 213 , 214 , 215 , 216 , 217 , 218 , 219 , 220 ,
221 , 222 , 223 , 224 , 225 , 226 , 227 , 228 , 229 , 230 , 231 ,
232 , 233 , 234 , 235 , 236 , 237 , 238 , 239 , 240 , 241 ,
242 , 243 , 244 , 245 , 246 , 247 , 248 , 249 , 250 , 251 , 252 ,
253 , 254 , 255]
# darker grey tones
l i s t _ s u p d r u g _ r e s i d u e _ c o l o r s =[50 , 51 , 52 , 53 , 54 , 55 , 56 ,
57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 ,
73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 ,
89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100]
m a x l i s t _ d r u g _ r e s i d u e _ c o l o r s = max ( l i s t _ d r u g _ r e s i d u e _ c o l o r s )

232
233
234
235

236
237

238

239

240

241

242

dr ug _a ss c_ re si du es = list ( dict . fromkeys ( d ru g_a ss c_ re si du es ) )
pdata [ ’ intensity ’] = 0 # 0 #1 for testing
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number ’] <=
en di ng _d ru g_ re sn um ) & ( pdata [ ’ residue_name ’] == drug ) &( pdata [ ’
atom_name ’] == ’ H31 ’) ,255 , pdata [ ’ intensity ’ ])
if resid :
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number ’]
<= end in g_ dr ug _r es nu m ) & ( pdata [ ’ residue_name ’] == drug ) &( pdata [ ’
atom_name ’] == ’ C5 ’) ,255 , pdata [ ’ intensity ’ ])
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number ’]
<= end in g_ dr ug _r es nu m ) & ( pdata [ ’ residue_name ’] == drugproximal ) &
( pdata [ ’ atom_name ’] == ’ C1 ’) ,250 , pdata [ ’ intensity ’ ])
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number ’]
<= end in g_ dr ug _r es nu m ) & ( pdata [ ’ residue_name ’] == ’ OCC ’) & ( pdata
[ ’ atom_name ’] == ’ C1 ’) ,245 , pdata [ ’ intensity ’ ])
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number ’]
<= end in g_ dr ug _r es nu m ) & ( pdata [ ’ residue_name ’] == ’ OCB ’) & ( pdata
[ ’ atom_name ’] == ’ C1 ’) ,240 , pdata [ ’ intensity ’ ])
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number ’]
<= end in g_ dr ug _r es nu m ) & ( pdata [ ’ residue_name ’] == ’ C11 ’) & ( pdata
[ ’ atom_name ’] == ’ H72 ’) ,235 , pdata [ ’ intensity ’ ])
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number ’]
<= end in g_ dr ug _r es nu m ) & ( pdata [ ’ residue_name ’] == ’ NP ’) & ( pdata [
’ atom_number ’] == 1) ,5 , pdata [ ’ intensity ’ ])

243
244
245

if supd :
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number ’]
>= s t a r t i n g _ s u p d r u g _ r e s n u m ) & ( pdata [ ’ residue_name ’] == ’ ONC ’) &(
pdata [ ’ atom_name ’] == ’N ’) ,120 , pdata [ ’ intensity ’ ])

61

Vid Sustar

246
247

248

249

250

251

252

253

254

255
256
257

258
259
260

261
262
263

264
265

266
267
268

269
270
271
272
273
274
275
276
277
278
279

280

7 APPENDIX

if resid :
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number
’] >= s t a r t i n g _ s u p d r u g _ r e s n u m ) & ( pdata [ ’ residue_name ’] == ’ OCN ’)
& ( pdata [ ’ atom_name ’] == ’ C1 ’) ,115 , pdata [ ’ intensity ’ ])
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number
’] >= s t a r t i n g _ s u p d r u g _ r e s n u m ) & ( pdata [ ’ residue_name ’] == ’ OCC ’)
& ( pdata [ ’ atom_name ’] == ’ C1 ’) ,110 , pdata [ ’ intensity ’ ])
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number
’] >= s t a r t i n g _ s u p d r u g _ r e s n u m ) & ( pdata [ ’ residue_name ’] == ’ OCB ’)
& ( pdata [ ’ atom_name ’] == ’ C1 ’) ,105 , pdata [ ’ intensity ’ ])
pdata [ ’ intensity ’] = np . where (( pdata [ ’ residue_number
’] >= s t a r t i n g _ s u p d r u g _ r e s n u m ) & ( pdata [ ’ residue_name ’] == ’ C11 ’)
& ( pdata [ ’ atom_name ’] == ’ H72 ’) ,100 , pdata [ ’ intensity ’ ])
gold_x_mean = pdata [ pdata [ ’ element_symbol ’] == ’ AU ’ ][ " x_coord "
]. mean ()
gold_y_mean = pdata [ pdata [ ’ element_symbol ’] == ’ AU ’ ][ " y_coord "
]. mean ()
gold_z_mean = pdata [ pdata [ ’ element_symbol ’] == ’ AU ’ ][ " z_coord "
]. mean ()
# calculating the max distance from gold centre for all
dimensions
max_val_x = pdata [ " x_coord " ]. max ()
min_val_x = pdata [ " x_coord " ]. min ()
maxdist_goldc_x = max ( abs ( gold_x_mean - max_val_x ) , abs (
gold_x_mean - min_val_x ) )
max_val_y = pdata [ " y_coord " ]. max ()
min_val_y = pdata [ " y_coord " ]. min ()
maxdist_goldc_y = max ( abs ( gold_y_mean - max_val_y ) , abs (
gold_y_mean - min_val_y ) )
max_val_z = pdata [ " z_coord " ]. max ()
min_val_z = pdata [ " z_coord " ]. min ()
maxdist_goldc_z = max ( abs ( gold_z_mean - max_val_z ) , abs (
gold_z_mean - min_val_z ) )
# finding absolutely max distance among all dimensions
maxdist_goldc = max ( maxdist_goldc_x , maxdist_goldc_y ,
maxdist_goldc_z )
gdata = pdata [ pdata [ ’ atom_number ’] == 1]
# remove zero elements of pdata
pdata = pdata . drop ( pdata [( pdata . intensity == 0) &( pdata [ ’
atom_number ’] != 1) ]. index )
pdata = s h i f t _ a t o m s _ t o _ o r i g i n ( pdata ,1)
# rotate everything by certain angle !
phi = angle # CHANGE TO WHATEVER YOU NEED !
phi = phi / 180 * np . pi
polar_angle = phi
theta = angle
theta = theta / 180 * np . pi
azim_angle = theta
pdata = transform_rotate ( pdata , azim_angle , polar_angle )
pdata = s h i f t _ a t o m s _ f r o m _ o r i g i n ( pdata , gdata ,1)
pdata = pdata . drop ([ ’ radius ’ , ’ polar_angle ’ , ’ azim_angle ’] , axis
=1)
pdata = pdata . round (3) # rounding all the numbers to 3 decimals ,
so it is a bit easier to handle

281
282

return pdata ,k , maxdist_goldc , gold_x_mean , gold_y_mean ,

62

Vid Sustar

7 APPENDIX

gold_z_mean , num_pdb_rows , drugname , drugproximal
283
284
285

286

287

288
289
290
291
292
293
294
295

296

297

298

299

300

301
302
303

def matrix_mapping ( pdata ,k , maxdist_goldc , gold_x_mean , gold_y_mean ,
gold_z_mean , num_pdb_rows ) :
factor = size /(( maxdist_goldc +3) *2) # +4) #+3 because of variation ! ,
actually the np distance shrinks ,
# so if this is constant by the first maxgold distance and not
dynamic , then the shrinking will be shown on the matrix
xcordlist = pdata [ " x_coord " ]. tolist ()
ycordlist = pdata [ " y_coord " ]. tolist ()
zcordlist = pdata [ " z_coord " ]. tolist ()
xcordlistnp = pdata [ " x_coord " ]. to_numpy () . astype ( float )
xcordlistnp = np . asarray ( xcordlistnp , dtype = float )
ycordlistnp = pdata [ " y_coord " ]. to_numpy () . astype ( float )
zcordlistnp = pdata [ " z_coord " ]. to_numpy () . astype ( float )
xcordlist =[ int ((( float ( element ) - gold_x_mean ) * factor ) +( size -1)
/2.0) for element in xcordlist ]
ycordlist =[ int ((( float ( element ) - gold_y_mean ) * factor ) +( size -1)
/2.0) for element in ycordlist ]
zcordlist =[ int ((( float ( element ) - gold_z_mean ) * factor ) +( size -1)
/2.0) for element in zcordlist ]
npmatrix = np . zeros ([ size , size , size ]) # creating matrix with 0 s of
size given
npmatrix [ xcordlist , ycordlist , zcordlist ] = pd . to_numeric ( pdata [ "
intensity " ]. tolist () ) # filling it up with intensities to rounded
coordinates
npmatrix = np . expand_dims ( npmatrix , axis =0) # adding one extra
dimension " in front " , so I can concatenate them with others !
n u m _ n o n z e r o _ e l e m e n t s = np . count_nonzero ( npmatrix )
n o n z e r o _ e l e m e n t s _ l i s t . append ( n u m _ n o n z e r o _ e l e m e n t s )
return npmatrix

304
305
306
307
308
309

310
311
312

313

def pandas_averaging ( all_res_tmpnts_temp , wind ) :
# function to do the averaging along timepoints
data_xyz = a l l _r e s _t m p nt s _ te m p [: ,4:7]. copy ()
data_xyz = data_xyz . astype ( float )
data_xyz_df = pd . DataFrame ( data_xyz , columns = [ ’ x_coord ’ , ’
y_coord ’ , ’ z_coord ’ ])
df3 = data_xyz_df . copy ()
for i in range ( len ( df3 . columns ) ) :
df3 . iloc [: , i ] = df3 . iloc [: , i ]. rolling ( window = wind ,
min_periods =1) . mean ()
return df3

314
315
316

317
318

319

320

def m a t r i x _ a v e r a g e d 2 d i s t (k , averaged_data_df , sec_port , drugname ,
drugproximal ) :
if 1:
gold_x_mean = averaged_data_df [ averaged_data_df [ ’
element_symbol ’] == ’ AU ’ ][ " x_coord " ]. mean ()
gold_y_mean = averaged_data_df [ averaged_data_df [ ’
element_symbol ’] == ’ AU ’ ][ " y_coord " ]. mean ()
gold_z_mean = averaged_data_df [ averaged_data_df [ ’
element_symbol ’] == ’ AU ’ ][ " z_coord " ]. mean ()

321

63

Vid Sustar

322

323
324

7 APPENDIX

# calculating the max distance from gold centre for all
dimensions
max_val_x = averaged_data_df [ " x_coord " ]. max ()
min_val_x = averaged_data_df [ " x_coord " ]. min ()

325
326

327
328
329

maxdist_goldc_x = max ( abs ( gold_x_mean - max_val_x ) , abs (
gold_x_mean - min_val_x ) )
max_val_y = averaged_data_df [ " y_coord " ]. max ()
min_val_y = averaged_data_df [ " y_coord " ]. min ()
maxdist_goldc_y = max ( abs ( gold_y_mean - max_val_y ) , abs (
gold_y_mean - min_val_y ) )

330
331
332
333

max_val_z = averaged_data_df [ " z_coord " ]. max ()
min_val_z = averaged_data_df [ " z_coord " ]. min ()
maxdist_goldc_z = max ( abs ( gold_z_mean - max_val_z ) , abs (
gold_z_mean - min_val_z ) )

334
335

maxdist_goldc = max ( maxdist_goldc_x , maxdist_goldc_y ,
maxdist_goldc_z )

336
337
338

339
340
341
342
343
344
345
346
347

gdata = averaged_data_df [ averaged_data_df [ ’ atom_number ’] == 1]
averaged_data_df = averaged_data_df . drop ( averaged_data_df [(
averaged_data_df . intensity == 0) &( averaged_data_df [ ’ atom_number ’]
!= 1) ]. index )
averaged_data_df = s h i f t _ a t o m s _ t o _ o r i g i n ( averaged_data_df ,1)
# rotate everything by certain angle !
phi =0 # CHANGE TO WHATEVER YOU NEED !
phi = phi / 180 * np . pi
polar_angle = phi
theta =0
theta = theta / 180 * np . pi
azim_angle = theta
averaged_data_df = transform_rotate ( averaged_data_df ,
azim_angle , polar_angle )

348
349
350

351
352

di st an ce _l is t_ dr ug = resid_distance ( averaged_data_df , drugname )
d i s t a n c e _ l i s t _ d r u g p r o x = resid_distance ( averaged_data_df ,
drugproximal )
di st an ce _l is t_ su ps = resid_distance ( averaged_data_df , " ONC " )
d i s t a n c e _ l i s t _ s u p s p r o x = resid_distance ( averaged_data_df , " OCN "
)

353
354
355
356
357

358

359

360

361

dis tance_ list_O CC = resid_distance ( averaged_data_df , " OCC " )
dis tance_ list_O CB = resid_distance ( averaged_data_df , " OCB " )
dis tance_ list_C 11 = resid_distance ( averaged_data_df , " C11 " )
d i s t a n c e _ l i s t _ j o i n e d = di sta nc e_ li st _d ru g +
d i s t a n c e _ l i s t _ d r u g p r o x + d is ta nce _l is t_ su ps + d i s t a n c e _ l i s t _ s u p s p r o x +
dist ance_l ist_O CC + dis tance_ list_O CB + dis tance_ list_C 11
averaged_data_df = s h i f t _ a t o m s _ f r o m _ o r i g i n ( averaged_data_df ,
gdata ,1)
averaged_data_df = averaged_data_df . drop ([ ’ radius ’ , ’
polar_angle ’ , ’ azim_angle ’] , axis =1)
averaged_data_df = averaged_data_df . round (3) # rounding all the
numbers to 3 decimals , so it is a bit easier to handle
return averaged_data_df ,k , maxdist_goldc , gold_x_mean , gold_y_mean ,
gold_z_mean , d i s t a n c e _ l i s t _ j o i n e d

64

Vid Sustar

7 APPENDIX

362
363
364
365

start_time = datetime . now ()
def main ( path , size , wind , angle ) :

366
367
368
369
370

371
372

373

sec_port =0
input_files = sorted ( glob . glob ( path + " *. pdb " ) )
counter =0
for k , filename in enumerate ( input_files ) : # enumerate is for
greater control , to put some extra constraints on steps of for
loop
if ( k == 0) :
pdata ,k , maxdist_goldc , gold_x_mean , gold_y_mean ,
gold_z_mean , num_pdb_rows , drugname , drugproximal = pdb2matrix (k ,
filename , angle , sec_port )
pdata_np = p a n da s d f_ n p _a p p en d s ( pdata , k )

374
375

376
377

378

all_res_tmpnts = np . zeros ([ pdata_np . shape [0] , 300 , 9] ,
dtype = object )
all_res_tmpnts [: , k : k +1 ,:]= pdata_np
pdata_index = pdata . index # to be used when reconstituting
individual timepoint dataframe from numpy matrix
pdata_columns = pdata . columns # later in the main function

379
380
381
382

383

384
385

if ( k !=0) :
# print (" new_run ")
pdata ,k , maxdist_goldc , gold_x_mean , gold_y_mean ,
gold_z_mean , num_pdb_rows , drugname , drugproximal = pdb2matrix (k ,
filename , angle , sec_port )
# actually gold parameters might need to be fixed on the
initial values
pdata_np = p a n da s d f_ n p _a p p en d s ( pdata , k )
all_res_tmpnts [: , k : k +1 ,:]= pdata_np

386
387
388
389
390

391
392
393

counter =0
z = 2 # number of steps to have printed reporting for debugging
# do the averaging
for i in range ( all_res_tmpnts . shape [0]) : # looping through all
the residues , averaging by wind ( ow ) size
paverage = pandas_averaging ( all_res_tmpnts [ i ] , wind )
paverage = paverage . to_numpy ()
all_res_tmpnts [ i ][0: all_res_tmpnts . shape [1] ,4:7]= paverage

394
395
396

sec_port =1
print ( " 2 ND LOOP , AFTER AVERAGING " )

397
398

tem p_data _colum ns =[ ’ atom_number ’ , ’ atom_name ’ , ’ residue_name ’ , ’
residue_number ’ , ’ x_coord ’ , ’ y_coord ’ , ’ z_coord ’ , ’ element_symbol ’
, ’ intensity ’]

399
400

401

for k , filename in enumerate ( input_files ) : # enumerate is for
greater control , to put some extra constraints on steps of for
loop
if ( k == 0) :

402
403

te mp pd at a_ fi na l_ np = all_res_tmpnts [: , k : k +1 ,:]

65

Vid Sustar

404

405

406

407

408
409

7 APPENDIX

temppdata_final = t emp pd at a_ fi na l_ np [: , 0 , :] # removing
the dimension that was used to enumerate the timepoint
temp _data _colum ns =[ ’ atom_number ’ , ’ atom_name ’ , ’
residue_name ’ , ’ residue_number ’ , ’ x_coord ’ , ’ y_coord ’ , ’ z_coord ’ ,
’ element_symbol ’ , ’ intensity ’]
temppdata_df_inp = pd . DataFrame ( data = temppdata_final ,
index = pdata_index , columns = temp_ data_c olumns )
temppdata_df_inp = temppdata_df_inp . astype ( dtype = { "
atom_number " : " int64 " ," atom_name " : " object " ," residue_name " : " object "
," residue_number " : " int64 " ," x_coord " : " float64 " ," y_coord " : " float64 "
," z_coord " : " float64 " ," element_symbol " : " object " ," intensity " : " int64
" })
temppdata_df_inp . round (3)
tempdata_df_out ,k , maxdist_goldc , gold_x_mean , gold_y_mean ,
gold_z_mean , distance_df_inp = m a t r i x _ a v e r a g e d 2 d i s t (k ,
temppdata_df_inp , sec_port , drugname , drugproximal )

410
411

array = matrix_mapping ( tempdata_df_out ,k , maxdist_goldc ,
gold_x_mean , gold_y_mean , gold_z_mean , te mp pd at a_f in al _n p . shape [0])

412
413

414

415

distance_df = pd . DataFrame ( columns =[ ’ DrgRDMn ’ , ’ DrgRDMdn ’
, ’ DrgRDMx ’ , ’ DrgRDMin ’ , ’ DrgIntDMn ’ , ’ DrgIntDMdn ’ , ’ DrgPrxRDMn ’ , ’
DrgPrxRDMdn ’ , ’ DrgPrxRDMx ’ , ’ DrgPrxRDMin ’ , ’ DrgPrxIntDMn ’ , ’
DrgPrxIntDMdn ’ , ’ SupRDMn ’ , ’ SupRDMdn ’ , ’ SupRDMx ’ , ’ SupRDMin ’ , ’
SupIntDMn ’ , ’ SupIntDMdn ’ , ’ SupPrxRDMn ’ , ’ SupPrxRDMdn ’ , ’ SupPrxRDMx ’ , ’
SupPrxRDMin ’ , ’ SupPrxIntDMn ’ , ’ SupPrxIntDMdn ’ , ’ OCCRDMn ’ , ’ OCCRDMdn ’ ,
’ OCCRDMx ’ , ’ OCCRDMin ’ , ’ OCCIntDMn ’ , ’ OCCIntDMdn ’ , ’ OCBRDMn ’ , ’ OCBRDMdn
’ , ’ OCBRDMx ’ , ’ OCBRDMin ’ , ’ OCBIntDMn ’ , ’ OCBIntDMdn ’ , ’ C11RDMn ’ , ’
C11RDMdn ’ , ’ C11RDMx ’ , ’ C11RDMin ’ , ’ C11IntDMn ’ , ’ C11IntDMdn ’ ])
d i s t a n c e _ d f _ i n p _ s e r i e s = pd . Series ( distance_df_inp ,
index = distance_df . columns )
distance_df = distance_df . append ( distance_df_inp_series
, ignore_index = True )

416
417
418
419

420

421

422

if (( k !=0) ) : # &( k <3) ) :
te mp pd at a_ fi na l_ np = all_res_tmpnts [: , k : k +1 ,:]
temppdata_final = t emp pd at a_ fi na l_ np [: , 0 , :] # removing
the dimension that was used to enumerate the timepoint
temppdata_df_inp = pd . DataFrame ( data = temppdata_final ,
index = pdata_index , columns = temp_ data_c olumns )
temppdata_df_inp = temppdata_df_inp . astype ( dtype = { "
atom_number " : " int64 " ," atom_name " : " object " ," residue_name " : " object "
," residue_number " : " int64 " ," x_coord " : " float64 " ," y_coord " : " float64 "
," z_coord " : " float64 " ," element_symbol " : " object " ," intensity " : " int64
" })
temppdata_df_inp . round (3)

423
424
425

tempdata_df_out ,k , maxdist_goldc , gold_x_mean , gold_y_mean ,
gold_z_mean , distance_df_inp = m a t r i x _ a v e r a g e d 2 d i s t (k ,
temppdata_df_inp , sec_port , drugname , drugproximal )

426
427

428

d i s t a n c e _ d f _ i n p _ s e r i e s = pd . Series ( distance_df_inp ,
index = distance_df . columns )
distance_df = distance_df . append ( distance_df_inp_series
, ignore_index = True )

66

Vid Sustar

429

7 APPENDIX

array_temp = matrix_mapping ( tempdata_df_out ,k ,
maxdist_goldc , gold_x_mean , gold_y_mean , gold_z_mean ,
te mp pd at a_ fi na l_ np . shape [0])

430

array = np . concatenate (( array , array_temp ) , axis = 0)

431
432
433

return distance_df , array

434
435
436
437
438
439
440
441
442
443
444
445
446
447

448

449

for wind in window_list :
print ( " wind : " , wind )
for angle in range (0 ,360 , angle_step ) :
print ( " angle : " , angle )
inds = [ i for i , c in enumerate ( path ) if c == ’/ ’]
try :
main_fold_idx_1 =( inds [ -3])
main_fold_idx_2 =( inds [ -2])
except :
print ( " ERROR ! main folder finding not working " )
mainfolder = path [ main_fold_idx_1 +1: main_fold_idx_2 ]
print ( " main folder name : " , mainfolder )
h5name = ’ pdb2mtrx_2_ ’+ mainfolder + str ( size ) + ’_ ’+ ending + ’_ ’+ str
( angle ) + ’_ ’+ str ( wind ) + ’. h5 ’
dist_csv_name = ’ distancescsv_2_ ’+ mainfolder + str ( size ) + ’_ ’+
ending + ’_ ’+ str ( angle ) + ’_ ’+ str ( wind ) + ’ 2. csv ’
print ( ’ h5name : ’ , h5name )

450
451

try :

452

h5f = h5py . File ( h5name , ’w ’)
except :
print ( " ERROR ! COULD NOT CREATE H5 FILE " )
distance_df_out , array1 = main ( path , size , wind , angle )
print ( " ########### FINAL ARRAY SHAPE : " , array1 . shape )
print ( " len ( distance_df_out . index ) " , len ( distance_df_out . index

453
454
455
456
457

))
458
459

dset = h5f . create_dataset ( ’ pdb2_3dmatrix ’ , data = array1 ,
compression = " gzip " , compression_opts = comp ressio n_leve l )

460
461

distance_df_out . to_csv ( dist_csv_name , encoding = ’utf -8 ’ , sep =
’ , ’ , index = False )

462
463
464

h5f . close ()
usage ()

465
466
467
468

time_elapsed = datetime . now () - start_time
print ( ’ Time elapsed ( hh : mm : ss . ms ) {} ’. format ( time_elapsed ) )
usage ()

Listing 5: Python script to rotate and convert PDB atom positions into Numpy 3D
matrices as separate quadrants and calculate the radial and inter-residue distances
in NP.
1
2
3
4

import numpy as np
import tensorflow . keras
import h5py

67

Vid Sustar

5
6
7
8
9
10

7 APPENDIX

import math
import time
from pympler import asizeof # to check the memory size of objects
from numpy import save
import csv
import pandas as pd

11
12
13
14

import sys
import os # psutil

15
16
17
18
19
20
21
22
23

# PLOTING WITH MATPLOT
import matplotlib
matplotlib . use ( ’ Agg ’)
import matplotlib . pyplot as plt
fig = plt . figure ()
ax = fig . add_subplot (111)
ax . plot ([1 ,2 ,3])
fig . savefig ( ’ test . png ’)

24
25

exp_n_col_y =17 # ### number of columns from final exposure y

26
27
28

29

30

31

size = int ( sys . argv [1]) # the size of the input matrix
typeinp = str ( sys . argv [2]) # the type of the matrix , whether there are
drugcentres , supsubscentres , other molecule centres etc
interval = int ( sys . argv [3]) # the number of timepoints ( pdbs , rows in
exposure csv ) to be skipped ( to make timedistributed input for
LSTM smaller and hence allow for bigger 3 d matrix )
quad = int ( sys . argv [4]) # quadrant (1) or no (0) , taking just one
eighth front top left quadrant of 3 D matrix , to lower the size of
input
conc_quad = int ( sys . argv [5]) # if one wants to take the other 7
quadrants into consideration , it will concatenate them !

32
33

34

35

36

37
38

39
40
41
42

43
44
45

sect = int ( sys . argv [6]) # sectioning of the 3 d matrix ( can be quadrant
even further ) (1 - yes ) (0 - no ) , taking the right most portion
of the matrix , see below
width = int ( sys . argv [7]) # width of the section from above , how many
cells , how much of the right most portion of the matrix
cpu = int ( sys . argv [8]) # CPU mode - disable usage of GPU ! (1 - CPU , 0
- GPU )
exp_n_col_y = int ( sys . argv [9]) # ### number of columns from final
exposure y
num_epochs = int ( sys . argv [10]) # number of epochs
sim_len_f = int ( sys . argv [11]) # input length ( the output should be of
same length )
inp_len = int ( sys . argv [12]) # numb of inputs per LSTM
wind = int ( sys . argv [13]) # averaging window of SASA csv
train_end = int ( sys . argv [14]) # t r a i n f o l d e r _ e n d i n g _ d i g i t
if quad : # adjusting reported size of input matrix according to
taking only quadrants
nsize = int ( size /2)
else :
nsize = size

46
47

if sect : # adjusting the first dimension ( width ) of 3 d matrix

68

Vid Sustar

48
49
50

7 APPENDIX

appropriatelty according to sectioning
nsize1 = width
else :
nsize1 = nsize

51
52
53
54
55
56
57
58

print ( " >>>>>>>> INPUT PARAMETERS : " )
print ( " size : " , size )
print ( " interval between timepoints : " , interval )
print ( " use of only single quadrant of matrix bool : " , quad )
print ( " use only section of matrix bool : " , sect )
print ( " width of section : " , width )
print ( " usage of cpu only bool : " , cpu )

59
60
61

62
63

pathtrain = os . getcwd () + ’/ train ’+ str ( train_end ) + ’/ ’# sys . argv [3] # path
to the input # make sure the train data in train folder !
pathtest = os . getcwd () + ’/ test / ’# sys . argv [3] # path to the input
import glob

64
65
66

from datetime import datetime
start_time = datetime . now ()

67
68
69
70

if cpu : # disabling gpu to force run it on cpu
os . environ [ " CU DA_DEV ICE_OR DER " ] = " PCI_BUS_ID "
os . environ [ " C U D A _ V I S I B L E _ D E V I C E S " ] = " "

71
72
73
74

import subprocess as sp
import tensorflow as tf

75
76
77

78
79
80
81
82

def gpu_memory_usage ( gpu_id ) :
command = r " nvidia - smi -- id =0 -- query - gpu = memory . used -- format =
csv "
output_cmd = sp . check_output ( command . split () )
memory_used = output_cmd . decode ( " ascii " ) . split ( " \ n " ) [1]
# Get only the memory part as the result comes as ’10 MiB ’
memory_used = int ( memory_used . split () [0])
return memory_used

83
84
85

86
87
88
89
90

def gpu_memory_total ( gpu_id ) :
command = r " nvidia - smi -- id =0 -- query - gpu = memory . total -- format =
csv "
output_cmd = sp . check_output ( command . split () )
memory_total = output_cmd . decode ( " ascii " ) . split ( " \ n " ) [1]
# Get only the memory part as the result comes as ’10 MiB ’
memory_total = int ( memory_total . split () [0])
return memory_total

91
92
93

# The gpu you want to check
gpu_id = 0

94
95
96

i n i t i a l _ m e m o r y _ u s a g e = gpu_memory_usage ( gpu_id )
i n i t i a l _ m e m o r y _ t o t a l = gpu_memory_total ( gpu_id )

97
98

# Set up the gpu specified

99

69

Vid Sustar

100
101
102

7 APPENDIX

gpu_devices = tf . config . experimental . l i s t _ p h y s i c a l _ d e v i c e s ( ’ GPU ’)
for device in gpu_devices :
tf . config . experimental . se t_memo ry_gro wth ( device , True )

103
104
105
106

107
108
109

110

sim_size = 1
full_sim_len =300
sim_len_init = 150 # because we are using one timepoint to predict
the next one ,
sim_len = math . ceil ( sim_len_init / interval )
# sim_len_f = 150 # input length ( the output should be of same length )
# so if there are 300 timepoints , the last needs to be used for
prediction ... and we set sim_len 299
sample_shape = ( sim_len_f , size , size , size )

111
112
113
114

115
116
117
118
119

def make_input_3d ( path ) :
namestring = path [ len ( path ) -6: len ( path ) ]
print ( " \ n \n -3 dmatrix - - h5 - -3 dmatrix - - h5 - -3 dmatrix - - h5 - -3 dmatrix - h5 - -3 dmatrix - - h5 - -3 dmatrix - - h5 - -3 dmatrix - - h5 - -3 dmatrix - - h5 - -\ n " )
atomtrain =[]
print ( " namestring " , namestring )
for typeinp in [ ’ s1r1 ’ ]:
print ( typeinp )
input_files = sorted ( glob . glob ( path + " *. h5 " ) )

120
121
122
123

124

125

126
127

128
129
130

131
132
133

134
135
136
137

138
139
140

141

input_files_csv = sorted ( glob . glob ( path + " * SASA . csv " ) )
i n p u t _ f i l e s _ c s v _ d i s t = sorted ( glob . glob ( path + " * s1r1 *. csv " ) )
print ( " input_files_len len ( input_files ) , len (
input_files_csv ) , len ( i n p u t _ f i l e s _ c s v _ d i s t ) " , len ( input_files ) ,
len ( input_files_csv ) , len ( i n p u t _ f i l e s _ c s v _ d i s t ) )
for k , filename in enumerate ( input_files ) : # enumerate is for
greater control , to put some extra contraints on steps of for
loop
sasa_k = int ( k /( len ( i n p u t _ f i l e s _ c s v _ d i s t ) /( len (
input_files_csv ) ) ) )
try :
if 1: # ( k ==0) : # for testing purposes ( set to if 1:
otherwise )
print ( " FILENAME : " , filename )
print ( " K : " ,k )
filename_clean = filename [ filename . rfind ( ’/ ’) +1:
len ( filename ) -3]
# filename_clean = filename [: len ( filename ) -3]
print ( " filename_clean first : " , filename_clean )
print ( " input_files_csv [ k ] " , input_files_csv [
sasa_k ])
# break # comment out
if ( k ==0) : # initial file
with h5py . File ( filename , ’r ’) as dataset :
for key in dataset : # if there were more
datasets in one h5 file !
print ( " key from h5py " , key )
# ASSUMPTION : 300 datapoints in data
atomtrain = dataset [ list ( dataset . keys () )
[0]] # [: sim_len_f ] # taking just first portion of the simulations
# the number of timepoints ( pdbs , rows in
exposure csv ) to be skipped ( to make timedistributed input for

70

Vid Sustar

142

143

144

145
146
147

148

149

150
151

152

153
154

155

7 APPENDIX

LSTM smaller and hence allow for bigger 3 d matrix )
print ( " atomtrain = dataset [ list ( dataset .
keys () ) [0]][: -1] " , atomtrain . shape )
atomtrain = atomtrain [0:: interval ] #
subsampling taking just every interval - th matrix ,
print ( " atomtrain = atomtrain [0:: interval ] "
, atomtrain . shape )
if quad :
# selecting quadrants :
# LEFT [: int ( h / 2) ] RIGHT [ int ( size /
2) :]
# front [: ,: int ( size / 2) ] back [: , int (
size / 2) :]
# top [: ,: , int ( size / 2) :] bot [: ,: ,:
int ( h / 2) ]
# flipping into front top left
# atomtrain = np . array ([ atomtrain [ i ][:
int ( size / 2) ][: ,: int ( size / 2) :][: ,: , int ( size / 2) :] for i in
range ( int ( sim_len_f / 1) ) ]) # taking just front top left quadrant
of the matrix
front_top_left = np . array ([ atomtrain [
i ][: int ( size / 2) ][: ,: int ( size / 2) :][: ,: , int ( size / 2) :] for i
in range ( int ( sim_len_f / 1) ) ])
atomtrain1 = front_top_left
atomtrain1b = np . rot90 ( atomtrain1 , k
=1 , axes =(1 ,2) )
atomtrain1b = np . rot90 ( atomtrain1b , k
=1 , axes =(1 ,3) )

156

atomtrain1c = np . rot90 ( atomtrain1b , k

157

=1 , axes =(1 ,2) )
atomtrain1c = np . rot90 ( atomtrain1c , k

158

=1 , axes =(1 ,3) )
# print (" atomtrain . shape " , atomtrain .

159

shape )
160
161
162

163

if conc_quad :
sim_size = 8
front_top_right = np . array ([
atomtrain [ i ][ int ( size / 2) :][: ,: int ( size / 2) :][: ,: , int ( size
2) :] for i in range ( int ( sim_len_f / 1) ) ])
print ( " atomtrain2 . shape " ,
front_top_right . shape )

/

164

atomtrain2 = front_top_right

165

[: ,:: -1 , : , :]
atomtrain2b = np . rot90 ( atomtrain2 ,

166

k =1 , axes =(1 ,2) )
atomtrain2b = np . rot90 ( atomtrain2b

167

, k =1 , axes =(1 ,3) )
168

atomtrain2c = np . rot90 ( atomtrain2b

169

, k =1 , axes =(1 ,2) )
atomtrain2c = np . rot90 ( atomtrain2c

170

, k =1 , axes =(1 ,3) )
171

print ( " atomtrain2 . shape " ,

172

71

Vid Sustar

7 APPENDIX

atomtrain2 . shape )
173

174

175

176

177

front_bot_left = np . array ([
atomtrain [ i ][: int ( size / 2) ][: ,: int ( size / 2) ][: ,: ,: int ( size /
2) ] for i in range ( int ( sim_len_f ) ) ])
print ( " atomtrain3 . shape " ,
front_bot_left . shape )
atomtrain3 = front_bot_left [: ,: ,
: , :: -1]
atomtrain3b = np . rot90 ( atomtrain3 ,
k =1 , axes =(1 ,2) )
atomtrain3b = np . rot90 ( atomtrain3b
, k =1 , axes =(1 ,3) )

178

atomtrain3c = np . rot90 ( atomtrain3b

179

, k =1 , axes =(1 ,2) )
atomtrain3c = np . rot90 ( atomtrain3c

180

, k =1 , axes =(1 ,3) )
print ( " atomtrain3 . shape " ,

181

atomtrain3 . shape )
182

183

184

185

front_bot_right = np . array ([
atomtrain [ i ][ int ( size / 2) :][: ,: int ( size / 2) ][: ,: ,: int ( size /
2) ] for i in range ( int ( sim_len_f ) ) ])
atomtrain4 = front_bot_right
[: ,:: -1 , : , :: -1]
atomtrain4b = np . rot90 ( atomtrain4 ,
k =1 , axes =(1 ,2) )
atomtrain4b = np . rot90 ( atomtrain4b
, k =1 , axes =(1 ,3) )

186

atomtrain4c = np . rot90 ( atomtrain4b

187

, k =1 , axes =(1 ,2) )
atomtrain4c = np . rot90 ( atomtrain4c

188

, k =1 , axes =(1 ,3) )
print ( " atomtrain4 . shape " ,

189

atomtrain4 . shape )
190

191

192

193

back_top_left = np . array ([
atomtrain [ i ][: int ( size / 2) ][: , int ( size / 2) :][: ,: , int ( size /
2) :] for i in range ( int ( sim_len_f ) ) ])
atomtrain5 = back_top_left [: ,: ,
:: -1 , :]
atomtrain5b = np . rot90 ( atomtrain5 ,
k =1 , axes =(1 ,2) )
atomtrain5b = np . rot90 ( atomtrain5b
, k =1 , axes =(1 ,3) )

194

atomtrain5c = np . rot90 ( atomtrain5b

195

, k =1 , axes =(1 ,2) )
atomtrain5c = np . rot90 ( atomtrain5c

196

, k =1 , axes =(1 ,3) )
print ( " atomtrain5 . shape " ,

197

atomtrain5 . shape )
198

199

back_top_right = np . array ([
atomtrain [ i ][ int ( size / 2) :][: , int ( size / 2) :][: ,: , int ( size
2) :] for i in range ( int ( sim_len_f ) ) ])
atomtrain6 = back_top_right
[: ,:: -1 , :: -1 , :]

72

/

Vid Sustar

7 APPENDIX

atomtrain6b = np . rot90 ( atomtrain6 ,

200

k =1 , axes =(1 ,2) )
atomtrain6b = np . rot90 ( atomtrain6b

201

, k =1 , axes =(1 ,3) )
202

atomtrain6c = np . rot90 ( atomtrain6b

203

, k =1 , axes =(1 ,2) )
atomtrain6c = np . rot90 ( atomtrain6c

204

, k =1 , axes =(1 ,3) )
print ( " atomtrain6 . shape " ,

205

atomtrain6 . shape )
206

207

208

209

back_bot_right = np . array ([
atomtrain [ i ][ int ( size / 2) :][: , int ( size / 2) :][: ,: ,: int ( size /
2) ] for i in range ( int ( sim_len_f ) ) ])
atomtrain7 = back_bot_right
[: ,:: -1 , :: -1 , :: -1]
atomtrain7b = np . rot90 ( atomtrain7 ,
k =1 , axes =(1 ,2) )
atomtrain7b = np . rot90 ( atomtrain7b
, k =1 , axes =(1 ,3) )

210

atomtrain7c = np . rot90 ( atomtrain7b

211

, k =1 , axes =(1 ,2) )
atomtrain7c = np . rot90 ( atomtrain7c

212

, k =1 , axes =(1 ,3) )
print ( " atomtrain7 . shape " ,

213

atomtrain7 . shape )
214

215

216

217

back_bot_left = np . array ([
atomtrain [ i ][: int ( size / 2) ][: , int ( size / 2) :][: ,: ,: int ( size /
2) ] for i in range ( int ( sim_len_f ) ) ])
atomtrain8 = back_bot_left [: ,: ,
:: -1 , :: -1]
atomtrain8b = np . rot90 ( atomtrain8 ,
k =1 , axes =(1 ,2) )
atomtrain8b = np . rot90 ( atomtrain8b
, k =1 , axes =(1 ,3) )

218

atomtrain8c = np . rot90 ( atomtrain8b

219

, k =1 , axes =(1 ,2) )
atomtrain8c = np . rot90 ( atomtrain8c

220

, k =1 , axes =(1 ,3) )
print ( " atomtrain8 . shape " ,

221

atomtrain8 . shape )
222

223
224

225

226
227

228

229

print ( " atomtrain . shape prior to
concatenation with quadrants " , atomtrain . shape )
print ( atomtrain . shape )
csv_np_array = np . genfromtxt (
input_files_csv [ sasa_k ] , delimiter = " ," , skip_header =1)
csv_ np_arr ay_di st = np .
genfromtxt ( i n p u t _ f i l e s _ c s v _ d i s t [ k ] , delimiter = " ," , skip_header =1)
if 0: # checking the saved stuff
print ( " #########
$ $$ $$ $$ $ $$ $$ $$ $ $$ $ LOADING FROM NPZ : " )
loaded = np . load (
filename_clean + ’ _01 . npz ’)
# GEM11_ s0r0_1 20_01 . npz

73

Vid Sustar

7 APPENDIX

print ( np . array_equal (

230

atomtrain1 , loaded [ ’a ’ ]) )
print ( loaded [ ’a ’ ]. shape )

231
232

print ( np . array_equal (

233

csv_np_array , loaded [ ’b ’ ]) )
print ( loaded [ ’b ’ ]. shape )

234
235

print ( np . array_equal (

236

csv_np_array_dist , loaded [ ’c ’ ]) )
237
238
239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

print ( loaded [ ’c ’ ]. shape )
if 1: # if saving
# filename_clean =
filename_clean [ filename_clean . rfind ( ’/ ’) +1: len ( filename_clean ) -3]
filename_clean = ’/ media /
volume / TF_TEST_20 -50 SS100q_2 / npzs / ’+ typeinp + ’/ ’+ filename_clean
print ( " filename_clean ,
before save " , filename_clean )
np . savez_compressed (
filename_clean + ’ _01 ’ , a = atomtrain1 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _01b ’ , a = atomtrain1b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _01c ’ , a = atomtrain1c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _02 ’ , a = atomtrain2 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _02b ’ , a = atomtrain2b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _02c ’ , a = atomtrain2c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _03 ’ , a = atomtrain3 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _03b ’ , a = atomtrain3b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _03c ’ , a = atomtrain3c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _04 ’ , a = atomtrain4 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _04b ’ , a = atomtrain4b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _04c ’ , a = atomtrain4c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _05 ’ , a = atomtrain5 , b = csv_np_array , c =

74

Vid Sustar

7 APPENDIX

csv_ np_arr ay_di st )
255

256

257

258

259

260

261

262

263

264

265

np . savez_compressed (
filename_clean + ’ _05b ’ , a = atomtrain5b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _05c ’ , a = atomtrain5c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _06 ’ , a = atomtrain6 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _06b ’ , a = atomtrain6b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _06c ’ , a = atomtrain6c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _07 ’ , a = atomtrain7 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _07b ’ , a = atomtrain7b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _07c ’ , a = atomtrain7c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _08 ’ , a = atomtrain8 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _08b ’ , a = atomtrain8b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _08c ’ , a = atomtrain8c , b = csv_np_array , c =
csv_ np_arr ay_di st )

266
267

# atomtrain = np . concatenate ((
atomtrain1 , atomtrain2 , atomtrain3 , atomtrain4 , atomtrain5 ,
atomtrain6 , atomtrain7 , atomtrain8 ) )

268
269

270
271
272
273

274

275

276

277

278

print ( " atomtrain . shape post
concatenation with quadrants " , atomtrain . shape )
else :
atomtrain = front_top_left
if sect :
atomtrain = atomtrain [: ,( nsize width ) : nsize ,: ,: ,] # -- taking the right portion of the matrix
atomtrain = atomtrain . reshape ( sim_size ,
sim_len_f , nsize1 , nsize , nsize )
print ( " atomtrain = atomtrain . reshape (
sim_size , sim_len , size , size , size ) " , atomtrain . shape )
atomtrain = atomtrain . reshape ( list (
atomtrain . shape ) + [1])
print ( " atomtrain = atomtrain . reshape (
list ( atomtrain . shape ) + [1]) " , atomtrain . shape )
dataset . close ()

279

75

Vid Sustar

7 APPENDIX

else : # every non - initial file , gets concatenated

280

to the first one
281
282

283
284
285

286

287

288

289

290

291
292

293
294

295

with h5py . File ( filename , ’r ’) as dataset :
for key in dataset : # if there were more
datasets in one h5 file !
print ( " key from h5py " , key )
print ( " sim_len_f " , sim_len_f )
print ( " type ( dataset [ list ( dataset . keys () )
[0]] " , type ( dataset [ list ( dataset . keys () ) [0]]) )
print ( " dataset [ list ( dataset . keys () ) [0]].
shape " , dataset [ list ( dataset . keys () ) [0]]. shape )
atomtrain2 = dataset [ list ( dataset . keys () )
[0]] # [: sim_len_f ] # taking just first portion of the simulations
# save ( filename_clean + ’ _01 . npy ’,
atomtrain2 )
atomtrain2 = atomtrain2 [0:: interval ] #
subsampling taking just every interval - th matrix ,
# the number of timepoints ( pdbs , rows in
exposure csv ) to be skipped ( to make timedistributed input for
LSTM smaller and hence allow for bigger 3 d matrix )
if quad :
front_top_left = np . array ([ atomtrain2
[ i ][: int ( size / 2) ][: ,: int ( size / 2) :][: ,: , int ( size / 2) :] for
i in range ( int ( sim_len_f / 1) ) ]) # taking just front top left
quadrant of the matrix
atomtrain1 = front_top_left
atomtrain1b = np . rot90 ( atomtrain1 , k
=1 , axes =(1 ,2) )
atomtrain1b = np . rot90 ( atomtrain1b , k
=1 , axes =(1 ,3) )

296

atomtrain1c = np . rot90 ( atomtrain1b , k

297

=1 , axes =(1 ,2) )
atomtrain1c = np . rot90 ( atomtrain1c , k

298

=1 , axes =(1 ,3) )
299
300
301

302

303

304

if conc_quad :
sim_size = 8
front_top_right = np . array ([
atomtrain2 [ i ][ int ( size / 2) :][: ,: int ( size / 2) :][: ,: , int ( size
2) :] for i in range ( int ( sim_len_f / 1) ) ])
atomtrain_2 = front_top_right
[: ,:: -1 , : , :]
atomtrain_2b = np . rot90 (
atomtrain_2 , k =1 , axes =(1 ,2) )
atomtrain_2b = np . rot90 (
atomtrain_2b , k =1 , axes =(1 ,3) )

/

305

atomtrain_2c = np . rot90 (

306

atomtrain_2b , k =1 , axes =(1 ,2) )
atomtrain_2c = np . rot90 (

307

atomtrain_2c , k =1 , axes =(1 ,3) )
308

309

front_bot_left = np . array ([
atomtrain2 [ i ][: int ( size / 2) ][: ,: int ( size / 2) ][: ,: ,: int ( size /
2) ] for i in range ( int ( sim_len_f ) ) ])
atomtrain3 = front_bot_left [: ,: ,
: , :: -1]

76

Vid Sustar

7 APPENDIX

atomtrain3b = np . rot90 ( atomtrain3 ,

310

k =1 , axes =(1 ,2) )
atomtrain3b = np . rot90 ( atomtrain3b

311

, k =1 , axes =(1 ,3) )
312

atomtrain3c = np . rot90 ( atomtrain3b

313

, k =1 , axes =(1 ,2) )
atomtrain3c = np . rot90 ( atomtrain3c

314

, k =1 , axes =(1 ,3) )
315

316

317

318

front_bot_right = np . array ([
atomtrain2 [ i ][ int ( size / 2) :][: ,: int ( size / 2) ][: ,: ,: int ( size /
2) ] for i in range ( int ( sim_len_f ) ) ])
atomtrain4 = front_bot_right
[: ,:: -1 , : , :: -1]
atomtrain4b = np . rot90 ( atomtrain4 ,
k =1 , axes =(1 ,2) )
atomtrain4b = np . rot90 ( atomtrain4b
, k =1 , axes =(1 ,3) )

319

atomtrain4c = np . rot90 ( atomtrain4b

320

, k =1 , axes =(1 ,2) )
atomtrain4c = np . rot90 ( atomtrain4c

321

, k =1 , axes =(1 ,3) )
322

323

324

325

back_top_left = np . array ([
atomtrain2 [ i ][: int ( size / 2) ][: , int ( size / 2) :][: ,: , int ( size /
2) :] for i in range ( int ( sim_len_f ) ) ])
atomtrain5 = back_top_left [: ,: ,
:: -1 , :]
atomtrain5b = np . rot90 ( atomtrain5 ,
k =1 , axes =(1 ,2) )
atomtrain5b = np . rot90 ( atomtrain5b
, k =1 , axes =(1 ,3) )

326

atomtrain5c = np . rot90 ( atomtrain5b

327

, k =1 , axes =(1 ,2) )
atomtrain5c = np . rot90 ( atomtrain5c

328

, k =1 , axes =(1 ,3) )
329

330

331

332

back_top_right = np . array ([
atomtrain2 [ i ][ int ( size / 2) :][: , int ( size / 2) :][: ,: , int ( size /
2) :] for i in range ( int ( sim_len_f ) ) ])
atomtrain6 = back_top_right
[: ,:: -1 , :: -1 , :]
atomtrain6b = np . rot90 ( atomtrain6 ,
k =1 , axes =(1 ,2) )
atomtrain6b = np . rot90 ( atomtrain6b
, k =1 , axes =(1 ,3) )

333

atomtrain6c = np . rot90 ( atomtrain6b

334

, k =1 , axes =(1 ,2) )
atomtrain6c = np . rot90 ( atomtrain6c

335

, k =1 , axes =(1 ,3) )
336

337

back_bot_right = np . array ([
atomtrain2 [ i ][ int ( size / 2) :][: , int ( size / 2) :][: ,: ,: int ( size
2) ] for i in range ( int ( sim_len_f ) ) ])
atomtrain7 = back_bot_right
[: ,:: -1 , :: -1 , :: -1]

77

/

Vid Sustar

7 APPENDIX

atomtrain7b = np . rot90 ( atomtrain7 ,

338

k =1 , axes =(1 ,2) )
atomtrain7b = np . rot90 ( atomtrain7b

339

, k =1 , axes =(1 ,3) )
340

atomtrain7c = np . rot90 ( atomtrain7b

341

, k =1 , axes =(1 ,2) )
atomtrain7c = np . rot90 ( atomtrain7c

342

, k =1 , axes =(1 ,3) )
343

344

345

346

back_bot_left = np . array ([
atomtrain2 [ i ][: int ( size / 2) ][: , int ( size / 2) :][: ,: ,: int ( size /
2) ] for i in range ( int ( sim_len_f / 1) ) ])
atomtrain8 = back_bot_left [: ,: ,
:: -1 , :: -1]
atomtrain8b = np . rot90 ( atomtrain8 ,
k =1 , axes =(1 ,2) )
atomtrain8b = np . rot90 ( atomtrain8b
, k =1 , axes =(1 ,3) )

347

atomtrain8c = np . rot90 ( atomtrain8b

348

, k =1 , axes =(1 ,2) )
atomtrain8c = np . rot90 ( atomtrain8c

349

, k =1 , axes =(1 ,3) )
350

351

352

print ( " atomtrain2 . shape prior to
concatenation with quadrants " , atomtrain2 . shape )
# atomtrain2 = np . concatenate ((
atomtrain1 , atomtrain_2 , atomtrain3 , atomtrain4 , atomtrain5 ,
atomtrain6 , atomtrain7 , atomtrain8 ) )
print ( " atomtrain2 . shape post
concatenation with quadrants " , atomtrain2 . shape )

353
354

# csv_np_array = np . genfromtxt (
input_files_csv [ sasa_k ] , delimiter =" ," , skip_header =1)

355

df = pd . DataFrame ( pd . read_csv (

356

input_files_csv [ sasa_k ]) )
for i in range ( len ( df . columns )

357

-1) :
358

359

df . iloc [: , i +1] = df . iloc [: , i
+1]. rolling ( window = wind , min_periods =1) . mean ()
csv_np_array = df . to_numpy ()

360
361
362

363
364

365

366

367

368

csv_ np_arr ay_di st = np .
genfromtxt ( i n p u t _ f i l e s _ c s v _ d i s t [ k ] , delimiter = " ," , skip_header =1)
if 1: # if saving
# filename_clean =
filename_clean [ filename_clean . rfind ( ’/ ’) +1: len ( filename_clean ) -3]
filename_clean = ’/ media /
volume / TF_TEST_20 -50 SS100q_2 / npzs / ’+ typeinp + ’/ ’+ filename_clean
# print ("############
filename_clean " , filename_clean )
print ( " filename_clean ,
before save " , filename_clean )
np . savez_compressed (
filename_clean + ’ _01 ’ , a = atomtrain1 , b = csv_np_array , c =

78

Vid Sustar

7 APPENDIX

csv_ np_arr ay_di st )
369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

np . savez_compressed (
filename_clean + ’ _01b ’ , a = atomtrain1b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _01c ’ , a = atomtrain1c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _02 ’ , a = atomtrain_2 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _02b ’ , a = atomtrain_2b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _02c ’ , a = atomtrain_2c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _03 ’ , a = atomtrain3 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _03b ’ , a = atomtrain3b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _03c ’ , a = atomtrain3c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _04 ’ , a = atomtrain4 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _04b ’ , a = atomtrain4b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _04c ’ , a = atomtrain4c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _05 ’ , a = atomtrain5 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _05b ’ , a = atomtrain5b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _05c ’ , a = atomtrain5c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _06 ’ , a = atomtrain6 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _06b ’ , a = atomtrain6b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _06c ’ , a = atomtrain6c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _07 ’ , a = atomtrain7 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (

79

Vid Sustar

388

389

390

391

392
393
394
395

396

397

398

399
400

401

402

403
404

405

406
407
408

409

7 APPENDIX

filename_clean + ’ _07b ’ , a = atomtrain7b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _07c ’ , a = atomtrain7c , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _08 ’ , a = atomtrain8 , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _08b ’ , a = atomtrain8b , b = csv_np_array , c =
csv_ np_arr ay_di st )
np . savez_compressed (
filename_clean + ’ _08c ’ , a = atomtrain8c , b = csv_np_array , c =
csv_ np_arr ay_di st )
else :
atomtrain2 = front_top_left
if sect :
atomtrain2 = atomtrain2 [: ,(
nsize - width ) : nsize ,: ,: ,] # -- taking the right portion of the
matrix
atomtrain2 = atomtrain2 . reshape ( sim_size ,
sim_len_f , nsize1 , nsize , nsize )
atomtrain2 = atomtrain2 . reshape ( list (
atomtrain2 . shape ) + [1])
print ( " prior to np . concatenate ((
atomtrain , atomtrain2 ) , axis = 0) " )
print ( " atomtrain . shape " , atomtrain . shape )
print ( " atomtrain2 . shape " , atomtrain . shape
)
# atomtrain = np . concatenate (( atomtrain ,
atomtrain2 ) , axis = 0)
print ( " asizeof . asizeof ( atomtrain ) /1000
000 000( Gb ) " , int ( asizeof . asizeof ( atomtrain ) ) /1000000000)
print ( " postconc " )
print ( " atomtrain2 . shape " , atomtrain2 .
shape )
print ( " atomtrain . shape " , atomtrain . shape
)
dataset . close ()
except :
print ( " error occured !
55555555555555555555555
**************** " )
return atomtrain

410
411
412

413
414
415

416
417
418

def make_input_expo ( path ) :
print ( " \ n \n - exposure - - CSV - - exposure - - CSV - - exposure - - CSV - exposure - - CSV - - exposure - - CSV - - exposure - - CSV - - exposure - - CSV - -\ n " )
import csv
input_files_csv = sorted ( glob . glob ( path + " *. csv " ) )
for k , filename_csv in enumerate ( input_files_csv ) : # enumerate is
for greater control , to put some extra contraints on steps of for
loop
if 1: # k ==0: # for testing ! ( set to if 1: otherwise )
print ( " FILENAME : " , filename_csv )
drugratio = filename_csv [ filename_csv . rfind ( ’/ ’) +4:

80

Vid Sustar

419

420
421
422

423
424

425
426

427
428

429
430

431
432
433
434
435
436
437
438
439
440
441
442
443
444

7 APPENDIX

filename_csv . rfind ( ’/ ’) +5] # finding the ratio in the filename if
the filename is in format / PAN11 fore example
ssratio = filename_csv [ filename_csv . rfind ( ’/ ’) +5:
filename_csv . rfind ( ’/ ’) +6] # finding the ratio in the filename if
the filename is in format / PAN11 fore example
print ( " K : " ,k )
ratio_add = np . array ([ int ( drugratio ) , int ( ssratio ) ])
if " PAN " in filename_csv : # adding the information about
drug # needs to be refined , reall one hot encoding
drug_add = np . array ([1 , 0 , 0 , 0 , 0])
if (( " OQL " in filename_csv ) or ( " S1 " in filename_csv ) ) :
# adding the information about drug # needs to be refined , reall
one hot encoding
drug_add = np . array ([0 , 1 , 0 , 0 , 0])
if " GEM " in filename_csv : # adding the information about
drug # needs to be refined , reall one hot encoding
drug_add = np . array ([0 , 0 , 1 , 0 , 0])
if " NCL " in filename_csv : # adding the information about
drug # needs to be refined , reall one hot encoding
drug_add = np . array ([0 , 0 , 0 , 1 , 0])
if " NHQ " in filename_csv : # adding the information about
drug # needs to be refined , reall one hot encoding
drug_add = np . array ([0 , 0 , 0 , 0 , 1])
if ( k ==0) : # initial file
exp_val = np . array ([])
arr_len = 0
# expo_train = np . array ([])
with open ( filename_csv ) as csv_file :
csv_reader = csv . reader ( csv_file , delimiter = ’ , ’)
line_count = 0
for row in csv_reader :
if line_count > 0:
row = [ float ( v ) for v in row ]
row = np . array ( row )
row = np . concatenate ([ ratio_add , row ])
row = np . concatenate ([ drug_add , row ])

445
446
447
448
449
450
451
452
453

454
455
456
457

458

459

460

# print (" row " , row )
# print (" row . shape " , row . shape )
if len ( exp_val ) == 0:
exp_val = row
arr_len = len ( row )
else :
exp_val = np . vstack (( exp_val , row ) )
if ( line_count > full_sim_len ) : # taking just
first portion of the simulations ) :
break
line_count += 1
exp_train = exp_val . copy () [: sim_len_f ]
print ( " exp_train = exp_val . copy () [0: -1] " , exp_train .
shape )
exp_train = exp_train [0:: interval ] # subsampling taking
just every interval - th matrix ,
print ( " exp_train = exp_train [0:: interval ] " , exp_train .
shape )
# the number of timepoints ( pdbs , rows in exposure

81

Vid Sustar

461
462

463

464
465

466

467
468

469
470

7 APPENDIX

csv ) to be skipped ( to make timedistributed input for LSTM
smaller and hence allow for bigger 3 d matrix )
exp_out = exp_val . copy () [ - sim_len_f :] # [1:]
exp_out = exp_out [0:: interval ] # subsampling taking
just every interval - th matrix ,
exp_train = exp_train . reshape ( sim_size , sim_len_f ,
arr_len )
if conc_quad :
exp_train = np . concatenate (( exp_train , exp_train ,
exp_train , exp_train , exp_train , exp_train , exp_train , exp_train ) ) #
simply concatenating - oktadupling the exposure input
print ( " exp_train = exp_train . reshape ( sim_size , sim_len
, arr_len ) " , exp_train . shape )
# exp_out = exp_out . reshape ( sim_size , 1 , arr_len )
exp_out = exp_out . reshape ( sim_size , sim_len_f , arr_len
)
if conc_quad :
exp_out = np . concatenate (( exp_out , exp_out , exp_out ,
exp_out , exp_out , exp_out , exp_out , exp_out ) )

471

print ( " exp_out . shape " , exp_out . shape )
# save ( filename_clean + ’ _02 . npy ’, atomtrain2 )
else : # every non - initial file , gets concatenated to the

472
473
474

first one
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493

494
495
496
497

498

499
500

501

exp_val = np . array ([])
arr_len = 0
# expo_train = np . array ([])
with open ( filename_csv ) as csv_file :
csv_reader = csv . reader ( csv_file , delimiter = ’ , ’)
line_count = 0
for row in csv_reader :
if line_count > 0:
row = [ float ( v ) for v in row ]
row = np . array ( row )
row = np . concatenate ([ ratio_add , row ])
row = np . concatenate ([ drug_add , row ])
# print (" row " , row )
if len ( exp_val ) == 0:
exp_val = row
arr_len = len ( row )
else :
exp_val = np . vstack (( exp_val , row ) )
if ( line_count > sim_len_f ) : # taking just
first portion of the simulations ) :
break
line_count += 1
exp_train2 = exp_val . copy () [: sim_len_f ]
exp_train2 = exp_train2 [0:: interval ] # subsampling
taking just every interval - th matrix ,
exp_train2 = exp_train2 . reshape ( sim_size , sim_len_f ,
arr_len )
exp_out2 = exp_val . copy () [ - sim_len_f :]
exp_out2 = exp_out2 [0:: interval ] # subsampling taking
just every interval - th matrix ,
exp_train2 = exp_train2 . reshape ( sim_size , sim_len_f ,
arr_len )

82

Vid Sustar

502
503

504
505
506

507
508

509
510

511
512

513

7 APPENDIX

# if conc_quad :
# exp_train2 = np . concatenate (( exp_train2 ,
exp_train2 , exp_train2 , exp_train2 , exp_train2 , exp_train2 , exp_train2
, exp_train2 ) )
print ( " exp_train2 . shape " , exp_train2 . shape )
# exp_out2 = exp_out2 . reshape ( sim_size , 1 , arr_len )
exp_out2 = exp_out2 . reshape ( sim_size , sim_len_f ,
arr_len )
# if conc_quad :
# exp_out2 = np . concatenate (( exp_out2 , exp_out2 ,
exp_out2 , exp_out2 , exp_out2 , exp_out2 , exp_out2 , exp_out2 ) )
print ( " exp_out2 . shape " , exp_out2 . shape )
exp_train = np . concatenate (( exp_train , exp_train2 ) ,
axis = 0)
print ( " exp_train . shape " , exp_train . shape )
exp_out = np . concatenate (( exp_out , exp_out2 ) , axis =
0)
print ( " exp_out . shape " , exp_out . shape )

514
515

516
517

518

exp_out = exp_out [: ,: ,: exp_n_col_y ] # taking just first four
columns ( so up to , including 2.2 Hdn ) out of 15
print ( " FINAL !!!!!!!!!!! exp_out . shape " , exp_out . shape )
ret urn_tr ain_te st = [ exp_train , exp_out , arr_len ] # , 4 because of
above , taking just first four columns instead of 15...
return ( re turn_t rain_t est )

519
520
521

# calling functions to import the train and test data
print ( " CHECKCHEKCHC ########### CHEKCHC ##### CHEKCHC ##### CHEKCHC #####
CHEKCHC ##### CHEKCHC ########## " )

522
523
524
525

print ( " TRAIN DATA : " )
atomtrain_inp = make_input_3d ( pathtrain )

Listing 6: Python script to flip the quadrants of NP in Numpy 3D matrices and
combine with distances and SASA exposure into Numpy compressed (.npz) files to
be feed into the model.
1
2

3

# run in Dione terminal with example line :" srun -p gpu -- nodelist =
di37 -t 99:00:00 -- mem =20 G python3
C N N _ L S T M _ 2 9 _ 3 i n p _ 4 c o n v _ d r p 0 5 _ p d _ 5 1 2 k r n _ d . py 120 s1r1 3 1 1 0 0 0
60 10 1 299 299 0 3 mae 100 1 5
c o n v d r p 0 5 p r e L S T M d r p L S T M 2 5 6 n e u r _ 2 9 9 T M P N T S i n t e r v a l 3 0 256 1 & >
printout_5conv_drp05_pd_512krn_5convdrp05p256neur_299TMPNTSinterval3
. out &
# change the parameters as explained bellow # INPUT PARAMETERS

4
5

6
7

8
9
10

from tensorflow . keras . layers import Conv3D , MaxPooling3D , Flatten ,
Dense , LSTM , TimeDistributed
from tensorflow . keras . layers import Concatenate
from tensorflow . keras . layers import Dropout , Input ,
Ba tc hN or ma li za ti on
from tensorflow . keras . losses import c a t e g o r i c a l _ c r o s s e n t r o p y
from tensorflow . keras . optimizers import Adadelta
from tensorflow . keras . models import Model , Sequential

83

Vid Sustar

11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30

7 APPENDIX

from tensorflow . keras import models
from tensorflow . keras import utils
from tensorflow . keras . callbacks import CSVLogger
from tensorflow . keras . callbacks import EarlyStopping
import tensorflow as tf
import numpy as np
import tensorflow . keras
import h5py
import math
import time
from pympler import asizeof # to check the memory size of objects
from numpy import load
import csv
import sys
import os # psutil
import datetime
import shutil
import glob
import subprocess as sp
import random

31
32
33
34
35
36
37
38
39
40
41
42

from datetime import datetime
start_time = datetime . now ()
# PLOTING WITH MATPLOT
import matplotlib
matplotlib . use ( ’ Agg ’)
import matplotlib . pyplot as plt
fig = plt . figure ()
ax = fig . add_subplot (111)
ax . plot ([1 ,2 ,3])
fig . savefig ( ’ test . png ’)

43
44

cn n_ fi n_ de ns e_ si ze =17 # ### number of columns from final exposure y

45
46
47

# INPUT PARAMETERS

48
49
50

51

52

53

size = int ( sys . argv [1]) # the size of the input matrix
typeinp = str ( sys . argv [2]) # the type of the matrix , whether there are
drugcentres , supsubscentres , other molecule centres etc
interval = int ( sys . argv [3]) # the number of timepoints ( pdbs , rows in
exposure csv ) to be skipped ( to make timedistributed input for
LSTM smaller and hence allow for bigger 3 d matrix )
quad = int ( sys . argv [4]) # quadrant (1) or no (0) , taking just one
eighth front top left quadrant of 3 D matrix , to lower the size of
input
conc_quad = int ( sys . argv [5]) # if one wants to take the other 7
quadrants into consideration , it will concatenate them !

54
55

56

57

sect = int ( sys . argv [6]) # sectioning of the 3 d matrix ( can be quadrant
even further ) (1 - yes ) (0 - no ) , taking the right most portion
of the matrix , see below
width = int ( sys . argv [7]) # width of the section from above , how many
cells , how much of the right most portion of the matrix
cpu = int ( sys . argv [8]) # CPU mode - disable usage of GPU ! (1 - CPU , 0

84

Vid Sustar

58

59
60
61

62
63
64
65
66
67
68
69

70

71

72
73
74

75
76
77

7 APPENDIX

- GPU )
cn n_ fi n_ de ns e_ si ze = int ( sys . argv [9]) # ### number of columns from
final exposure y
num_epochs = int ( sys . argv [10]) # number of epochs
batch_size = int ( sys . argv [11])
sim_len_f = int ( sys . argv [12]) # input length ( the output should be of
same length )
sim_len_f = math . ceil ( sim_len_f / interval )
out_len = int ( sys . argv [13]) # numb of inputs per LSTM
out_len = math . ceil ( out_len / interval )
C U D A _ V I S I B L E _ D E V I C E S = str ( sys . argv [14]) # either 1 or 0
kernel_size = int ( sys . argv [15])
loss_funct = str ( sys . argv [16]) # mae , mse !
average = int ( sys . argv [17]) # input_how much it was averaged
files_nth = int ( sys . argv [18]) # taking just every nth file from original
list , for testing purposes do not want to run through all the
files for faster outcome
u n i q u e _ a b o u t _ t h i s _ r u n = str ( sys . argv [19]) # write some unique
descriptive notion about this training used for discerning saved
models etc
load_weights = int ( sys . argv [20]) # whether this training should load
previous loads
n_neurons = int ( sys . argv [21]) # number of neurons in LSTM layer
fit_model = int ( sys . argv [22]) # whether to fit the model or not
if quad : # adjusting reported size of input matrix according to
taking only quadrants
nsize = int ( size /2)
else :
nsize = size

78
79

80
81
82

if sect : # adjusting the first dimension ( width ) of 3 d matrix
appropriatelty according to sectioning
nsize1 = width
else :
nsize1 = nsize

83
84
85

86

# SAVING THE INFORMATION ON TRAINING
path_new_folder = os . getcwd () + " / " + str ( sim_len_f ) + " _ " + str ( width ) + " _ " +
typeinp + " _ " + str ( kernel_size ) + " _ " + loss_funct + " _ " + str ( average ) + " _ " +
datetime . now () . strftime ( " % Y % m %d -% H % M % S " ) + u n i q u e _ a b o u t _ t h i s _ r u n + "
/"
os . mkdir ( path_new_folder )

87
88
89
90
91
92

93

94

95
96

# function to print the printout also into a file
def fprint (* argv ) :
for arg in argv :
print ( arg )
original_stdout = sys . stdout # Save a reference to the original
standard output
with open ( path_new_folder + ’/ ’+ str ( sim_len_f ) + " _ " + str ( width ) + " _ " +
typeinp + " _ " + str ( kernel_size ) + " _ " + loss_funct + " _ " + str ( average ) + ’.
txt ’ , ’a ’) as f :
sys . stdout = f # Change the standard output to the file we
created .
for arg in argv :
print ( arg )

85

Vid Sustar

97

7 APPENDIX

sys . stdout = original_stdout # Reset the standard output to
its original value

98
99
100
101
102
103
104
105
106
107
108
109

110
111
112

113
114

115
116
117

fprint ( " >>>>>>>> INPUT PARAMETERS : " )
fprint ( " datetime . now () ," , datetime . now () )
fprint ( " size : ," , size )
fprint ( " typeinp : ," , typeinp )
fprint ( " interval between timepoints : ," , interval )
fprint ( " use of only single quadrant of matrix bool : ," , quad )
fprint ( " use only section of matrix bool : ," , sect )
fprint ( " width of section : ," , width )
fprint ( " usage of cpu only bool : ," , cpu )
fprint ( " cnn _f in _d en se _s iz e : ," , cn n_f in _d en se _s iz e ) # ### number of
columns from final exposure y
fprint ( " num_epochs : ," , num_epochs ) # number of epochs
fprint ( " batch_size : ," , batch_size ) # number of epochs
fprint ( " sim_len_f : ," , sim_len_f ) # input length ( the output should be
of same length )
fprint ( " out_len : ," , out_len ) # numb of inputs per LSTM
fprint ( " C U D A _ V I S I B L E _ D E V I C E S : ," , C U D A _ V I S I B L E _ D E V I C E S ) # either 1 or
0
fprint ( " kernel_size : ," , kernel_size )
fprint ( " loss_funct : ," , loss_funct ) # mae , mse !
fprint ( " average : ," , average ) # input_how much it was averaged

118
119
120

path = os . getcwd () + ’/ npzs / ’+ str ( typeinp ) + ’/ ’# sys . argv [3] # path to the
input # make sure the train data in train folder !

121
122
123
124
125

if cpu : # disabling gpu to force run it on cpu
os . environ [ " CU DA_DEV ICE_OR DER " ] = " PCI_BUS_ID "
os . environ [ " C U D A _ V I S I B L E _ D E V I C E S " ] = " "

126
127
128

gpus = tf . config . l i s t _ p h y s i c a l _ d e v i c e s ( ’ GPU ’)

129
130
131
132
133
134
135
136

# giving chmod rights to files in path
def recursive_chmod ( path , permission ) : # 0 o777
for dirpath , dirnames , filenames in os . walk ( path ) :
os . chmod ( dirpath , permission )
for filename in filenames :
os . chmod ( os . path . join ( dirpath , filename ) , permission )

137
138
139
140
141

142
143
144
145
146

def gpu_memory_usage ( gpu_id ) :
command = r " nvidia - smi -- id =0 -- query - gpu = memory . used -- format =
csv "
output_cmd = sp . check_output ( command . split () )
memory_used = output_cmd . decode ( " ascii " ) . split ( " \ n " ) [1]
# Get only the memory part as the result comes as ’10 MiB ’
memory_used = int ( memory_used . split () [0])
return memory_used

86

Vid Sustar

7 APPENDIX

147
148
149

150
151
152
153
154

def gpu_memory_total ( gpu_id ) :
command = r " nvidia - smi -- id =0 -- query - gpu = memory . total -- format =
csv "
output_cmd = sp . check_output ( command . split () )
memory_total = output_cmd . decode ( " ascii " ) . split ( " \ n " ) [1]
# Get only the memory part as the result comes as ’10 MiB ’
memory_total = int ( memory_total . split () [0])
return memory_total

155
156
157

# The gpu you want to check
gpu_id = 0

158
159
160

i n i t i a l _ m e m o r y _ u s a g e = gpu_memory_usage ( gpu_id )
i n i t i a l _ m e m o r y _ t o t a l = gpu_memory_total ( gpu_id )

161
162

# Set up the gpu specified

163
164
165
166

gpu_devices = tf . config . experimental . l i s t _ p h y s i c a l _ d e v i c e s ( ’ GPU ’)
for device in gpu_devices :
tf . config . experimental . se t_memo ry_gro wth ( device , True )

167
168
169
170
171
172

173
174
175

176

sim_size = 1
full_sim_len =300
sim_len_init = 150 # because we are using one timepoint to predict
the next one ,
sim_len = math . ceil ( sim_len_init / interval )
# sim_len_f = 150 # input length ( the output should be of same length )
# so if there are 300 timepoints , the last needs to be used for
prediction ... and we set sim_len 299
sample_shape = ( sim_len_f , size , size , size )

177
178
179
180
181

files = glob . glob ( path + " *. npz " )
print ( " len ( files ) pre " , len ( files ) )
files = files [0:: files_nth ]
print ( " len ( files ) post " , len ( files ) )

182
183
184

# filtering for the files that have the correct averraging :
files_filt =[]

185
186
187

188

189
190
191

for s_file in files :
seclast_index = s_file [: s_file . rfind ( " _ " ) ]. rfind ( " _ " ) # looking for
second last occurence of " _ " , in file like "
S 1 _ 1 1 R 4 _ 3 _ s 1 r 1 _ 5 0 _ 5 0 _ 0 3 c . npz "
last_index = s_file . rfind ( " _ " ) # looking for last occurence of " _ " ,
in file like " S 1 _ 1 1 R 4 _ 3 _ s 1 r 1 _ 5 0 _ 5 0 _ 0 3 c . npz "
f i le s _ cu r _ av e r ag i n g = int ( s_file [( seclast_index +1) : last_index ])
if fi l e s_ c u r _a v e ra g i ng == average :
files_filt . append ( s_file )

192
193

files = files_filt

194
195

print ( " FILES POST FILTERING " , files )

196

87

Vid Sustar

197
198
199
200
201

202

7 APPENDIX

fprint ( " CHECK THIS ! , " )
fprint ( ’ path , ’ , path )
fprint ( ’ type ( files ) , ’ , type ( files ) )
fprint ( ’ len ( files ) , ’ , len ( files ) )
test_t =0 # #### IMPORTANT CHECK CHANGE to print the filenames of test
files
test_t_filenames =[]

203
204
205
206

207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222

# data generator function that feeds tensorflow data on the fly
def t f_data _gener ator ( file_list , sim_len_f , test_t , batch_size =
batch_size ) :
print ( " DGW1 " )
i = 0
fprint ( " len ( file_list ) ," , len ( file_list ) )
random . shuffle ( file_list )
while True :
if i * batch_size >= len ( file_list ) :
i = 0
np . random . shuffle ( file_list )
else :
file_chunk = file_list [ i * batch_size :( i +1) * batch_size ]
# data = []
atom_x = np . array ([])
exp_x = np . array ([])
dist_x = np . array ([])
exp_y = np . array ([])
for k , file in enumerate ( file_chunk ) :

223
224
225
226
227
228

229
230
231

232

print ( " k " ,k )
temp = np . load ( file )
filename = str ( file )
if test_t :
filename_sh = filename [( filename . rfind ( " / " ) +1) :
filename . rfind ( " . npz " ) ]
print ( " !!! in gen TEST file : " , filename_sh )
test_t_filenames . append ( filename_sh )
drugratio = tf . constant ( filename [ filename . rfind ( " / " )
+4: filename . rfind ( " / " ) +5]) # finding the ratio in the filename if
the filename is in format / PAN11 fore example
ssratio = tf . constant ( filename [ filename . rfind ( " / " ) +5:
filename . rfind ( " / " ) +6]) # finding the ratio in the filename if the
filename is in format / PAN11 fore example

233
234

235

236
237

238
239

240

ratio_add = np . array ([ int ( drugratio ) , int ( ssratio ) ]) #
creating an array that informs of the drug ratio
if " PAN " in filename : # adding the information about
drug # needs to be refined , reall one hot encoding
drug_add = np . array ([1 , 0 , 0 , 0 , 0])
if (( " OQL " in filename ) or ( " S1 " in filename ) ) : #
adding the information about drug # needs to be refined , reall one
hot encoding
drug_add = np . array ([0 , 1 , 0 , 0 , 0])
if " GEM " in filename : # adding the information about
drug # needs to be refined , reall one hot encoding
drug_add = np . array ([0 , 0 , 1 , 0 , 0])

88

Vid Sustar

241

242
243

244
245

7 APPENDIX

if " NCL " in filename : # adding the information about
drug # needs to be refined , reall one hot encoding
drug_add = np . array ([0 , 0 , 0 , 1 , 0])
if " NHQ " in filename : # adding the information about
drug # needs to be refined , reall one hot encoding
drug_add = np . array ([0 , 0 , 0 , 0 , 1])
# # # # # # # # # # # # # # # # # # # # # ADD OTHER DRUG TYPES ! ,
expand the array !

246

drug_add = np . concatenate ([ ratio_add , drug_add ]) # JOIN

247

ratio + drug
248
249
250
251
252

253
254
255

256

257

if k == 0:
# atom_3d_matrix_numpy_array
atom_x = temp [ ’a ’]
atom_x = atom_x [0:: interval ] # subsampling taking
just every interval - th matrix ,
atom_x = atom_x [: sim_len_f ]
if sect :
atom_x = atom_x [: ,( nsize - width ) : nsize ,: ,: ,] #
-- taking the right portion of the matrix
atom_x = atom_x . reshape ( sim_size , sim_len_f ,
nsize1 , nsize , nsize )
atom_x = atom_x . reshape ( list ( atom_x . shape ) + [1])

258
259
260

261
262

drug_add = np . expand_dims ( drug_add , axis =0)
drug_static = np . repeat ( drug_add , repeats =
full_sim_len , axis =0) # full_sim_len = 300 # creating an array that
informs of the drug type , one hot encoding
drug_static = drug_static [0:: interval ]
drug_static_len = drug_static . shape [ -1]

263
264
265

drug_static = drug_static [: sim_len_f ]
drug_static = drug_static . reshape ( sim_size ,
sim_len_f , drug_static_len )

266
267
268

# exposures_matrix_numpy_array
exp_temp = temp [ ’b ’]

269
270

271
272

exp_temp = exp_temp [0:: interval ] # subsampling
taking just every interval - th matrix ,
arr_len = exp_temp . shape [ -1]
exp_x = exp_temp [: sim_len_f ]

273
274

275

276

277
278

exp_y = exp_temp [ - out_len :] # taking the last
portion of the exposures of the size out_len specified by user
input
exp_x = exp_x . reshape ( sim_size , sim_len_f , arr_len
)
exp_y = exp_y [: , -1] # keeping just the
percentage of SASA
exp_y = exp_y . reshape ( sim_size , out_len , 1)
exp_y = np . round ( exp_y , 2)

279
280
281

dist_temp = temp [ ’c ’]
dist_temp = dist_temp [0:: interval ] # subsampling

89

Vid Sustar

282

7 APPENDIX

taking just every interval - th matrix ,
dist_x = dist_temp [: sim_len_f ]

283

dist_len = dist_temp . shape [ -1]

284
285

dist_x = dist_x . reshape ( sim_size , sim_len_f ,

286

dist_len )
287
288
289
290
291

292
293
294

295

296

297

if k !=0:
atom_x_temp = temp [ ’a ’]
atom_x_temp = atom_x_temp [0:: interval ] #
subsampling taking just every interval - th matrix ,
atom_x_temp = atom_x_temp [: sim_len_f ]
if sect :
atom_x = atom_x [: ,( nsize - width ) : nsize ,: ,: ,] #
-- taking the right portion of the matrix
atom_x_temp = atom_x_temp . reshape ( sim_size ,
sim_len_f , nsize1 , nsize , nsize )
atom_x_temp = atom_x_temp . reshape ( list ( atom_x_temp
. shape ) + [1])
atom_x = np . concatenate (( atom_x , atom_x_temp ) ,
axis = 0)

298
299
300
301

302
303
304
305

306

307
308

309
310

311

312
313

314

315
316

317
318

319

drug_add = np . expand_dims ( drug_add , axis =0)
drug_static_temp = np . repeat ( drug_add , repeats =
full_sim_len , axis =0) # full_sim_len = 300 # creating an array that
informs of the drug type , one hot encoding
drug_static = drug_static [0:: interval ]
drug_static_len = drug_static_temp . shape [ -1]
drug_static_temp = drug_static_temp [: sim_len_f ]
drug_static_temp = drug_static_temp . reshape (
sim_size , sim_len_f , drug_static_len )
drug_static = np . concatenate (( drug_static ,
drug_static_temp ) , axis = 0)
exp_temp = temp [ ’b ’]
exp_temp = exp_temp [0:: interval ] # subsampling
taking just every interval - th matrix ,
exp_x_temp = exp_temp [: sim_len_f ]
exp_x_temp = exp_x_temp . reshape ( sim_size ,
sim_len_f , arr_len )
exp_x = np . concatenate (( exp_x , exp_x_temp ) , axis
= 0)
exp_y_temp = exp_temp [ - out_len :]
exp_y_temp = exp_y_temp [: , -1] # keeping just the
percentage of SASA
exp_y_temp = exp_y_temp . reshape ( sim_size , out_len ,
1)
exp_y_temp = np . round ( exp_y_temp , 2)
exp_y = np . concatenate (( exp_y , exp_y_temp ) , axis
= 0)
dist_temp = temp [ ’c ’]
dist_temp = dist_temp [0:: interval ] # subsampling
taking just every interval - th matrix ,
dist_x_temp = dist_temp [: sim_len_f ]

90

Vid Sustar

320

321

7 APPENDIX

dist_x_temp = dist_x_temp . reshape ( sim_size ,
sim_len_f , dist_len )
dist_x = np . concatenate (( dist_x , dist_x_temp ) ,
axis = 0)

322
323

324

325
326
327
328
329

330

331
332
333
334

335
336
337
338
339
340

if test_t : # ## just a small function to list all the test
files used to be compared later
with open ( " t e s t _ f i l e s _ l i s t _ i n _ g e n . csv " , ’w ’ , newline
= ’ ’) as myfile :
wr = csv . writer ( myfile , quoting = csv . QUOTE_ALL )
wr . writerow ( test_t_filenames )
late st_gp u_memo ry = gpu_memory_usage ( gpu_id )
l at e s t_ m e m or y _ to t a l = gpu_memory_total ( gpu_id )
print ( " ( GPU %: " , 100*( lates t_gpu_ memory i n i t i a l _ m e m o r y _ u s a g e ) / l at e s t_ m e mo r y _t o t al )
exp_x = exp_x [: ,: ,:3] # take just last 3 columns of
exposure !
yield (( atom_x , exp_x , dist_x , drug_static ) , exp_y )
i = i + 1
print ( " preDGW1 " )
check_data = tf_da ta_gen erator ( files , sim_len_f , test_t , batch_size =
1)
num = 0
print ( " postDGW1 " )
arr_len =0
arr_len_y =0
drug_st_len =0
fprint ( " arr_len , " , arr_len )

341
342
343
344

345
346
347
348
349
350
351

# to get the parameters for the dimensions of model input
for ( atom_x , exp_x , dist_x , drug_static ) , exp_y in check_data :
fprint ( " check_data : atom_x , exp_x , dist_x , exp_y , drug_static , " ,
atom_x . shape , exp_x . shape , dist_x . shape , exp_y . shape , drug_static
. shape )
arr_len = exp_x . shape [ -1]
arr_len_y = exp_y . shape [ -1]
arr_len_dist = dist_x . shape [ -1]
drug_st_len = drug_static . shape [ -1]
print ()
num = num + 1
if num > 5: break

352
353
354
355
356

357

# spliting the data into train , and val
from sklearn . model_selection import train_test_split
train , test = train_test_split ( files , test_size = int ( len ( files )
*0.05) , random_state = 54321) # check the test size etc , I took
10% of all !
train , val = train_test_split ( files , test_size = int ( len ( files )
*0.05) , random_state = 12345) # check the val size etc , I took 10%
of all !

358
359

random . shuffle ( train )

360
361
362

91

Vid Sustar

363
364
365

7 APPENDIX

fprint ( " Number of train_files : , " , len ( train ) )
fprint ( " Number of validation_files : , " , len ( val ) )
fprint ( " Number of test_files : , " , len ( test ) )

366
367

batch_size = 1

368
369
370

print ( " === > Num GPUs Available : " , len ( tf . config .
l i s t_ p h y s i c a l _ d e v i c e s ( ’ GPU ’) ) )

371
372

if 1:

373
374

375

376

377

378
379

380

train_dataset = tf . data . Dataset . from_generator ( tf_data_generator
, args = [ train , sim_len_f , test_t , batch_size ] , output_types = (( tf .
float32 , tf . float32 , tf . float32 , tf . float32 ) , tf . float32 ) ,
output_shapes =
((( None , sim_len_f , nsize1 , 60 , 60 , 1) ,( None , sim_len_f , 3) ,( None ,
sim_len_f , 42) ,( None , sim_len_f , 7) ) ,( None , out_len , 1) ) )
v ali da ti on _d at as et = tf . data . Dataset . from_generator (
tf_data_generator , args = [ val , sim_len_f , test_t , batch_size ] ,
output_types = (( tf . float32 , tf . float32 , tf . float32 , tf . float32 ) , tf
. float32 ) ,
output_shapes =
((( None , sim_len_f , nsize1 , 60 , 60 , 1) ,( None , sim_len_f , 3) ,( None ,
sim_len_f , 42) ,( None , sim_len_f , 7) ) ,( None , out_len , 1) ) )
test_t =1
test_dataset = tf . data . Dataset . from_generator ( tf_data_generator ,
args = [ test , sim_len_f , test_t , batch_size ] , output_types = (( tf .
float32 , tf . float32 , tf . float32 , tf . float32 ) , tf . float32 ) ,
output_shapes =
((( None , sim_len_f , nsize1 , 60 , 60 , 1) ,( None , sim_len_f , 3) ,( None ,
sim_len_f , 42) ,( None , sim_len_f , 7) ) ,( None , out_len , 1) ) )

381
382
383
384

385
386
387
388
389
390
391
392

# FOR CHECKING PURPOSES SOME PRINTOUTS !
fprint ( " TEST DATASET CHECK : , " )
for ( atom_x , exp_x , dist_x , drug_static ) , exp_y in test_dataset .
take (2) :
fprint ( " atom_x . shape ," , atom_x . shape )
fprint ( ’ type ( atom_x ) , ’ , type ( atom_x ) )
fprint ( ’ exp_x . shape , ’ , exp_x . shape )
fprint ( ’ exp_x , ’ , exp_x )
fprint ( ’ type ( exp_x ) , ’ , type ( exp_x ) )
fprint ( " dist_x . shape , " , dist_x . shape )
fprint ( ’ dist_x , ’ , dist_x )
fprint ( ’ type ( dist_x , ’ , type ( dist_x ) )

393
394
395
396
397
398

fprint ( ’ drug_static . shape , ’ , drug_static . shape )
fprint ( ’ type ( drug_static ) , ’ , type ( drug_static ) )
fprint ( ’ exp_y . shape , ’ , exp_y . shape )
fprint ( ’ type ( exp_y ) , ’ , type ( exp_y ) )
fprint ( ’ exp_y , ’ , exp_y )

399
400
401
402
403

steps_per_epoch = np . int ( np . ceil ( len ( train ) / batch_size ) )
validation_steps = np . int ( np . ceil ( len ( val ) / batch_size ) )
steps = np . int ( np . ceil ( len ( test ) / batch_size ) )
fprint ( " steps_per_epoch = ," , steps_per_epoch )

92

Vid Sustar

404
405
406

7 APPENDIX

fprint ( " validation_steps = ," , validation_steps )
print ( " validation_steps = ," , validation_steps )
fprint ( " steps = ," , steps )

407
408
409
410

window_length = out_len

411
412
413
414

415
416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

cnn_droupout =0.5
# model architecture
cnn_i = Input ( shape =( sim_len_f , nsize1 , nsize , nsize ,1) , name = ’
Input_3D ’) # try without this
fprint ( " # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # INPUT SHAPE : , " )
fprint ( " cnn_i = Input ( shape =( sim_len_f , " , sim_len_f , " , nsize1 , " ,
nsize1 , " , nsize , " , nsize , " , nsize , " , nsize , " ,1) ) " )
cnn = TimeDistributed ( Conv3D (32 , kernel_size =( kernel_size ,
kernel_size , kernel_size ) , activation = ’ relu ’ , padding = ’ same ’ ,
ke rn el _i ni ti al iz er = ’ he_uniform ’) , name = ’ Conv3D_1 ’) ( cnn_i ) #
initially Conv3D (64 , lowered to 32 to have less variables , less
needed mempry
cnn = TimeDistributed ( MaxPooling3D ( pool_size =(2 , 2 , 2) ) , name = ’
MaxPooling3D_1 ’) ( cnn )
cnn = TimeDistributed ( Ba tc hN or ma liz at io n ( center = True , scale = True
) , name = ’ B a t c h N o r m a l i z a t i o n _ 1 ’) ( cnn )
cnn = TimeDistributed ( Dropout ( cnn_droupout ) , name = ’ Dropout_1 ’) (
cnn )
cnn = TimeDistributed ( Conv3D (64 , kernel_size =( kernel_size ,
kernel_size , kernel_size ) , activation = ’ relu ’ , padding = ’ same ’ ,
ke rn el _i ni ti al iz er = ’ he_uniform ’) , name = ’ Conv3D_2 ’) ( cnn ) # initially
Conv3D (64 , lowered to 32 to have less variables , less needed
mempry
cnn = TimeDistributed ( MaxPooling3D ( pool_size =(2 , 2 , 2) ) , name = ’
MaxPooling3D_2 ’) ( cnn )
cnn = TimeDistributed ( Ba tc hN or ma liz at io n ( center = True , scale = True
) , name = ’ B a t c h N o r m a l i z a t i o n _ 2 ’) ( cnn )
cnn = TimeDistributed ( Dropout ( cnn_droupout ) , name = ’ Dropout_2 ’) (
cnn )
cnn = TimeDistributed ( Conv3D (128 , kernel_size =( kernel_size ,
kernel_size , kernel_size ) , activation = ’ relu ’ , padding = ’ same ’ ,
ke rn el _i ni ti al iz er = ’ he_uniform ’) , name = ’ Conv3D_3 ’) ( cnn ) # initially
Conv3D (64 , lowered to 32 to have less variables , less needed
mempry
cnn = TimeDistributed ( MaxPooling3D ( pool_size =(2 , 2 , 2) ) , name = ’
MaxPooling3D_3 ’) ( cnn )
cnn = TimeDistributed ( Ba tc hN or ma liz at io n ( center = True , scale = True
) , name = ’ B a t c h N o r m a l i z a t i o n _ 3 ’) ( cnn )
cnn = TimeDistributed ( Dropout ( cnn_droupout ) , name = ’ Dropout_3 ’) (
cnn )
cnn = TimeDistributed ( Conv3D (256 , kernel_size =( kernel_size ,
kernel_size , kernel_size ) , activation = ’ relu ’ , padding = ’ same ’ ,
ke rn el _i ni ti al iz er = ’ he_uniform ’) , name = ’ Conv3D_4 ’) ( cnn ) # initially
Conv3D (64 , lowered to 32 to have less variables , less needed
mempry
cnn = TimeDistributed ( MaxPooling3D ( pool_size =(2 , 2 , 2) ) , name = ’
MaxPooling3D_4 ’) ( cnn )
cnn = TimeDistributed ( Ba tc hN or ma liz at io n ( center = True , scale = True

93

Vid Sustar

432

433

434

435

436

437
438

7 APPENDIX

) , name = ’ B a t c h N o r m a l i z a t i o n _ 4 ’) ( cnn )
cnn = TimeDistributed ( Dropout ( cnn_droupout ) , name = ’ Dropout_4 ’) (
cnn )
cnn = TimeDistributed ( Conv3D (512 , kernel_size =( kernel_size ,
kernel_size , kernel_size ) , activation = ’ relu ’ , padding = ’ same ’ ,
ke rn el _i ni ti al iz er = ’ he_uniform ’) , name = ’ Conv3D_5 ’) ( cnn ) # initially
Conv3D (64 , lowered to 32 to have less variables , less needed
mempry
cnn = TimeDistributed ( MaxPooling3D ( pool_size =(2 , 2 , 2) ) , name = ’
MaxPooling3D_5 ’) ( cnn )
cnn = TimeDistributed ( Ba tc hN or ma liz at io n ( center = True , scale = True
) , name = ’ B a t c h N o r m a l i z a t i o n _ 5 ’) ( cnn )
cnn = TimeDistributed ( Dropout ( cnn_droupout ) , name = ’ Dropout_5 ’) (
cnn )
cnn = TimeDistributed ( Flatten () , name = ’ Flatten_3D ’) ( cnn )
cnn = TimeDistributed ( Dense ( cnn_fin_dense_size , activation = ’ relu ’
) , name = ’ Dense_3D ’) ( cnn )

439

exposure_i = Input ( shape =( sim_len_f ,

440

arr_len ) , name = ’ Input_SASA ’

)
441

exposure = TimeDistributed ( Dense ( arr_len , activation = ’ sigmoid ’ ,
input_shape =( sim_len_f , arr_len ) ) , name = ’ Dense_SASA ’) ( exposure_i )

442
443
444

445

distance_i = Input ( shape =( sim_len_f , arr_len_dist ) , name = ’
Input_Distances ’)
distance = TimeDistributed ( Dense ( arr_len_dist , activation = ’
sigmoid ’ , input_shape =( sim_len_f , arr_len_dist ) ) , name = ’
Dense_Distances ’) ( distance_i )

446
447
448

449

drug_i = Input ( shape =( sim_len_f , drug_st_len ) , name = ’
Input_DrugType ’)
drug = TimeDistributed ( Dense ( arr_len , activation = ’ sigmoid ’ ,
input_shape =( sim_len_f , drug_st_len ) ) , name = ’ Dense_DrugType ’) (
drug_i )

450
451

n_neurons = n_neurons # 128#1024#128#512 # check

452
453

454
455
456
457

# some formula to set the size of batches in regard to the input
size , if input is fed in small windows and the matrix size small
then there should be largers batches
n_batch = batch_size # int (300/(( size /(1+ quad *7) + sim_len_f ) ) )
if ( n_batch < 1) :
n_batch = 1
print ( "
#######|||||||||||#####|||||#####|||||#####|||||#####||||||||||||||||##########
n _ b at c h n _ b a t c h n _ b a t c h " , n_batch )

458
459

460

461

merged1 = Concatenate ( name = ’ Concatenate_3D - SASA ’) ([ cnn , exposure
])
merged2 = Concatenate ( name = ’ Concatenate_3DSASA - Distances ’) ([
merged1 , distance ])
merged3 = Concatenate ( name = ’ Concatenate_3DSASADistances - DrugType
’) ([ merged2 , drug ]) # maybe it works to concatenate all in one
level like merged1 = Concatenate () ([ cnn , exposure , drug ])

94

Vid Sustar

462

463

464

465

7 APPENDIX

merged = LSTM ( n_neurons , return_sequences = True , name = ’ LSTM ’) (
merged3 ) # return_sequences = False would return just last timestep ?
merged = Dense ( n_neurons , activation = ’ relu ’ , name = ’
Dens e_afte rLSTM _1 ’) ( merged )
merged = Dense ( int ( n_neurons /2) , activation = ’ relu ’ , name = ’
Dens e_afte rLSTM _2 ’) ( merged )
merged = TimeDistributed ( Dense ( arr_len_y , activation = ’ relu ’ ,
dtype = tf . float32 ) , name = ’ Dense_afterLSTM ’) ( merged )

466
467

468

model = Model ( inputs =[ cnn_i , exposure_i , distance_i , drug_i ] ,
outputs = merged ) # POTENTIAL PROBLEM , MAYBE DRUG CONCATENATION
NEEDS TO BE DONE BEFORE !
model . compile ( loss = str ( loss_funct ) , optimizer = ’ adam ’ , metrics =[
tensorflow . keras . metrics . MeanA bsolut eError () ]) # optimizer adjust
the learning rate , uses scheduled learning rate

469
470

fprint ( model . summary () )

471
472
473

474

475

# Open the file
with open ( path_new_folder + ’/ ’ + ’ model_summary ’+
u n i qu e _ a b o u t _ t h i s _ r u n + ’. txt ’ , ’w ’) as fh :
# Pass the file handle in as a lambda function to make it
callable
model . summary ( print_fn = lambda x : fh . write ( x + ’\ n ’) )

476
477

utils . plot_model ( model , path_new_folder + "
m u l t i _ i n p u t _ a n d _ o u t p u t _ m o d e l _ m i 0 1 " + " _ " + str ( size ) + " _ " + str ( interval
) + " _ " + str ( quad ) + " _ " + str ( sim_len_f ) + str ( kernel_size ) + " _ " +
loss_funct + " _ " + str ( average ) + u n i q u e _ a b o u t _ t h i s _ r u n + " . png " ,
show_shapes = True )

478
479

480

481

utils . plot_model ( model , path_new_folder + " TB " + " _ " + str ( size ) + " _ " +
str ( interval ) + " _ " + str ( quad ) + " _ " + str ( sim_len_f ) + str ( kernel_size ) + "
_ " + loss_funct + " _ " + str ( average ) + u n i q u e _ a b o u t _ t h i s _ r u n + " . png " ,
show_shapes = False , show_dtype = False , show_layer_names = False ,
rankdir = ’ TB ’ , expand_nested = False , dpi =300)
utils . plot_model ( model , path_new_folder + " TBnames " + " _ " + str ( size ) +
" _ " + str ( interval ) + " _ " + str ( quad ) + " _ " + str ( sim_len_f ) + str (
kernel_size ) + " _ " + loss_funct + " _ " + str ( average ) +
u n i qu e _ a b o u t _ t h i s _ r u n + " . png " , show_shapes = False , show_dtype = False
, show_layer_names = True , rankdir = ’ TB ’ , expand_nested = False , dpi
=300)
utils . plot_model ( model , path_new_folder + " TBdtype " + " _ " + str ( size ) +
" _ " + str ( interval ) + " _ " + str ( quad ) + " _ " + str ( sim_len_f ) + str (
kernel_size ) + " _ " + loss_funct + " _ " + str ( average ) +
u n i qu e _ a b o u t _ t h i s _ r u n + " . png " , show_shapes = False , show_dtype = True ,
show_layer_names = False , rankdir = ’ TB ’ , expand_nested = False , dpi
=300)

482
483
484
485

486
487

lat est_gp u_memo ry = gpu_memory_usage ( gpu_id )
print ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ###( GPU ) #( GPU ) #( GPU
) #####( GPU ) Memory used : " , lates t_gpu_ memory initial_memory_usage )
l a te s t _m e m or y _ to t a l = gpu_memory_total ( gpu_id )
print ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ###( GPU ) #( GPU ) #( GPU

95

Vid Sustar

488

7 APPENDIX

) #####( GPU ) memory_total : " , l a t es t _ me m o ry _ t ot a l )
print ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ###( GPU ) #( GPU ) #( GPU
) #####( GPU ) Memory used of total % : " , 100*( lates t_gpu_ memory i n i t i a l _ m e m o r y _ u s a g e ) / l at e s t_ m e mo r y _t o t al )

489
490
491
492
493
494
495

496
497
498

499
500
501
502
503
504
505

# ####### to time the epochs
class timecallback ( tf . keras . callbacks . Callback ) :
def __init__ ( self ) :
self . times = []
# use this value as reference to calculate cummulative
time taken
self . timetaken = time . process_time ()
def on_epoch_end ( self , epoch , logs = {}) :
self . times . append (( epoch , time . process_time () - self .
timetaken ) )
def on_train_end ( self , logs = {}) :
fig = plt . figure ()
plt . xlabel ( ’ Epoch ’)
plt . ylabel ( ’ Total time taken until an epoch in seconds ’)
plt . plot (* zip (* self . times ) )
fig . savefig ( ’ epochs . png ’)
timetaken = timecallback ()

506
507
508
509
510

# ## to be able to plot loss per batch !
class LossHistory ( tf . keras . callbacks . Callback ) :
def on_train_begin ( self , logs ={}) :
self . history = { ’ loss ’ :[] , ’ val_loss ’ :[] , ’
m ea n _ ab s o lu t e _e r r o r ’ :[] , ’ v a l _ m e a n _ a b s o l u t e _ e r r o r ’ :[]}

511
512
513
514

515
516

def on_batch_end ( self , batch , logs ={}) :
self . history [ ’ loss ’ ]. append ( logs . get ( ’ loss ’) )
self . history [ ’ m ea n _ ab s o lu t e _e r r or ’ ]. append ( logs . get ( ’
m ea n _ ab s o lu t e _e r r o r ’) )
self . history [ ’ val_loss ’ ]. append ( logs . get ( ’ val_loss ’) )
self . history [ ’ v a l _ m e a n _ a b s o l u t e _ e r r o r ’ ]. append ( logs . get (
’ v a l _ m e a n _ a b s o l u t e _ e r r o r ’) )

517
518

history = LossHistory ()

519
520
521

csv_logger = CSVLogger ( path_new_folder + ’/ ’+ ’ training_ ’+ str (
sim_len_f ) + " _ " + typeinp + str ( kernel_size ) + " _ " + loss_funct + " _ " + str (
average ) + " _ " + datetime . now () . strftime ( " % Y % m %d -% H % M % S " ) +
u n i qu e _ a b o u t _ t h i s _ r u n + ’. log ’)

522
523

n_epoch = num_epochs

524
525

526

checkpoint_path = " training_1_cp / cp29_ " + u n i q u e _ a b o u t _ t h i s _ r u n + " _
" + str ( sim_len_f ) + " _ " + typeinp + " _ " + str ( kernel_size ) + " _ " + loss_funct +
" _ " + " . ckpt "
model_path = " model_ " + u n i q u e _ a b o u t _ t h i s _ r u n + " . h5 "

527
528

checkpoint_dir = os . path . dirname ( checkpoint_path )

529
530

# Create a callback that saves the model ’s weights

96

Vid Sustar

531

7 APPENDIX

cp_callback = tf . keras . callbacks . ModelCheckpoint ( filepath =
checkpoint_path , monitor = ’ val_loss ’ , save_best_only = True ,
save _weigh ts_on ly = True , verbose =1)

532
533
534
535

# TENSORBOARD
logfold = os . getcwd () + ’/ logs ’
print ( logfold )

536
537
538

539

log_dir = " logs / fit / " + str ( sim_len_f ) + " _ " + typeinp + " _ " + str (
kernel_size ) + " _ " + loss_funct + " _ " + str ( average ) + " _ " + datetime . now () .
strftime ( " % Y % m %d -% H % M % S " )
t e n s o r b o a r d _ c a l l b a c k = tf . keras . callbacks . TensorBoard ( log_dir =
log_dir , histogram_freq =1)

540
541
542

if load_weights :
model . load_weights ( checkpoint_path )

543
544
545

if fit_model :
model . fit ( train_dataset , validation_data =
validation_dataset , steps_per_epoch = steps_per_epoch ,
validation_steps = validation_steps , callbacks =[ csv_logger ,
tensorboard_callback , history ] , epochs = num_epochs )

546
547
548
549

550

# Calling ‘ save ( ’ my_model ’) ‘ creates a SavedModel folder ‘
my_model ‘.
model . save ( " my_model " + str ( sim_len_f ) + " _ " + str ( width ) + " _ " + " _ " +
typeinp + " _ " + str ( kernel_size ) + " _ " + loss_funct )

551
552
553

554
555

556

lat est_gp u_memo ry = gpu_memory_usage ( gpu_id )
fprint ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ### Memory used : , "
, latest_g pu_mem ory - i n i t i a l _ m e m o r y _ u s a g e )
l a te s t _m e m or y _ to t a l = gpu_memory_total ( gpu_id )
fprint ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ### memory_total
: , " , l a t es t _ me m o ry _ t ot a l )
fprint ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ### Memory used of
total % : , " , 100*( lates t_gpu_ memory - i n i t i a l _ m e m o r y _ u s a g e ) /
l at e s t_ m e mo r y _t o t a l )

557
558

test_loss , test_accuracy = model . evaluate ( test_dataset , steps =
20)

559
560
561
562
563
564
565
566
567
568
569
570

steps = np . int ( np . ceil ( len ( test ) / batch_size ) )
predictions = model . predict ( test_dataset , steps = steps )
fprint ( " type ( prediction ) ," , type ( predictions ) )
fprint ( " prediction . shape , " , predictions . shape )
i =0
n_test_sims = len ( test )
numtest = predictions . shape [0]
pred_strt =0
pred_end = int ( numtest / n_test_sims )
fprint ( " Number of test_files : , " , len ( test ) )
fprint ( " test files : , " , test )

571

97

Vid Sustar

572
573
574
575

576

7 APPENDIX

t e s t _ t _ f i l e n a m e s _ o u t =[]
for testfile in test :
testfile = str ( testfile )
testfile_sh = testfile [( testfile . rfind ( " / " ) +2) : testfile . rfind (
" . npz " ) ]
t e s t _ t _ f i l e n a m e s _ o u t . append ( testfile_sh )

577
578
579

580
581
582
583
584
585
586
587
588

589
590

with open ( " t e s t _ f i l e s _ l i s t _ o u t s i d e . csv " , ’w ’ , newline = ’ ’) as
myfile2 :
wr = csv . writer ( myfile2 , quoting = csv . QUOTE_ALL )
wr . writerow ( t e s t _ t _ f i l e n a m e s _ o u t )
fprint ( " PREDICTIONS : ," , type ( predictions ) )
fprint ( " PREDICTIONS : ," , predictions . shape )
fprint ( " predictions , " , predictions [0])
fprint ( " predictions , " , predictions [0][0])
for i in range ( n_test_sims ) :
fprint ( " print ( i ) , predictions [ i ][0]) ," ,i , predictions [ i ][0])
np . savetxt ( path_new_folder + test_t_filenames [ i ]+ " _ " + str ( i ) + " _
" + str ( size ) + " _ " + str ( interval ) + " _ " + str ( quad ) + " _ " + str ( sim_len_f ) + " _
" + str ( out_len ) + " . csv " , predictions [ i ] , delimiter = " ," )
pred_strt = pred_end
pred_end = pred_end + int ( numtest / n_test_sims )

591
592
593
594

recursive_chmod ( path_new_folder , 0 o777 ) #
time_elapsed = datetime . now () - start_time
fprint ( ’ Time elapsed ( hh : mm : ss . ms ) , {} ’. format ( time_elapsed ) )

595
596
597

598
599

600

601

lat est_gp u_memo ry = gpu_memory_usage ( gpu_id )
print ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ###( GPU ) #( GPU ) #( GPU
) #####( GPU ) Memory used : " , lates t_gpu_ memory initial_memory_usage )
l a te s t _m e m or y _ to t a l = gpu_memory_total ( gpu_id )
print ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ###( GPU ) #( GPU ) #( GPU
) #####( GPU ) memory_total : " , l a t es t _ me m o ry _ t ot a l )
print ( " ##( GPU ) #( GPU ) #( GPU ) ##( GPU ) #( GPU ) #( GPU ) ###( GPU ) #( GPU ) #( GPU
) #####( GPU ) Memory used of total % : " , 100*( lates t_gpu_ memory i n i t i a l _ m e m o r y _ u s a g e ) / l at e s t_ m e mo r y _t o t al )
model . save ( model_path )

Listing 7: Example Python - Tensorflow script to train and use the 3D CNN LSTM
model to predict the SASA exposure as shown in Results.

98

